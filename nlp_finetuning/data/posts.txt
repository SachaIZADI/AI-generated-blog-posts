=========== The complex challenge of demand forecasting for business ===========
Demand forecasting is critical to businesses across almost all industries. It can seem easy, because there are easy ways to build simple models. But in practice, building a demand forecasting model that is accurate and useful is a complex challenge.

As an illustration, below are four types of models, each with more complexity than the one before it.

Exhibit 1: Four demand forecasting models, each of which is more complex than the one before it

Most companies are doing (1) or (2), and the most advanced ones have implemented (3). But very few are actually working on (4), which is where the true value can come from. Indeed, with each use case, there are specific tools that need to be used. An off-the-shelf solution will never yield the level of accuracy desired.

When demand forecasting is not critical to the business, simple models will suffice. But when the lion’s share of operational decisions made are based on it, having a robust and accurate model is key. Among the industries where demand forecasting is business critical are:

· Retail: to determine the amount of particular product that should be available and at what price, via which channels, etc.

· Industrial manufacturing: to decide how much of each product should be produced, the amount of stock that should be available at various points in time, when maintenance should be performed, etc.

· Travel and tourism: to assess, in light of available capacity, what price should be assigned (for hotels, flights), which destinations should be highlighted, what types of packages should be advertised, etc.

Traditionally, demand forecasting has largely been done using time-series algorithms. Such techniques make use of signal extrapolation whereby trends, seasonality, and cycles that occurred in the past are used to forecast demand in the future. This method works well when the forecast horizon is short-term, when the demand has some stationarity properties (i.e. trends and cycles can be identified), and when the number of variables that influence demand is limited.

However, if those conditions are not met — for example, if the forecasting is done too far in advance — as long as the demand is product-based (quality, price, etc.) or location- or people-based (store, occasion), then the use of feature-based algorithms, which rely on an abundance of data, is preferable.

The use of feature-based algorithms has been made possible, in large part, by four factors:

· The development of feature-based algorithms such as random forest, gradient boosting, and deep learning, which take advantage of data quantity to increase predictive power

· The creation of advanced sequential and “signal extrapolation” models such as recurrent neural networks, long short-term memory, which can take in more features and perform more granular analysis than simpler models, improving the accuracy of outcomes

· The emergence of new data lake technologies such as Hadoop and Spark, which allow for efficient storing and processing of enormous amounts of data at a reasonable cost

· The availability of better tools, such as Python, as well as access to libraries (most of which are open source) and the online community that builds and supports them

A demand forecast model is not something one can build blindly. Before diving into the algorithms, the data scientist should first determine what it will be used for (e.g., buying optimization, store allocation, pricing optimization, etc.). This rigorous definition of the business case at stake is needed to make sure the right granularity, cost function, and expected accuracy will be applied.

Moreover, there is no single demand forecast model that will work across use cases. Let’s look at two very different use case examples, examine where and how they differ, and which demand forecast model should be used for each.

Let’s first consider a fashion accessories retailer. It plans to launch a number of new products — among them a new line of watches and a new line of handbags — and so needs to order the right amount of goods from its suppliers some three to six months in advance, the lead time needed to manufacture them and ship them out. Each product has a planned lifetime of between three and 12 months (see Exhibit 2).

To get an accurate demand forecast, the retailer can rely on past point-of-sale or online transactions of similar products. It has multiple data points for each of those similar products, among them the time and date when each purchase was made, the specific products that were purchased, the channels through which the purchases were made, whether or not a discount was applied, and any related advertising campaigns. The retailer also has data pertaining to each product’s characteristics (e.g., price, color, material, look and feel, etc.). So, the retailer has a lot of buying context from which it can extract information to potentially create a very accurate demand forecast.

Because the retailer needs to order up to six months in advance, it can only order once, as products rotate quickly. As such, the model needs to output one single value that encapsulates the total demand for each product’s lifespan. It doesn’t need to be precise on a more granular level, such as weekly or daily.

Because the retailer knows some of the feature values at the time the orders are made — such as placement and product characteristics — they can be used in the forecast to predict demand. For those feature values that are unknown at the time of ordering, such as advertising campaigns, it can either build individual scenarios or use averages, for example by assuming a product will be, on average, advertised during a certain period of time.

The retailer expects high substitution among the products; in other words, the demand for one product will depend on the number of other, “similar” products available at the same time. It could subsequently decide to create such a feature (i.e., number of similar products available at the same time) by building a similarity measure among the products in order to learn how it has impacted demand in the past.

If the retailer wants to predict one single value per product, the use of regression analysis is appropriate. Depending on the type of data, (e.g., numeric or text), volume, collinearity, etc., it can leverage generalized linear models, random forest, gradient boosting, or neural networks.

For its similarity measure, the retailer can either use the existing product categories or build a clustering model. Or it can try both and see which one is the most significant.

The retailer expects some non-stationarity, as certain products can be trending. Hence it can build an overall trend prediction model based on external data — for example by learning the relative sales of a certain type of product based on the number of tweets mentioning it — and use it as another feature in the final model. Or it can build a separate model and stack the two together.

For the optimization part (forecasting how much of each product should be ordered), the retailer knows that the demand will not be perfectly forecasted. So, it needs to either account for a specific strategy — we can buy a bit more, or we can buy a bit less — or build a stochastic optimization model to decide how much to purchase based on the cost of under- and over-buying. (See Exhibit 3)

Let’s now consider the case of a steel manufacturer that produces several types of products for large clients in the automotive, construction, and packaging industries.

Each of the manufacturer’s products has its own specifications (e.g., grade, thickness, purity), but they share the same raw material input and some similarities in the production chain. It has several assets along that production chain tasked with various processes, such as hot rolling and galvanizing, but each of the assets has a constrained capacity.

The manufacturer’s most profitable clients expect a quick turnaround when they place orders, which is much faster than the lead time needed to actually produce those orders. As such, the orders have to be anticipated and scheduled in a smart way. (See Exhibit 4)

Exhibit 4: Three stages at which demand forecasting is needed to optimize a steel manufacturer’s supply chain

Because the manufacturer has clients from different industries, whose demand for steel is largely driven by their own commercial activity, it makes sense to use a time-series model that includes their specific business drivers, such as new car production, new construction projects, and seasonality.

Also, because the necessary lead time is long while the order delays can be short, a demand forecast needs a long time horizon — one at least equal to the lead time.

While the products are different, some share similarities regarding production, hence the demand has to be forecasted at different stages: both demand for final products as well as demand for each of the intermediary products.

And finally, because the production capacity is constrained, demand cannot be forecasted by directly linking to actual past sales. Rather, past demand has to be extracted from the order book.

For each client and each product, the manufacturer will want to forecast a time series of weekly demand that will cover an entire year, with less accuracy needed over time.

The time series’ main drivers will be signal features (e.g., trend, seasonality) and external drivers; for example, when it comes to automotive clients, external drivers would include the growth of the automotive industry as well as those clients’ own financials. The models the steel manufacturer could use would be ARIMA or LSTM, maybe stacked with a feature-based regression model for other drivers.

Since the different products share some similarities, it would also aggregate each of the products’ demand at each step of the production chain along with the capacity of each asset in order to understand what order volume would be feasible.

Then, given the demand probability for each product and each client, the corresponding price, the current stock, the current maintenance plan, and other potential business constraints, it could perform production optimization to decide which material should be produced (including intermediary products), what levels of stock will be needed, which orders can be filled and which orders have to be delayed or refused, etc. (See Exhibit 5)

As we’ve seen, demand forecasting is a large class of complicated analytical problems, where multiple models can be used depending on the business context and needs. Here the role of the data scientist and their ability to translate an organization’s business needs into data science, pick the right algorithms, and implement them correctly is key.

That’s because demand forecasting is a complex challenge, one that requires significant knowledge in order to determine which of the various models can and should be used for the situation in question. Among them:

· Potentially NLP or image processing, if the input data is in a non-structured format

· Optimization models to translate the demand forecast into actual decisions (e.g., linear programming, genetic algorithms, stochastic optimization)

Indeed, there is no single demand forecast model that will work across use cases. Which is why when demand forecasting is business critical, the role of the data scientist is, too.
=========== An “Algorithmization” of Business Operations ===========
A company’s data should be delivering real, quantifiable value. BCG Gamma’s Sylvain Duranton talks about what a company can expect when AI experts apply analytics.

This interview, conducted by Muriel Motte, was published in L’Opinion, a French newspaper, in April 2017

BCG Gamma, which uses advanced analytics to achieve breakthrough business results, was launched in 2016. Under the supervision of Sylvain Duranton, a senior partner in the Paris office of The Boston Consulting Group, 250 expert data scientists (400 by the end of the year) in North America, Europe, and Asia provide support to businesses in the field of artificial intelligence.

Muriel Motte: What is the mission of BCG Gamma?

Sylvain Duranton: We aim to assume responsibility for the analytics, or artificial intelligence, modules of BCG’s client projects. We operate in every domain, for example, optimizing container transport and improving the management of yield per passenger for an airline company. The distribution sector is particularly fertile terrain for the utilization of data. For one major US group, we set up a system of personalized promotions that addressed 12 million loyalty card holders. Every week, this group had been sending out different offers to 30 customer segments. Using internal customer data, we established a personalized marketing system that can produce almost 400,000 promotions that correspond to specific customer profiles. For a fashion house, our algorithms can even enable the company to predict, with 80% accuracy, whether or not a particular loyalty card holder will buy any clothing within the next 30 days and, if so, to anticipate that he will buy a trendy beige cotton jacket. For companies, the potential gain from data analytics can easily represent 1% to 3% of turnover. So this is huge.

Motte: On what is your work based?

Duranton: Essentially, we use the data stored by our clients and connect that with the external data collected from outside the company. For example, we have developed an application in New York that helps taxi drivers look for customers where they actually are. To do that, we divided the city into a grid of 200-meter squares and then identified, for example, all the bars, restaurants, museums, and offices in each square. We developed an algorithm that is based on this information and historical data of taxi transportation in New York. We tested the model to check its relevance and to improve it with data obtained in real time. Finally, we studied the way drivers use the data: What level of precision is relevant to making the right decision? Once the technology was working well with the users, we launched the industrialization phase. Today, our model is running also in London and other big cities. In summary, starting with data from the past, we developed a predictive algorithm that can even be prescriptive: in other words, it can either make decisions or aid in making decisions.

Motte: Are businesses underprepared for the data revolution?

Duranton: There is considerable literature on the gold rush represented by the conquest of data, the oil of tomorrow. Many companies have already recruited analytics teams — young people fresh out of the top universities who have PhDs in computer science and data science. The next major difficulty is to convert this science into action. We often see these teams working in isolation, like goldfish in their bowl, with no connection to the rest of the company. Or else we see teams that are well integrated but are working simply as prototype factories. They construct an algorithm and demonstrate that it can work, which reassures the board of directors. And then, nothing happens. But if you want to make money with data analysis, the rule is that the algorithm itself represents only 10% of the total effort. Technology and the information systems account for 20%. The essential part — 70% — is the adaptation of working processes, so that everyone is capable of using these new technologies. The devotees of analytics too often do the first 10% of the work ten times over and then simply stop short of the rest. My recommendation to companies is to do less but to do it all the way. That’s when analytics is really worth it: the positive impact can affect a company’s turnover by several percentage points. In the field of predictive maintenance, analytics can bring considerable savings by enabling companies to prevent breakdowns of extremely expensive machines.

Motte: After the digital divide, should we now be fearing a data divide?

Duranton: In many fields, companies have already been exploiting data for 20 or 30 years: telecom operators have been using it to model churn, the rate of subscriber cancellations, and airline and passenger rail companies and the hotel industry, for yield management, or setting the right prices as a function of varying demand over time. The rupture stems from the fact that today all industries, all sectors, and all functions — including HR — are concerned. In general, startups are well-equipped for this new revolution. Many are founded on the data universe. Among major corporations, taken broadly, industrial companies are trailing services. Small and midsize enterprises will have to progress on the cultural level, but they are more adept than others at concentrating on a few very important subjects. They do not suffer from the prototype factory syndrome. By contrast, all businesses, small and large, are facing difficulties of recruitment. The job called data scientist was voted the hottest job of the year in 2016, and it will likely be so again this year. To hire a data scientist, a small company in the provinces is now in competition with Google and Facebook. This HR battle is very different from recruiting a production manager that another small company down the road also wants to hire. A profound overhaul of HR career paths is necessary because these populations have very different expectations. This is a major upheaval.

Motte: Ray Dalio, the founder, cochief investment officer, and cochairman of the speculative investment firm Bridgewater Associates, envisions being replaced by an algorithm that will define the company’s fund strategy and manage the career of its employees. Is that the future?

Duranton: I’m going to use a word that is not very elegant: we are certainly going to see an “algorithmization” of business operations. However, corporate strategy is one of the areas that cannot be modeled. Anticipating economic crises is very difficult. With corporate strategy, it’s more or less the same picture. Machines are unbeatable at games of go and chess. These are closed environments, where it is possible to test millions of combinations. With very open subjects, it’s not the same at all.

Motte: So we shouldn’t be afraid of the emergence of a world of robots?

Duranton: Our big area of expertise is the man-machine interface. Instances of entirely automated universes are quite rare. For example, medical diagnosis will probably always be the work of a doctor. By contrast, computer-assisted diagnosis will be the product of hundreds of millions of consultations or studies analyzed by algorithms. We are heading not for a world of robots but for a world of droids — in other words, robots that assist humans but do not replace them. On the societal level, this means that we will undoubtedly be able to avoid popular revolts. But that also means allowing for a long period of familiarization and hands-on training in the new technologies. This corresponds to the 70% that I mentioned earlier. We will have to appropriate this new world. One outcome of the revolution already underway: the companies that collect the most data and are best able to exploit their data will be giving themselves a significant competitive advantage. Like brands, data will become an asset that will have to be managed and nurtured in compliance with clearly defined ethics and principles. How will Google be able to guarantee us that its search engine is not biased? The societal challenges are immense.

Motte: What do you expect from your collaboration with Cédric Villani?

Duranton: We recruit the teams of BCG Gamma from advanced mathematics laboratories. We formed our partnership with Inria, the French Institute for Research in Computer Science and Automation, because, for certain subjects, we want to have the capacity to put researchers to work on client problems. In fact, we are currently in the process of signing the same types of partnerships with other major research institutes in England, the United States, and Germany. Our work with Cédric Villani is vital. He is a great expert on the connection between the academic and the business worlds. He is one of the scientists who are most involved in creating bridges between these two universes. We share with him a common aspiration for the development of the mathematics sector in France. Mathematics, being less hampered by social predeterminism, is an extremely important sector for professional integration and insertion.
=========== Hooked for Good, with Gamification ===========
The same physiological mechanism that hooks people on Fortnite or Pokémon Go can be harnessed to foster customer connections, spur employee ingenuity, or even solve biological mysteries.

For 15 years, biologists struggled to decipher the crystal structure of the HIV virus. Then, in 2008, scientists at the University of Washington found the key to a solution. They developed an interactive online game that challenged players to make a molecule matched to a human receptor, like a 3D puzzle. The game, which they named Foldit, attracted millions of amateurs. In 10 days, the mystery was solved.

Foldit is an extraordinary example of gamification, a technique that applies the fun and satisfying design elements of games to engage people in real-world activities, such as problem solving, promoting a product, or learning. Over the past decade, gamification has become all the rage. In a world where people are constantly barraged with messages, it’s a powerful way to capture and retain audience attention. Gamification is changing how we play, work, and interact with others. And as Foldit demonstrates, its potential is immense.

Perhaps you’re looking for creative ways to build and sustain a connection with customers or engage employees in tough tasks. Maybe, like the biologists, you’re trying to solve a seemingly intractable problem. Or you might simply be wondering what it is about those challenges from your favorite microbrewer that you can’t resist. Here’s the lowdown on how you’re getting hooked — and how you can do the hooking for your particular audience.

What is it?

Gamification is all about engaging an audience in a way that enhances the user experience — motivating people to play for points or prizes or for the sheer fun of competing with others. A key design principle of gamification is that it keeps people coming back for more. Think about social media. When you post something on Facebook or LinkedIn, you feel the anticipation of getting responses and the excitement that comes with receiving them. You watch the number of “likes” grow. These feelings induce you to post more, go back to see your feedback, and so on, over and over again.

Consider some of the more successful examples of gamification:

· M&M’s Eye Spy pretzel game helped put the candymaker’s new pretzel-flavored M&M on the map. Users competed to find the tiny pretzel in an image of masses of M&M candies. The game attracted 25,000 “likes” and thousands of “shares” and comments.

· The Nike + Fuelband inspired millions of users to track their physical activity and workouts with a bracelet and an app. In return, they got feedback, reward messages, the ability to challenge friends, and connection to an online community.

· Heineken, a sponsor of the Union of European Football Associations’ Champions League, created its Star Player Game app that challenges users to win points by making predictions about a football game while watching it in real time. (Will the team score in 10 seconds? Will the goal be made by the head or the foot? )

· 4food.com, a fast-food burger outlet that promises to “de-junk” fast food, has a website that is essentially a game suite. It encourages users to profile their tastes in exchange for a free meal, to make recommendations and win coupons, and to brand and market their own burger creations to earn credits, among other things.

Of course, gamification is more than just a consumer marketing tool.

· Khan Academy uses all kinds of gamification elements in the online video courses it offers for free. (That’s by design: the academy grew out of online math lessons founder Sal Khan developed for his nephew, who hated math in school.) A visualization map lets users follow their progress through a subject, as though advancing through a video game. Users gain rewards and reinforcement by build winning streaks for solving problems quickly. They also earn points for time spent on instruction or exercises.

· The UK Department of Work and Pensions modernized the old suggestion box with Idea Street, launched in late 2009. Within one year, the game had attracted 4,500 users and generated 1,400 ideas — 1,100 of which were seriously considered. Ultimately, 63 were implemented.

· Lawley Insurance had no luck persuading employees to maintain the company database. So this independent insurer developed a contest to motivate people to correct missing or inaccurate information and keep records up to date. Users earn points for logging activities (such as phone calls and meetings with prospects) and managing sales opportunities (such as updating close dates). In its initial launch, the game got employees to log in two weeks what normally took more than seven months.

· SAP’s Roadwarrior helps the company’s sales team stay prepared. The tool simulates the sales planning meeting (pre-customer call), using multiple choice questions. Leaderboards track the best players, and players can challenge winners to a quiz duel. The game also helps team members stay motivated and up to date on product changes.

So how does it work?

Gamification taps basic psychological elements (fun, the joy of play) and their deeper physiological sources. Biologically, it triggers the continuous loop of “wanting and seeking” and “liking” by stimulating the release of the neurotransmitter dopamine in the brain. Dopamine induces the seeking impulse, and when you get the reward, it stimulates you to seek more. In other words: it hooks us. Some people call this “the winner effect”: like the poker player on a winning streak, the more we win, the more we want.

Dopamine release is an evolutionary mechanism designed to enable humans to move successfully through the world. It ensures that rather than sit around and starve to death, we continue to learn and survive, learn and survive.

Men and women participate in gamification in equal numbers, and the average age is 35. But the biological and psychological lures are so deeply ingrained in the human brain that all of us are potential players. Yu-Kai Chou, one of the pioneers of gamification, identified eight core psychological drives that gamification feeds. Among them: the need to be part of something greater than ourselves, the need to interact with others (in friendship or competition, or for mentoring), the desire to possess things, the desire for something we can’t have; and the urge to know what will happen next.

Amy Jo Kim, another gamification guru, classifies gamification types according to their primary motivation. These motivations are important to consider when developing gamification for targeted audiences.

Explorers are motivated by the opportunity to learn, explore boundaries, and know the rules and loopholes.

Expressers are motivated by the chance to express themselves. They enjoy using tools and systems that enable them to do so. Through their creativity, they look for status and recognition.

Competitors enjoy putting their skills and prowess to the test. External rankings and mastery are important to them, as is building friendly relationships through competition.

Collaborators like working with other people toward a greater goal. They value partnership and shared learning.

According to Kim, the essence of the game is “the journey towards mastery.” A well-constructed journey will attract an audience. If you doubt that, just remember the mushroom-eating, turtle-throwing Italian plumber who attracted millions of players over the past 30 years. (Super Mario, for those of you who are videogame-clueless.) The journey must have three steps:

Onboarding. Here, the new player discovers the story, the goal, the point system, and the basics of how to play. This step is where a game designer presents the core drives that the game will address.

Habit-building. Once familiar with the moves, the player can now engage on his or her own. The challenge for the designer is to figure out how to take the player through repetitive actions — aligning candies (as in Candy Crush), throwing birds with a slingshot (as in Angry Birds), killing bad guys (countless games), and so on — without getting bored.

Because the game must satisfy the winner effect, the game designer must keep fueling the “win-state” to get the player to keep returning. Keeping players entertained means progressively increasing the difficulty of challenges, by creating new and more complex patterns. The trick is finding the sweet spot: making the changes occur soon enough so players don’t get bored, but not too fast to frustrate them.

Mastery. Also known as the endgame, this step is attained only by the most persevering. Because such players are a game’s greatest evangelists, it is crucial that this step be exciting. The designer needs to ensure that these players promote the benefits of reaching mastery, to motivate all the other players to continue.

Finally, no game would be complete without compelling rewards. To make players happy, the game must keep feeding them with points, money, badges, status, rankings on leaderboards or other systems — whatever its particular rewards — so they can track their progress while satisfying their hunger for more.

Good marketers have long understood the importance of this game element. A classic example is the frequent flyer mileage program. The monetary value of the accumulated miles doesn’t even matter; flyers just want to earn more of them. On top of winning prizes (“free” flights), players also win status: they bypass long lines, win upgrades to first class, and earn privileges, like access to the lounge.

Onboarding, habit-building, mastery and rewards are essential elements for gamification. Games intended to be learning tools need challenges that connect to learning objectives as well as levels that indicate progress.

Take a closer look at the websites, apps, and products you use every day. You’ll start to see just how many are using elements of gamification to keep you coming back for more. And whether you’re selling candy or cross-training shoes, trying to boost employee engagement or productivity or aiming to unlock the key to a devastating disease, gamification may be just the ticket for capturing and keeping your audience through the element of fun.

For more on about Yu-Kai Chou’s gamification framework, go to http://yukaichou.com/gamification-examples/octalysis-complete-gamification-framework. Learn more about Amy Jo Kim’s theories and work at amyjokim.com.
=========== The complex challenge of demand forecasting for business ===========
Demand forecasting is critical to businesses across almost all industries. It can seem easy, because there are easy ways to build simple models. But in practice, building a demand forecasting model that is accurate and useful is a complex challenge.

As an illustration, below are four types of models, each with more complexity than the one before it.

Exhibit 1: Four demand forecasting models, each of which is more complex than the one before it

Most companies are doing (1) or (2), and the most advanced ones have implemented (3). But very few are actually working on (4), which is where the true value can come from. Indeed, with each use case, there are specific tools that need to be used. An off-the-shelf solution will never yield the level of accuracy desired.

When demand forecasting is not critical to the business, simple models will suffice. But when the lion’s share of operational decisions made are based on it, having a robust and accurate model is key. Among the industries where demand forecasting is business critical are:

· Retail: to determine the amount of particular product that should be available and at what price, via which channels, etc.

· Industrial manufacturing: to decide how much of each product should be produced, the amount of stock that should be available at various points in time, when maintenance should be performed, etc.

· Travel and tourism: to assess, in light of available capacity, what price should be assigned (for hotels, flights), which destinations should be highlighted, what types of packages should be advertised, etc.

Traditionally, demand forecasting has largely been done using time-series algorithms. Such techniques make use of signal extrapolation whereby trends, seasonality, and cycles that occurred in the past are used to forecast demand in the future. This method works well when the forecast horizon is short-term, when the demand has some stationarity properties (i.e. trends and cycles can be identified), and when the number of variables that influence demand is limited.

However, if those conditions are not met — for example, if the forecasting is done too far in advance — as long as the demand is product-based (quality, price, etc.) or location- or people-based (store, occasion), then the use of feature-based algorithms, which rely on an abundance of data, is preferable.

The use of feature-based algorithms has been made possible, in large part, by four factors:

· The development of feature-based algorithms such as random forest, gradient boosting, and deep learning, which take advantage of data quantity to increase predictive power

· The creation of advanced sequential and “signal extrapolation” models such as recurrent neural networks, long short-term memory, which can take in more features and perform more granular analysis than simpler models, improving the accuracy of outcomes

· The emergence of new data lake technologies such as Hadoop and Spark, which allow for efficient storing and processing of enormous amounts of data at a reasonable cost

· The availability of better tools, such as Python, as well as access to libraries (most of which are open source) and the online community that builds and supports them

A demand forecast model is not something one can build blindly. Before diving into the algorithms, the data scientist should first determine what it will be used for (e.g., buying optimization, store allocation, pricing optimization, etc.). This rigorous definition of the business case at stake is needed to make sure the right granularity, cost function, and expected accuracy will be applied.

Moreover, there is no single demand forecast model that will work across use cases. Let’s look at two very different use case examples, examine where and how they differ, and which demand forecast model should be used for each.

Let’s first consider a fashion accessories retailer. It plans to launch a number of new products — among them a new line of watches and a new line of handbags — and so needs to order the right amount of goods from its suppliers some three to six months in advance, the lead time needed to manufacture them and ship them out. Each product has a planned lifetime of between three and 12 months (see Exhibit 2).

To get an accurate demand forecast, the retailer can rely on past point-of-sale or online transactions of similar products. It has multiple data points for each of those similar products, among them the time and date when each purchase was made, the specific products that were purchased, the channels through which the purchases were made, whether or not a discount was applied, and any related advertising campaigns. The retailer also has data pertaining to each product’s characteristics (e.g., price, color, material, look and feel, etc.). So, the retailer has a lot of buying context from which it can extract information to potentially create a very accurate demand forecast.

Because the retailer needs to order up to six months in advance, it can only order once, as products rotate quickly. As such, the model needs to output one single value that encapsulates the total demand for each product’s lifespan. It doesn’t need to be precise on a more granular level, such as weekly or daily.

Because the retailer knows some of the feature values at the time the orders are made — such as placement and product characteristics — they can be used in the forecast to predict demand. For those feature values that are unknown at the time of ordering, such as advertising campaigns, it can either build individual scenarios or use averages, for example by assuming a product will be, on average, advertised during a certain period of time.

The retailer expects high substitution among the products; in other words, the demand for one product will depend on the number of other, “similar” products available at the same time. It could subsequently decide to create such a feature (i.e., number of similar products available at the same time) by building a similarity measure among the products in order to learn how it has impacted demand in the past.

If the retailer wants to predict one single value per product, the use of regression analysis is appropriate. Depending on the type of data, (e.g., numeric or text), volume, collinearity, etc., it can leverage generalized linear models, random forest, gradient boosting, or neural networks.

For its similarity measure, the retailer can either use the existing product categories or build a clustering model. Or it can try both and see which one is the most significant.

The retailer expects some non-stationarity, as certain products can be trending. Hence it can build an overall trend prediction model based on external data — for example by learning the relative sales of a certain type of product based on the number of tweets mentioning it — and use it as another feature in the final model. Or it can build a separate model and stack the two together.

For the optimization part (forecasting how much of each product should be ordered), the retailer knows that the demand will not be perfectly forecasted. So, it needs to either account for a specific strategy — we can buy a bit more, or we can buy a bit less — or build a stochastic optimization model to decide how much to purchase based on the cost of under- and over-buying. (See Exhibit 3)

Let’s now consider the case of a steel manufacturer that produces several types of products for large clients in the automotive, construction, and packaging industries.

Each of the manufacturer’s products has its own specifications (e.g., grade, thickness, purity), but they share the same raw material input and some similarities in the production chain. It has several assets along that production chain tasked with various processes, such as hot rolling and galvanizing, but each of the assets has a constrained capacity.

The manufacturer’s most profitable clients expect a quick turnaround when they place orders, which is much faster than the lead time needed to actually produce those orders. As such, the orders have to be anticipated and scheduled in a smart way. (See Exhibit 4)

Exhibit 4: Three stages at which demand forecasting is needed to optimize a steel manufacturer’s supply chain

Because the manufacturer has clients from different industries, whose demand for steel is largely driven by their own commercial activity, it makes sense to use a time-series model that includes their specific business drivers, such as new car production, new construction projects, and seasonality.

Also, because the necessary lead time is long while the order delays can be short, a demand forecast needs a long time horizon — one at least equal to the lead time.

While the products are different, some share similarities regarding production, hence the demand has to be forecasted at different stages: both demand for final products as well as demand for each of the intermediary products.

And finally, because the production capacity is constrained, demand cannot be forecasted by directly linking to actual past sales. Rather, past demand has to be extracted from the order book.

For each client and each product, the manufacturer will want to forecast a time series of weekly demand that will cover an entire year, with less accuracy needed over time.

The time series’ main drivers will be signal features (e.g., trend, seasonality) and external drivers; for example, when it comes to automotive clients, external drivers would include the growth of the automotive industry as well as those clients’ own financials. The models the steel manufacturer could use would be ARIMA or LSTM, maybe stacked with a feature-based regression model for other drivers.

Since the different products share some similarities, it would also aggregate each of the products’ demand at each step of the production chain along with the capacity of each asset in order to understand what order volume would be feasible.

Then, given the demand probability for each product and each client, the corresponding price, the current stock, the current maintenance plan, and other potential business constraints, it could perform production optimization to decide which material should be produced (including intermediary products), what levels of stock will be needed, which orders can be filled and which orders have to be delayed or refused, etc. (See Exhibit 5)

As we’ve seen, demand forecasting is a large class of complicated analytical problems, where multiple models can be used depending on the business context and needs. Here the role of the data scientist and their ability to translate an organization’s business needs into data science, pick the right algorithms, and implement them correctly is key.

That’s because demand forecasting is a complex challenge, one that requires significant knowledge in order to determine which of the various models can and should be used for the situation in question. Among them:

· Potentially NLP or image processing, if the input data is in a non-structured format

· Optimization models to translate the demand forecast into actual decisions (e.g., linear programming, genetic algorithms, stochastic optimization)

Indeed, there is no single demand forecast model that will work across use cases. Which is why when demand forecasting is business critical, the role of the data scientist is, too.
=========== 7 Stories Behind the World’s Most Popular Machine Learning Algorithms ===========
The world of AI/Machine Learning is evolving fast. If you’re like me, keeping up with the latest developments can seem like trying to reach a destination while walking on a treadmill. Sometimes it’s worth it to just step off, pause, and look back at the origins of many of the paradigms and algorithms that got us to where AI is today.

We are very fortunate that many but, sadly, not all, of the inventors who shaped AI are alive today. It can be inspiring (if sometimes intimidating) to hear about pivotal moments in the field from the very people who made them so significant. To that end, I’ve included the following seven videos (taken from past interviews and talks) because of what we can learn from these luminaries of our profession.

Together, the videos shine a light on the history of these algorithms, particularly on specific problems these researchers were trying to solve. Their solutions eventually led to the invention of the algorithms themselves. This glance at the past provides a deeper understanding of the methods and suitability of the algorithms for different applications.

The videos also give us a glimpse into the thought processes behind these inventions. An understanding of these mental processes might, in turn, help us apply these similar processes to solve the problems our field currently faces.

Finally, the videos provide an entertaining history of the development of the algorithms, analogous to the way the “origin stories” in comic books help readers understand the “back story” of popular heroes and heroines.
=========== Using deep learning to predict not just what, but when ===========
Using deep learning to predict not just what, but when

Deep learning/machine learning/AI is increasingly being used in business to predict customer behavior such as purchasing. In addition to improving operational efficiency, such as with inventory management, accurately predicting customer purchasing behavior helps organizations improve brand engagement, cross-sell, upsell, optimize pricing, and prevent churn.

But current approaches focus solely on what customers will buy. They fail to consider both what and when customers will buy it.

The ability to predict what a consumer will buy next is useful, especially when it comes to estimating customer lifetime value (CLV). There are several approaches that can be used when determining CLV, among them recency, frequency, monetary (RFM) analysis and beta geometric/negative binomial distribution (BG/NBD). However, none of the current methods capture the richness of customer transactions. That’s because they reduce the information to a set of parsimoniously parameterized probability distributions, such as the mean and standard deviation of a purchase rate.

Moreover, because they look at average — not individual — customer behavior over a period of time, they are unable to determine when, exactly, a customer will buy an item. To enable that level of hyper-personalization, we need to leverage deep learning.

At BCG Gamma, we’ve recently patented (US patent number 10,002,322) a next-gen forecasting and personalization model, which we call “Crystal,” that uses deep learning to predict both what transaction will be made and when — to within a time frame of a mere few hours. (See Exhibit 1).

To build this model, we took publicly available data and applied long short-term memory (LSTM), a framework whose key differentiator is that it remembers what is important and what you need to remember over the long term and also notes what is irrelevant, not useful, and can be forgotten in the short term. Among the applications LSTM is used for is to formulate how we learn languages, by predicting the next word based on the previous words used.

We adopted the LSTM-based language modelling work and applied it to transaction history by treating transactions as a sequence of words, with single words corresponding to single transactions and sentences to series of transactions. We then used LSTM to predict the next transaction based on previous transactions. (See Exhibit 2). And because deep learning is used to find and “learn” non-linear and rare relationships in transaction history, Crystal understands non-linear trends and one-off behavior, which means it doesn’t just recommend the most commonly purchased items.

Exhibit 2: Crystal uses the language modeling LSTM to predict transactions

More importantly, Crystal extends the language learning framework to predict the when in addition to the what. It learns the eccentric buying patterns of consumers to predict when — to within a few hours — they will make a purchase next.

Companies that can leverage the ability to predict both the what and when of future transactions aren’t limited to retailers; they can come from industries such as consumer packaged goods, industrial goods, as well as banking (to analyze spend patterns on credit card transactions) and energy (to track and predict consumption). It is not a solution for every industry; it won’t work in insurance, for example, where the transaction patterns are far too few.

Moreover, it is not a plug-and-play solution, but one that must be customized to the consumer consumption or transaction history use case in question. Once customized, however, the ability to predict both the what and the when of future transactions enables multiple use cases: from getting customers to purchase more expensive/higher-value products (product mix) more often (frequency), to decreasing churn by targeting customers with a high propensity to leave the customer base (churn prevention), to improving internal costs through better optimization (operational efficiency) and receiving a better ROI on coupons, discounts, and rewards offered (promotional efficiency).

Crystal can predict what transactions will take place with a more than 95% degree of accuracy. And it can predict when those transactions will take place to within just a few hours. (See Exhibit 3).

Algorithmically, Crystal is similar to CLV models like RFM or BG/NBD. But while they try to reduce a customer’s transaction history to just a handful of parameters, Crystal takes a more longitudinal approach, learning from the richness of long transaction histories and using that information to make predictions with high latitudinal precision. Think of it in terms of the law of averages vs. learning from the specifics.

When it comes to recommendation systems, the first question most companies ask, especially retailers, is how well do we know our customers? Because the better a company knows its customers, the more empowered it is to offer them the most personalized recommendations around what to buy.

But knowing what a customer will buy is helpful only to a point. By using a customizable deep learning model like Crystal, companies can also know when they will buy it.
=========== Shuffling Paragraphs: Using Data Augmentation in NLP to Increase Accuracy ===========
While data augmentation is increasingly being used in machine learning to train models to classify images, when it comes to natural language processing (NLP) and text analytics, its application is still limited.

That’s partly because data augmentation is relatively new. But most importantly, it simply hasn’t been explored much in the text analytics space yet. It’s also less intuitive to do in NLP than it is with images.

But it is possible and in practice, deeply impactful. In this article, inspired by fast.ai’s Practical Deep Learning for Coders, a 7-week course taught by Enlitic founder and former Kaggle President and Chief Scientist Jeremy Howard, I share my hypothesis that we can use data augmentation in NLP to make a model significantly more accurate.

Let’s first consider the challenge of classifying specific images.

Say we want our model to be able to correctly classify images of cats and dogs, but we only have a limited number of labeled images of cats and dogs. How can we create more to better train our model? And how can we make sure our model doesn’t just learn to recognize specific cats and dogs from among the few examples that we do have?

To create more images, we could modify slightly the existing ones. For example, we could flip an image of a dog horizontally, shift it slightly to the left or to the right, or zoom in or out. (See Exhibit 1.) We could also combine those modifications in various ways.

Each time we make one of those modifications, we are effectively “augmenting” our dataset by generating more examples of cats and dogs that we can feed into our model to make it “smarter” (i.e., better at classifying images of cats and dogs).

But there are limits to this approach. If we take that original dog image and flip it vertically, for example, we now have an image of an upside-down dog. And because we don’t have upside-down dogs in real life, that image would “confuse” our model.

Text, like a picture of a dog, cannot just be indiscriminately flipped and shifted.

If we did indiscriminately flip and/or shift it, we would end up with examples that would confuse our model, not make it smarter. Think of trying to train a model to classify movie reviews using both the review text and the same text written in reverse. It would make no sense.

So, while we can use data augmentation to get more text samples and improve our NLP models, we must first consider the message the text is conveying and the format in which it’s being presented.

Let’s take an example of a document with a specific format, like a résumé.

A résumé is usually laid out in sections, such as personal information, work experience, education, interests, hobbies, etc. (See Exhibit 2.)

From a content perspective, those sections are independent from each other. In other words, each section would convey the same message and meaning even if the order in which they appeared in the document changed. If we shuffled the paragraphs in a résumé, we’d get another résumé conveying the same message.

More importantly, for the purposes of training our model, each shuffled résumé would preserve the label we gave to its original version (e.g., a good resume).

Say that we have identified 100 résumés as representing a “candidate we want to talk to.” A résumé with its paragraphs shuffled around would be still classified as representing a “candidate we want to talk to.” That’s because the order of the paragraphs on a résumé does not change the way we would classify it. Put another way, we would talk to a good candidate regardless of whether she listed her education before her work experience or vice versa.

And as I learned when I tested my hypothesis, even a rudimentary and random shuffling of résumé sentences will create thousands of “artificial” résumés that can be fed into our machine learning algorithm — and improve our classification accuracy by a striking 10 percent!

This approach could easily be applied to a real-world hiring situation. Since organizations don’t typically have thousands of résumés on hand to train a model, they could use this augmentation technique to create a machine learning classification to prioritize the efforts of their HR team, saving both time and money during the hiring process.

As the résumé classification example makes clear, there are two components that we need to consider when using data augmentation in NLP to improve the accuracy of a machine learning model.

1. The context in which the augmentation technique is used. To fully harvest the boost to our NLP model, we need to base our data augmentation design on a deep understanding of our document’s structure and content.

With the résumé, the fact that the information contained within the different sections isn’t altered by being moved around is critical. That same data augmentation technique could never be applied to a format where the meaning of the text depends on its order, such as a novel. The application of data augmentation must be specific to the context in which it’s being used.

2. The augmentation technique being applied. The impact of applying data augmentation will depend on the specific technique itself. Every technique can potentially induce the machine to learn something different and in turn, generate a different impact.

Once we have a deep understanding of our document’s format and the information that it contains, we can experiment with different augmentation techniques to find the one that most meaningfully impacts our issue.

Although data augmentation for NLP and text analytics techniques are probably underdeveloped, they are worth pursuing as they can quite substantially boost our NLP model performance. To fully harvest their potential, however, we must base our data augmentation design on a deep understanding of both our document’s structure and content.

This technique has been tested on a dataset of about 450 documents 55% of which are labeled “positive” and 45% are labeled “negative”.

Because of the structure and semantic of each of those documents, paragraphs can be assumed to be independent of each other.

After several training cycles, a DNN with (not pre-trained) word embeddings performed correct classification of about 68% of the documents in the test group.

The same DNN classified 75% of the documents correctly when applying the data augmentation technique discussed in this paper (~10% increase).
=========== Beyond the ROC AUC: Toward Defining Better Performance Metrics ===========
Data Science is a powerful tool to create new services and improve business operations. Its list of potential applications is virtually infinite, and includes such services as demand forecast, store-localization optimization, industrial-process improvement, marketing personalization and the subject of this article, performance metrics measurement.

The success of a project depends to a large extent on finding the right performance metrics at an early stage of the project. These metrics are used not only to evaluate the project at its conclusion, but to set a target and drive choices, big and small, made throughout the project.

Any bias, such as data acquired in different conditions or discrepancies in class distributions, will affect the relevance of the performance evaluation. So too will the amount of test data needed to obtain precise performance. The number of significant digits in the error rates depends on the number of actual tests that each digit represents (look for the famous n=30 rule of thumb).

This article is focused on the process of choosing performance metrics. We will not discuss the need for appropriate data for performance evaluation. All test data should reflect operational data as closely as possible.

There are three main properties to consider when defining the performance metrics, presented here in decreasing order of importance:

1. Relevance: The set of performance measures chosen should reflect all the aspects of the business problem. This enables you to make the link between “evaluation of the project’s output is good” and “business value is created” as close as possible. Choosing an incorrect or incomplete set of performance metrics can lead to situations in which a project is deemed a “success” based on its performance evaluation but is, in fact, completely unusable.

2. Understandability: The metrics should lend themselves to clear interpretation. They should make it as easy as possible to answer the question: “Is a value X for the indicator Y a good result?”

3. Compactness: The fewer values in the performance indicators, the better. This property is often in contradiction with the previous two properties. Building aggregated metrics is a way to obtain compact metrics, but such metrics can reduce relevance and understandability.

In this article, we will focus on two data science projects in which input data is used to produce a performance score. The score is then compared to a threshold in order to make a decision.

· Project 1: Churn. A company wants a list of its highly likely-to-churn customers to serve as the basis of a preemptive action. A performance score is computed for each customer. If the score is higher than a given threshold, the customer is put on the action list.

· Project 2: Facial recognition. An automatic boarding-control gate is installed in an airport gate to permit or prevent boarding. A similarity score is computed between each passenger’s face and his or her passport photo. If the score is higher than the given threshold, the passenger is granted access to the airplane.

Many performance measures are derived from the ROC (receiver operating characteristic) curve that most data scientists are familiar with. (Other graphs based on type I and type II errors include DET curves, Precision-Recall, and FAR/FRR graphs). Often, the information in the ROC curve is summarized by:

· The Equal Error Rate (EER) which corresponds to error rate at which the False Negative Rate equals the False Positive Rate, or

· The Area Under the Curve (AUC) which corresponds to the surface below the ROC curve.

These measures provide a good indication of the global performance of the methods generating the scores. They are not useful, however, for distinguishing between methods that are comparatively better at low or high False Positive Rates.

For example, the methods a and b whose ROC curves are shown in Figure 1 have the same EER (17%) and the same AUC (0.91). However, they exhibit very different behaviors, which can have a significant impact on their use. Consider Project 1: Churn.

· Imagine that the preemptive action to be taken for customers on the list has a low cost, such as a phone call to the customer. The company will be willing to accept numerous false positive errors in return for maximize the number of highly likely-to-churn customers it reaches. In this scenario, Method a is better because the company will take this low-cost action to reach nearly all (95%) of its potential future churners. In exchange, they will have to call only 26% of probable non-churners. Should they choose Method b, the company would have to call 60% of probable non-churners. By choosing Method a, the company can focus its efforts on the most appropriate target audience.

· What if, on the other hand, the action has a much higher cost, such as giving generous discount to potential churners? In this scenario, the company will want to strongly limit the number of discounts it gives out to non-target customers. By choosing Method b, the company can reach 74% of highly likely churners, while giving unnecessary discounts to only 3% of probable non-churners. Should the company chose Method a, it will reached less than a third (30%) of probable churners.

The point of these examples is that, instead of using EER and AUC, a better way to find the best performance methods is to set the True Positive Rate or False Positive Rate based on one of the business aspects, then find a method that optimizes the other. The portion of the ROC curve that should be of interest in a given situation totally depends on the actual business cost (such as money, security, or comfort) of a false positive error and a false negative error.

Another way for the company to meet its goal might be to optimize a weighted sum of the False Positive Rate and False Negative Rate with a weight derived from the relative cost of the two error types. Because this second solution if often less easy to interpret, the former is generally preferred.

Selecting the most appropriate performance measurement method at the operating point of interest is good, but not sufficient. We also need to be able to set the threshold to reach this operating point. This calibration is fairly easy when the data is homogeneous, but reality is often more complex than that.

Consider Project 2: Facial recognition. In this use case, a false positive represents a security threat: Someone will be able to board an airplane by faking his or her identity. As such, the threshold should be set so that the False Positive Rate corresponds to an acceptable value. The problem is that most face-recognition algorithms behave differently when analyzing people of different gender, ethnicity, or age groups. Given this inconsistency, it is quite likely that even if the exact same system is installed in different countries, it will return different False Positive Rates for members of these different groups. This discrepancy cannot be detected by looking at global performance on a test set. We need evaluate performance independently for each group, using the same threshold, and then observe the distribution of the False Positive Rates. The variance of this distribution should also be included in the set of performance metrics of interest.

The consequence of a facial recognition False Negative Rate may be somewhat milder than that of a False Positive Rate. When someone is denied boarding at an automatic gate, airline staff would have to personally check the traveler’s passport. This may seem like a minor inconvenience, but consider what would happen if the system had very different False Negative Rates for members of specific groups. Imagine the rejection rate averages 2% overall, but jumps to 30% for Asian women or black teenagers. Affected groups would understandably accuse the system of being racist and demand its deactivation. It may be beneficial to observe the distribution of false negative as they vary across the different groups, as well as the difference between the global and group-based False Negative Rates.

In this case, as in most business cases, it is critical to not just choose the best method for measuring performance, but set thresholds that will achieve intended positive outcomes without generating unintended consequences.

· It is crucial to the success of any project that time be taken at the start to find performance metrics that can be used to guide choices throughout the project, and then to aid analysis during project post-mortems.

· The usefulness of performance evaluation metrics is highly dependent on the specifics of the use case. What may be optimal for one situation may be less so for another.

· The design of performance evaluation metrics is a broad topic. The two examples presented in this article present just some of the factors that must be considered when choosing a performance metric.
=========== Deriving true probability from an ensemble classifier ===========
This article is co-signed with Antti Niskanen

One of the most common uses of data science in a business context is to predict binary outcomes, as they give organizations the opportunity to take action before it’s too late.

For example, a company might look to predict churn so that it can take steps to retain its customers. Churn is of particular interest in industries like banking or insurance, where the cost of customer acquisition tends to be high compared to the cost of retention.

To efficiently target and size the steps it would need to take, a company would need to compare the expected loss from churn — the value at risk, or VaR — against the cost of mitigating that churn. The VaR in this context would be the probability of an individual customer being a churner in a given period multiplied by the expected revenue (or profit) generated by that customer if he does not churn.

A subscription-based company like a telecom provider, for example, may want to offer free service to retain its existing subscribers. If it predicts a 50% probability of churn for a given customer, it could give that customer three months of free service without having the loss of subscription fees impact its bottom line. But if the probability of churn is only 20%, eliminating that customer’s subscription fees for three months would lower its profit.

There are a lot of models available to predict binary outcomes. Ensemble methods such as random forest (RF) are popular because they are easy to implement, do not require a lot of feature engineering, and can model the data using non-linear functions.

But even if they are very good at ranking observations from the most probable to the least probable positive outcome, one drawback of ensemble methods is that they don’t predict the probability of being one or zero, but rather just a score that has no statistical interpretation. While such a score is good enough to plot an ROC curve and show fancy AUC, it’s insufficient when it comes to providing real businesses with usable insights. In order to act wisely, an organization would want to know the actual churn probability.

Luckily, very simple Bayesian logic allows us to transform any score predicted by an (ensemble) classifier to a probability that can be used in a business context. All we need is a prior on the distribution of positive and negative outcomes for the sample population, such as with this formula:

The most straightforward priors would involve setting P(1) and P(0) to be equal to the observed shares of positive and negative outcomes from the training set, which in practice would be equal to the current churn rate. Then the conditional probability density could simply be derived from histogram plots of the two categories.

The following code takes the classifier output probabilities as input along with the ground truth values and returns the “true” probabilities. The output probabilities can be obtained by utilizing the “predict_proba” method in the popular sklearn package. In pyspark.ml the pseudoprobability score can also be obtained.

There are alternative methods available to calibrate the probabilities given by a classifier as discussed in Niculescu-Mizil and Caruana [2005]¹, some of them available in the calibration module of scikit-learn².

To illustrate that this type of a Bayesian check is indeed worth performing if you intend to interpret the classifier scores as probability in your business application, let’s construct a simple example where we can see a random forest both succeed and fail miserably at predicting the class probability, even though the data set is as simple as it gets.

Going back to our telecom provider churn example, we might want include in the predictive model all of its customers’ characteristics (subscription type, contract duration, current usage or amount paid on top of it, etc.) as well as external factors such as seasonality, competitive offers, etc. It’s easy to think of hundreds of factors that might be relevant, and it might be hard to choose between them a priori. This is where ensemble methods are especially convenient: if your data set is large enough, adding a lot of features — even with weak predictive power — rarely alters to a significant degree the quality of the prediction. But as the example below makes clear, there is a drawback: it will greatly affect the out-of-the-box predicted probability of the classifier.

Suppose we have data from an underlying two-dimensional normal distribution. (See Exhibit 1). The two classes — 0 (no churn) and 1 (churn) — have slightly different means (the data is balanced in this illustration but the method is robust for unbalanced data sets). There are also 48 other dimensions with just Gaussian noise, which we know doesn’t correlate with class.

What happens if we train a model with just the two leading dimensions (where all the signal is), and then with all 50? As expected, both seem to work well: we get nice ROC curves and AUCs of 0.77 and 0.75, respectively, even though the data sets seem to overlap a lot. According to Exhibit 2, there’s no indication that something is wrong. Granted, handpicking the leading dimensions seems to result in slightly better performance, which in many applications is worth money.

But what about the relationship between the classifier score and the corrected Bayesian estimate? As we see with the two leading dimensions in Exhibit 3, the probability correction is relatively minor and wouldn’t lead to wrong conclusions in churn modelling or other business predictions such as purchasing behavior.

However, the higher-dimension example is way off in terms of the classifier score. But once we fix that, we can use the corrected values instead.

Would we be able to see this from the ROC curve? Not from that curve alone, but we can easily see that P(RFscore|class) is in fact equal to the negative derivative of the true/false positive rate with respect to the classifier score. Having the parametrization of these rates would work just as well. The parametrization is clearly needed to be able to produce a plot like that shown in Exhibit 3.

We can therefore conclude that it is worthwhile to be suspicious of classifier output probabilities. That’s especially true if you’re using tools out of the box, be they closed or open-source, since the scores that are outputted by the implementation of RF in Python, Spark, or R might differ from one another. You need to check the math. Luckily, our favorite Bayes rule can come to the rescue.

What about business impact? Over- or underestimating the churn probability, as detailed above, can have a significant financial impact. For instance, if we decided to offer discounts based on a churn probability that is overestimated, that could result in shrinking margins. On the other hand, underestimating churn can lead to insufficient action, as illustrated by the pattern in Exhibit 3.

Indeed, if the raw output of the RF yields a churn rate (probability) of 70% when the reality is actually closer to 90%, that’s a big — and unnecessary — gap. And one that will cost a company a lot of money.
=========== Manufacturing Analytics: The 20% Conversion-cost Opportunity ===========
Manufacturing lines convert resources such as materials, energy, time as well as machines and workforce into finished products. This conversion takes place either through a sequence of discrete process steps, such as the assembly of parts along stations on the line, or continuously by means of physical-chemical processes, as in the production of pharmaceuticals.

Manufacturing lines can be seen as an implementation of a solution to a complex problem: namely to produce thousands of different products efficiently and reliably on one and the same line. Each product is the result of a series of specific tasks and operations that must be carried out sequentially by machines, by man and, often, by a combination of the two. Each step in the process must be reliably carried out in a precise way and time to meet customer requirements, delivery schedules or industrial quality standards.

For more than a hundred years, manufacturing lines have been setup linearly as a sequence of stations at which workers or machines carry out specific tasks (see Fig. 1) But how good of a solution is the line to the production problem? To find improvement opportunities, manufacturing analytics taps into the high-dimensional space of data from materials, machines, workers and products. This data describes how these resources are utilized and converted step by step into products of certain quality. The ideal data foundation contains the records of all events that take place along the production line, and provides end-to-end traceability, capturing the “when” and “where” and “how” of each and every product that is made. Increasingly, manufacturers collect this data in clouds. More often however, it still resides in separate MES, ERP and quality systems.

Analytics generates value only when it changes business decisions or actions. Manufacturing analytics can do this by improving the utilization of critical resources, which, in turn, improves key manufacturing performance metrics such as Operating Equipment Efficiency (OEE) or Right-First-Time (RFT).

There are essentially two classes of manufacturing analytics applications. A first class addresses the process of planning the use of resources such as machines, materials, time and workforce. Here, analytics determine how much of and when each resource will be needed to make a certain quantity of product. This class provides production forecasts and production plans, as well as production schedules and the sequences in which a certain product mix will be made. A second class of applications determines how these resources will be converted into products by maximizing resource-efficiency and product quality.

1. Production Forecasting and Planning: The Importance of Accuracy

The first step toward optimizing resource utilization is to know how much of a given resource will be needed. The key to this lies in understanding future customer-product demand and deriving accurate production forecasts. Different products are manufactured in different ways, with different tasks and steps, often with varying degrees of difficulty for the workers. Hence, accurate understanding of the upcoming product mix is fundamental to determine the optimal configuration of the line, and to make the right amount of resources and inventories available at the right time. Many companies continue to rely on best-guess and incomplete manual forecasts from their sales teams, who rarely make demand forecasting a priority task. From the start, this lack of accurate, forward-looking product-mix visibility undermines the ability to configure the manufacturing process for optimal efficiency, throughput, and quality. The ability to accurately forecast demand and production depends to a great degree on the industry and product in question. Applicable analytics methods can range from time-series forecasting to feature-based predictions. Which one is best suited depends to a large extent on the time horizon and context.

For short-term forecasting of product demand, ARIMA and related models such as UCM can often provide a good starting point (The article “The complex challenge of demand forecasting” elaborates further on the methodology).

A leading global cement manufacturer used to rely on manually created demand forecasts its salesforce would periodically generate by calling hundreds of customers and asking them for their upcoming needs in terms of tons of cement. This data would then be manually aggregated and split down by cement plant to deliver the foundation for plant production plans. This process, in addition to tying up significant resources, proved to be highly inaccurate and resulted in cement plants incurring multimillion losses a year. Cement demand follows strong cyclic patterns, with short-term correlations and dependence on external factors such as rain fall, temperature and GDP. Modelling these factors in an ARIMA-type statistical forecast model has improved the forecast accuracy on more than 90% of all cement types. At the same time, it has decreased forecasting errors by more than half, achieving greater than 90% accuracy on large volume products. For this client, the use of analytics improved production and energy planning, and enabled it to accurately schedule yearly maintenance shutdowns into low-demand time windows. The resulting benefits added up to €800K per plant and year. Furthermore, these statistical forecasts established a reliable single-point of truth, accepted across the commercial and production sides of the organization and with consistent stakeholder-centric demand-views, including by plant, sales representative, area, region, and country. These statistical-demand forecasts therefore have also improved the overall Sales & Operations Planning process.

Switching production from one product to another, known in the industry as making a “changeover”, can involve the time- and resource-consuming exchange or re-configuration of machines, tools and production lines. This unproductive time can account for several percentage points of reduced machine utilization and line downtime. In non-optimized operations, as much as 50% more changeovers can take place than necessary, representing a staggering loss in productivity.

Manufacturing analytics can minimize these efficiency losses by helping to optimally schedule and sequence the production of different products. The goal is to minimize the number of unnecessary changeovers, taking into consideration factors such as product demand, inventory levels, and availability of resources. The schedule and sequence optimization then frequently falls into the well-studied class of computer science and operations-research problems known as the “Job Shop Problem” or, more generally, “Resource Constrained Scheduling Problems”.

The pressing facility of a parts supplier can produce more than 400 different parts. While every part requires a specific press in order to deliver the desired part shape, the production line can carry only 40 presses at a time. The product mix ordered by the customer, on the other hand, can vary a great deal more. The manufacturing challenge is to configure the line correspondingly and dynamically to deliver the ordered products in the most efficient way possible. Replacing a press takes about 3 minutes and can take place as many as 30 times a day. Hence, the schedule for each press, which defines when and where each press is placed on the line and when it is taken off the line and replaced, must meet customer demand — while minimizing the number of times the line needs to be stopped to replace one press with another. Given the combinatorial complexity of the task, it should come as no surprise that the manual press-schedule developed by the client required twice as many change-overs than the schedule optimized using manufacturing analytics, which delivers 1% less line downtime.

Frequently the line setup, which defines the work and tasks allocated to each station, is not well adapted to the actual mix of products the line has to produce. This leads to systematic, “built-in” structural line inefficiencies and bottlenecks that prevent workers from completing their tasks at their stations “in-takt.” These bottlenecks, in turn, force downstream workers to wait for upstream tasks to be finished before they can proceed.

Analytics can help optimally balance a line by simulating both the dynamics on the line and individual stations as a function of incoming product mix and sequence. Discrete-event simulation and Monte Carlo methods can evaluate different line-setups and provide both tactical information, such as where and when bottlenecks on the line occur, and strategic information, such as whether a fixed conveyor, a decoupled conveyor, or even a network-like arrangement of flexible manufacturing cells (FMCs) is the better option. With this knowledge in hand, managers can understand the drawbacks and benefits of different options, and setup the line correspondingly. This might involve removing built-in bottlenecks by rebalancing workload, or making a greenfield project decide between conveyor and FMCs.

A recent BCG study (“Will Flexible-Cell Manufacturing Revolutionize Carmaking” https://www.bcg.com/publications/2018/flexible-cell-manufacturing-revolutionize-carmaking.aspx) showed by means of simulation that, particularly in the presence of high product variety and complexity, Flexible Manufacturing Cells can yield as much as a 12% increase in worker utilization, and significantly higher throughput compared to conveyor belts. On conveyor belt lines, for example, workers frequently wait for cars to arrive from upstream stations. With FMCs, it is the parts that wait for the workers to become available. This is because longer process times at one FMC do not slow down or stop the entire assembly process at other FMCs. These situations are unavoidable with conveyor belt lines due to their rigid linear setup.

Across industries, manufacturing requires manual completion of specific tasks, especially in discrete assembly. Each worker has different skills to carry out certain tasks, derived from his or her experience. Some workers are faster and more reliable, while others tend to have a higher rate of line-stoppages or defects depending on the product and station which they are working at.

To turn these variations into an advantage, managers must first recognize the high degree of diversity between products, tasks, and skills, in particular when product mix varies over short time scales. With this knowledge, managers are able to allocate workers strategically to tasks and stations. More often than not, though, managers base these line staffing decisions on other criteria, and rarely on actual performance data, as they should.

One client’s car seats factory had a five-fold variation in worker performance on the same tasks, and line-efficiency and defect rates that were highly dependent on seat-mix and worker-station allocation. Predictive models were trained on historic data from defects, line-stops, seats and staffing configurations to calculate the probability of line-stoppages and defects for a given staffing configuration and incoming seat-mix. By simulating different staffing configurations, the “best-fit configuration” for each shift was determined, which the shift-managers could use during the shift briefings to advise workers on their tasks. Use of the configuration has reduced line-stoppages caused by workers by a full 50%.

The data used for staffing optimization can also serve as a foundation to identify specific workers’ weaknesses and pair them with appropriate training. Compare this to the norm, in which training needs are usually discovered on an ad-hoc basis. Assignment of training usually comes down to a manager’s ability to identify problems on the line, even as he or she deals with other pressing issues. The inability to pinpoint the need for training not only harms efficiency and quality, but reduces management’s ability to leverage training and performance as incentives for workers.

Using the line’s production data and shift plans, manufacturing analytics can help determine an individual worker’s contribution to quality and efficiency while he or she is performing certain tasks on certain products. This insight provides very specific and individualized training opportunities. (Note: The ability to take full advantage of the data to help specific workers improve their skills depends on a company’s ability to tie performance data to individual workers. Such use of personalized data is typically determined by legislation.)

Delivering quality throughout the manufacturing process is equally as important as efficiently using resources. In some factories we have seen, the rate of defective parts was so high — as much as 30% — that the factory spent almost as much money making defective products as it did making quality products. The factories would then have to commit even more resources to repair or scrap these defects. Clearly, understanding and correcting the causes behind defective parts can provide significant cost savings.

Especially in the process industry, resources undergo complex physical-chemical processes that determine the physical properties and quality of the finished goods. In practice, despite the complexity of these processes, the production parameters (“recipes”) that define how these resources are used are often determined manually over many months by trial and error. Determining these recipes normally relies heavily on expert knowledge. Even so, the resulting recipes are rarely optimal.

By combining the recipes of different products and their resulting quality and defect rates, manufacturing analytics can improve quality delivery. The first step is to develop a model that predicts how the quality of the finished product depends on the parameters of production — that is to say, on the recipe. Once this model is in place, methods such as simulated annealing can help find the best parameter combination that minimizes the defect rate. This process can help define new and improved set-points for production, and massively shorten the industrialization phase.

A global PU player client makes foam parts by injecting a mixture of different polymers into a mold. Each of the more than 350 part-types has a specific injection recipe that determines, for each of 6 polymer components, the proper shot duration, flow speed, temperature, pressure and other physical parameters of the injection and mixing process. Overall, however, approximately 25% of the parts produced have at least one defect, such as air pockets, that requires the part to be either reworked or scrapped. Using a gradient-boosting algorithm that was trained on more than 1.5 million parts, the client is now able to predict the defect rate for a given part and a given recipe. By searching the parameter space for those parameters that minimize defects, the quality manager can now identify better recipes. This revealed an overall defect-reduction potential of one third.

7. Yield Maximization: More with less

Many industries require large amounts of energy and other expensive raw materials. A few percentage points worth of consumption savings on these resources can translate into multi-million-dollar PnL savings for a single plant or plant network. Despite the complexity of having to configure more than 20 parameters on large machinery, the critical operating set-points that determine consumption are still often based exclusively on expert knowledge.

Predictive analytics can inform on the hypersurface of potential set-points and their energy consumption and quality. In fact, frequently solutions exist which lead to a similar outcome, but require different amounts of resources. This also applies, for example, to the rate of wear of spare-parts. Machine operators can use this information to make better decisions on set-points and maximize the yield from precious resources.

An Asian copper manufacturer we have worked with was struggling with declining copper yields in its smelters. The company’s converting furnaces are fed melted copper matte, which contains mostly copper and iron sulfide. By blowing oxygen through the matte, the furnace converts it into slag and blister, which contains copper and sulfur dioxide. Ideally, all the copper is contained in the blister. The client however, was losing a significant amount of the copper (2%) into the slag. The entire process involves more than 100 parameters including raw material mix, oxygen injection, and other factors. The company’s engineers had not been successful at modelling the process to explain — and avoid — the loss. A neural network, in turn, successfully integrated the dependencies between parameters of the process to predict copper loss. By simulating multiple parameter settings on the neural network, the client was able to determine better operating set-points which reduced the loss almost entirely — and to experience a 2% increase in revenue and 1% increase in profit.

8. Alerts and Previews: Using History to Predict and Prevent Risk

Beyond strategic applications for analytics, there are also powerful tactical applications. For example, predictive risk alerts can help mitigate short-term efficiency risks that can account for up to 30% of efficiency losses. These alerts can help avoid unexpected line stoppages or delays in task-completions by workers. They can also reduce quality risks by alerting managers and workers to specific events, such as when a sequence of particularly complex products comes on the line.

Analytics can help managers understand which production patterns correlate with efficiency drops or defects, and predict the probability at which either of these will happen. By combining the configuration state of the line with a production forecast, these probabilities become forward looking. When, for example, the defect probability for a certain product at a particular station and time in the future exceeds a threshold, a predictive alert can be elevated to operators or line managers. Visualizing these alerts and probabilities forward over time is a powerful way to preview when and at which station on the line risks will appear. With this forewarning, shift teams have the ability to prepare for — and prevent — these risks.

The factories of our leading car-seat manufacturing client can produce more than 2,000 seat variants on the same line. When the factory’s customer, a car maker, sends a seat order to the factory, the factory will produce these seats just-in-time over the next 240 minutes. This puts enormous pressure on quality and efficiency, as penalties for delays or inferior quality are high. Working with the client, we built a predictive alert system to provide a production-risk preview and advise shift and line managers about which seat at which station and at what time will have a high risk of line stoppage or defect. The model is a Random Forest and combines current worker-station allocation, more than 300 features per seat, and the incoming seat order. By continuously displaying its output above the production line, the model helps managers anticipate and mitigate these incoming risks.

Get Analytics in Place and Manage the Change

The opportunity to fully leverage manufacturing data and analytics is within the grasp of many companies. It comes closer still with increasingly inexpensive and accessible Internet of Things (IoT) devices. With this technology so readily available, many BCG clients now seek advice on how to fully exploit it.

The above-mentioned solutions are good starting points for the ideation and prioritization of high-impact analytics applications. While the examples are neither exhaustive nor applicable in each and every manufacturing context, they outline some of the most promising areas for the use of analytics to deliver high value.

The details of a manufacturing analytics application will always depend on the specific industry, product and process involved — and, often, on the individual factory itself. No off-the-shelf solution could deliver this level of customization. Even when companies focus on the right high-value opportunities, though, key challenges will remain. Data quality, richness and accessibility, for instance, are typically low in manufacturing environments that previously used data for reporting purposes alone. And with the introduction of analytics tools, companies must learn to actively manage the cultural and organizational changes that follow.

At BCG Gamma, we are convinced that high-value transformative analytics can be delivered only through a combination of state-of-the-art data science and a deep understanding of the industry. To that end, we deliver bespoke solutions tailored to each client’s specific operations, along with a clear change agenda. By bringing together best-in-class data scientists, generalist consultants, and operations and industry experts, we can show you how to use manufacturing analytics to increase your production efficiency, enhance your product quality and, ultimately, improve your bottom line.
=========== Hands-on: Modeling Non-linearity in Linear Optimization Problems ===========
Reader note: this article primarily targets a technical audience. It requires a basic understanding of Mixed Integer Programming (MIP) and model building technics. If you are new to these topics, a good practical introduction can be found, for instance, here [1] and here [2]. AIMMS has also published a very detailed case book here

Linear optimization solvers have made dramatic algorithmic progress over the last 20 years. Modern solvers such as CPLEX or GUROBI can now solve problems involving thousands of variables and constraints in matter of minutes. This scalability, as well as the ability to provide fairly transparent optimization decisions, have made these solvers very successful in industries in which making complex optimization decisions is a business imperative.

Linear solvers are sometimes criticized for their apparent inability to model non-linear problems. In this article, we will describe one possible way to overcome this limitation in practice through piecewise linear modeling using “type-2” Special Ordered Sets (SOS2).

Piecewise linear relationships appear in a wide range of optimization problems. Most often, we see two use cases involving that kind of interactions:
• When dealing with inherently piecewise linear interactions: fixed costs, certain types of discounts such as “buy 2 get one free” offers, etc.
• When dealing with non-linear/non-convex/non-continuous interactions approximated as piecewise linear over a discrete set of points.

This second use case is of great interest to practitioners, enabling them to model almost any kind of single-variable interaction as a linear one and inject it into a regular MIP solver. This, of course, comes at the cost of an approximation error, and requires the non-linear relationship to be measurable over a discrete set of points prior to optimization.

As a very simple example, let’s take the classic ‘sigmoid’ function, centered in 0, and its (5 points) piecewise linear approximation.

Imagine that x is a variable of the optimization problem to be solved, and y another variable depending non-linearly on x, approximated as shown above. We would like to input these 2 variables into our MIP solver. In other words, we would like to encode this relationship using only some additional variables and a set of linear constraints. This can be achieved using a type-2 Special Ordered Set.

SOS2 are sets of variables for which:
• There exists an order, in other words the variables can be sorted along one of their attributes
• They are all positive and continuous
• At most 2 of them are non-zero
• The two variables have to be adjacent

Let w1,…,wn be a SOS2, let z1,…,zn a set of binary instrumental variables whose only purpose is to enforce the SOS2 conditions over w1,…,wn. The 4 conditions above translate into the following set of equations:

As you can see, a measure of complexity has been introduced by these equations. This is, in part, because of the second equation: modeling the SOS requires the use of binary variables. That means our problem automatically becomes a Mixed Integer Program, which is proven to be NP-Hard to solve (but can still often be efficiently managed by modern solvers). It is also due to the fifth equation, in which the number of constraints required to model the set is geometrical with respect to its size. As a result, large ordered sets may not scale very well.

At first sight, the link between SOS2 and piecewise linearity is not obvious. However, by enforcing simultaneously the two following constraints:

we define a set of ordered weights. These weights can now be used to describe the segments that constitute our previous piecewise linear approximation of y given x by enforcing the following two additional constraints, in which xi and yi are the coordinates of the breakpoints of the approximation:

Using again our previous example, the following animation simulates how our SOS2 would behave in practice:

Note: In order to run the code snippet below, you need a valid installation of the Gurobi solver. Gurobi is not open-source. However, free licenses are available for researchers and students.

Now that we’ve seen how to interpret SOS2 and their link to piecewise linear approximation, let’s deep dive into an illustrative implementation. We are going to maximize the linear approximation of the sigmoid displayed before. Formally, this is the problem we want to solve:

Graphically, we observe that the optimal value for x is 6. We therefore expect our solver to return us this optimal value for x once the piecewise linear approximation of y is encoded. The following snippet encodes the problem and solves it with Gurobi. Note that, again, we introduce a SOS2 of weights to link x and y as described before. Then, we use y as our objective and are able to reach an optimal value. (In this example the solution is almost instantaneous since the problem is very small.)

As you see, GUROBI, like CPLEX, provides an API to SOS2 constraints. Using the solver’s own implementation has 2 advantages:
• We don’t have to encode the set of equations ourselves (they are automatically generated for us under the hood), and we can spend time on what matters most: finding the best modelization of the problem at stake.
• When declaring the SOS2, the order in the set in kept in memory by the solver. The solver later takes this order to advantage to accelerate the search for an optimal solution when performing the branch-and-bound/branch-and-cut.

Using a less user-friendly solver, we would have to re-encode all the constraints defining the SOS2. This approach requires a bit more work, but is feasible as long as the solver supports problems involving binary variables. We would not, however, get the branch-and-bound speed-up.

If you are new to this technic or to MIP in general, I hope that this little tutorial will make your modeling easier and stimulate your curiosity! Keep in mind that the applications of piecewise linear modeling in MIP go far beyond the tiny scope of this article. In particular, in some cases, this can be used to model stochastic variables in linear models, thus making it possible to transform statistical/machine-learned predictions into concrete and optimal business decisions.
=========== Revenue Management System: In-house or Outsourced? ===========
Revenue management (RM) systems are ubiquitous in the travel industry, from airlines to hotels, rental car companies to cruise lines. The question facing many of these companies these days is whether to build their own in-house RM system or select a vendor-provided, outsourced solution. When Continental and United merged in 2010, both teams got together to decide whether to keep Continental’s PROS solution, or stick with United’s own homegrown Orion solution. In the end, and after very careful consideration, they chose to focus on their existing in-house solution. I know from numerous conversations with the team that it was not an easy decision. By all accounts, though, the airline has been very happy with since — admittedly after some hiccups with Orion and a successful transition to Gemini (c.f. Scott Kirby’s comments during United’s Q3, 2016 investor presentation, slide 42 and onwards). If you are considering replacing or upgrading your current RM system, I hope this post will help you make a wise decision.

A quick survey of the revenue management systems (RMS) used by the top companies in air transportation (airlines), hospitality (hotels), car rental and cruise lines finds an even mix of in-house RMS and outsourced vendor RMS. Table 1 below shows that the largest airlines are split almost evenly between these two solutions, with PROS the most widely used among outsourced systems. The same mix applies, but with different vendors, across top hotels, cruise lines and car rental companies. In general, larger companies tend to be more likely to invest in their own RMS, while smaller players lean toward outsourced solutions. When expanding the above analysis to the top 250 airlines worldwide, in-house solutions are used by about 50 percent of the larger airlines and by 10–15% of the industry overall.

In my experience, the decision to go in-house or outsourced hinges on three major decision points: speed, flexibility and cost, and control. I will save for another time a discussion about two other important aspects of revenue management: governance and internal organization. These two aspects both influence and are a byproduct of the system decision you make. Drush and Horowitz have a good piece that discusses some of these aspects in more detail.

Let’s examine each of the three major decision points in more detail.

Speed is critical when implementing a revenue management solution. You want to realize the revenue benefits of your solution as quickly as possible, particularly in highly competitive environments where an effective RMS solution can mean the difference between profit and loss.

From the perspective of speed, the outsourced solution can look very attractive, given that the solution already exists and “only” needs to be implemented into the customer’s environment. While this process may sound relatively straightforward, it is usually anything but. In the airline industry, systems (e.g. distribution, reservations, scheduling, etc.) have to be identical or very similar around the world to allow for a relatively seamless travel experience. This greatly simplifies the process for RMS vendors, in that the majority of the systems they need to connect to are very standardized. Having said that, anyone at PROS or any other RMS-solution provider would emphasize that, even with such standardization, implementing an RMS requires a great deal of skill acquired only after years of experience.

Even when a vendor possesses those skills, implementing an RMS in an industry with standardized systems requires much more than simply plugging in a machine and flipping the “on” switch. In industries where systems may not be quite as standardized, deploying an RMS can be a lot trickier.

It may seem counterintuitive, but when speed is the issue, choosing to develop your own solution could result in a shorter time to value. If you design your roadmap to prioritize the development of those modules that you know will bring the biggest value — and design the system such that the modules can operate independently of one another — you could potentially realize the revenue benefits of your in-house solution very quickly, and accelerate the time to a profitable, full-blown, self-funded RMS.

Next comes the flexibility and cost aspect of the equation. Specifically, is your organization flexible enough to redistribute internal resources to focus on this project? How much time can you commit to such an endeavor? And how much are you willing to invest in the project? Once again, the perceived advantage of outsourced solutions is that they should come online faster — at least in a quasi-completed form.

The challenge with in-house solutions is that they require a huge commitment of time, resources and budget from the company. They also require a willingness to weather the difficult times that invariably come with implementing complex, company-wide solutions. Building software from scratch is not easy, and progress can sometimes be agonizingly slow. Just ask the FBI, which contracted with a third party to upgrade its IT systems. The project failed rather spectacularly — and at a very high cost.

Vendor-provided solutions are usually much more cost effective — in the short term. Over the long run, however, the license and subscription costs that come with an outsourced solution can catch up to and eventually overtake the admittedly large upfront investment cost of developing a solution in house.

The question becomes whether you are willing to spend the money upfront. If so, have you calculated how long it will take to recover the cost of implementation? Surprisingly, anecdotal evidence I’ve gathered from conversations with colleagues who have implemented in-house RMS solutions suggests that, for larger entities at least, implementation and development costs are recovered very quickly — often within 3–6 months of the system going online!

Keep in mind that the spending doesn’t end once an in-house solution has been successfully implemented. Once the system is in place, you must provide on-going support and development or your system will slowly become outdated and an impediment to your business. Vendors have a strong incentive to stay at the forefront of technology. Companies with in-house systems all too often tend to follow the “if it’s not broken why fix it” philosophy. This approach may be cheap in the short run, but ruinous over time.

The final and perhaps most important decision point is control, which is better understood as a combination of: (1) science, algorithms & data, (2) roadmap and features, (3) confidentiality, and (4) security. While there are additional controls involved when building complex systems, these are the top four to consider, in my opinion. (Comments welcomed and encouraged).

The most obvious area where in-house RMS has a distinct advantage is with respect to control of the backend algorithms. While it is true in part that basic RM techniques are well documented, managing the specific quirks of your business will benefit from the use of bespoke algorithms or tools. Consider fare rules that apply to some specific markets, but not to others. Each market might require a very different approach to optimization and forecasting. Generic systems may be unable to support such an approach, while a custom system can be designed to do exactly that.

Another aspect of the science side is data. Despite the fact that data is of utmost importance in this day and age, numerous industries and companies are hard pressed to even gain access to their own data. Implementing an outsourced solution will require much more control than that. Anything from minor to major adjustments may need to be made to data sources to make them fit the required input formats. Creating this level of control can be both a good and a bad thing, but it is unavoidable when implementing a vendor solution. Choosing to develop an in-house system can provide more flexibility up front, enabling you to design the system to take full advantage of your data, even if that means that, in the end, not all that data you’re gained control of is used.

Another issue has to do with the output of the data into control systems. Revenue management systems need to distribute their optimized inventory and pricing into reservation systems and global distribution systems. This requires interfaces to these external systems so that both the input and the output data remain in sync. While these interfaces and connectors are usually available and developed by both CRS/GDS vendors and RMS vendors, these data exchanges can create friction between competitors. Imagine using an Amadeus RMS with a Sabre CRS: Sabre may not have much of an incentive to facilitate integration of the Amadeus system, since Sabre itself offers an RMS solution. This friction can arise with in-house solutions as well, although the CRS providers may be more inclined to work with the customer since there is no intermediary RMS provider involved.

Another very important aspect of control is the ability to create your own roadmap and feature development. Developing in-house means you aren’t competing with other customers to convince the vendor which features to develop first and to save for later. Developing in-house essentially means you gain total control over your own destiny when it comes to designing and building the overall system.

At the same time, working in your own “bubble” means you are not part of a community of practitioners, learning from your peers and adopting best practices. Vendors typically work with many clients with different needs and unique problems. This enables them to learn from their customers and bring these insights to their development work. If you’re isolated from this process, your solution may not include these best practices.

Confidentiality is a very important aspect of control in a highly competitive environment. I have often heard people point to the ability to keep a solution confidential as one of the positive differentiators of an in-house system. Numerous research findings, however, including MIT’s PODS consortium studies, have a different take on the issue. Research generally supports the idea that there may be a first mover advantage in RMS, in the sense that if one company develops a highly productive system it will have a leg up on the competition. But it also suggests that there is a clear global benefit to the industry when all competitors are operating at the same approximate level of sophistication. Contrary to the idea of a zero-sum game, companies — and, in fact, an entire industry — can actually benefit by sharing, at a high level, aspects of the RMS they have developed and found to work well.

In my mind, the jury is still out on whether or not keeping your secret sauce secret is truly helping. The Nash equilibrium suggests that when all participants use revenue management with the same approximate level of sophistication, everyone wins.

Data security is of paramount importance, and is a subject that often comes up when considering external software providers. It is a topic that I feel is slightly secondary to the previous three major considerations we’ve just discussed, but it is top of mind in today’s cloud-based world. Given the nature of revenue management systems and their necessary openness to a large number of external distribution systems, I feel that regardless of whether you choose an internal or external RMS solution, data security should be a critical part of your decision process.

It may seem that hosting your own system with internal hardware and data centers would be the more secure approach. However, in this day and age of cloud computing, an in-house system built with purely internal hardware is unlikely to be the economical choice. From there, you have to weigh the pros and cons of choosing a cloud-based platform for your internal systems or of relying on the vendor if you choose an external solution. Do you feel like your in-house team has sufficient expertise regarding data security? Will you need to hire an external provider to test your systems? Are you more comfortable with a vendor system that has been externally certified against intrusions? You must consider all these questions when making a selection, particularly as more vendors choose to embrace the benefits of cloud-computing platforms.

In my experience, I have found that the choice between in-house and outsourced RMS is never an easy one. Having been on both sides (the airline side at Continental and the vendor side at PROS), I have felt the pain of both customers and vendors. The decision truly boils down to three points:

How quickly can you get a system off the ground? How important is speed to your business? Can you afford to spend a few months to a few years to build an in-house solution, or will an off-the-shelf solution serve you better?

Do you have the financial means and resources to invest in your own in-house system and to support it for years to come? Or would it make more sense to essentially “rent” a system from a vendor?

Do you want to have the kind of total control over the solution, the roadmap and the backend you can achieve with an entirely in-house solution? Or would you benefit more from a solution informed by the best practices of an entire community of users, as you would be with an outsourced solution? Does your organization possess the internal resources to develop an in-house solution — and then support it over time? Are you ready to embrace the cloud? If you are, are you confident that your internal teams can prevent all external intrusions? Or it is better for your company to leave that to “the experts” — vendors that are in business to do that exact thing?

Once you make the final decision on your RMS solution, it will then become extremely important to confirm commitment from leadership and from the company as a whole. Implementing RMS, whether in-house or outsourced, takes time and hard work. Changing direction midway through the process can be difficult and costly.

You may have noticed that in the course of this analysis I did not discuss the people or processes associated with RMS. These are both, of course, very important aspects of the decision process. They too will have an impact on your decision. Consider how the revenue management function would be handled within your organization. If you develop in-house, is leadership aligned? If you outsource, does your vision match that of your vendor? Can you reconcile any differences, and possibly learn from a vendor solution? Drush and Horowitz, as referenced earlier, also provide some valuable insights into these aspects in their post.

Before I close, I should mention that there is actually a third choice: outsourcing the RMS function in its entirety. This is a very rare solution — one that few companies employ. Hotel groups sometimes use this approach with franchised properties, providing their franchisees with price and inventory recommendations through their central RM solutions. One can argue that these are not fully outsourced RMS functions, but they are as close as it gets today. I am waiting for the day when a revenue management vendor considers this solution as a possible end-to-end service for their customers. Until then, you’ve got a binary choice ahead of you: in-house or outsourced.

What’s your take on this thorny decision?
=========== Stages of AI in Business and Beyond : The Final Lesson from Chess ===========
Many pioneers of computer science and artificial intelligence (AI) played chess and extensively researched the game. For about 40 years, chess was the reference system for progress in AI — similar to E.coli or Drosophila in genetics. Many people think the story ended when DeepBlue beat chess world champion Garry Kasparov in the famous rematch in 1997.

Instead, the “defeat” sparked a fruitful 20-year era of shifting man-machine collaboration that culminated in December 2017 when AlphaZero demonstrated what we call “hyperlearning,” playing chess in a remarkably human-like style. The history of man-machine interaction in chess over this era contains many lessons for AI in business and beyond.

“Computer” chess began with a hoax, the “Mechanical Turk,” a chess-playing “machine” constructed in 1770 in Vienna. In fact, a hidden human hidden was manipulating the machine. Nonetheless, the Mechanical Turk made various tours of Europe and even played Benjamin Franklin and Napoleon Bonaparte before it was destroyed by fire in 1854. Despite its fraudulent history, the stunt did inspire the longstanding quest to build a machine capable of beating man in chess.

When computers finally became a reality, pioneers Alan Turing and Claude Shannon were among the earliest researches to write chess programs. They were followed by Alan Newell and Herb Simon, and later Ken Thomson and John McCarthy, the organizer of the seminal 1956 “Summer Research Project,” a brainstorming session at Dartmouth College that is often considered the birth of AI.

Chess quickly evolved into a reference system for exploring and testing AI systems. As the “queen of games,” chess was seen as both a complex reasoning problem and the ultimate stretch of intelligence. It had the virtue of relative simplicity compared to other potential references — 32 pieces, 64 squares, and an outcome of win, draw or loss.

The human element of chess was also appealing. The relative strengths of opponents was calibrated through their ratings, and the game was universally known and studied in the Western world for centuries.

So what were the stages of AI in chess and what can it teach us about AI in business and beyond?

Alan Turing finished his chess program in 1950 but did not live to see it implemented. He incorrectly predicted machines would beat humans within ten years. The real competition was initiated in 1968, when McCarthy met the 23-year-old Scottish chess champion David Levy and bet £500, the equivalent of close to £10.000 today, that his program would beat Levy within 10 years. However, in 1978 David Levy won the match in Toronto and indeed kept winning many more — until finally losing in 1989, 21 years after the original encounter.1

At that time, chess started to decouple from AI research.2 In chess, strategies that relied on memory and brute force — efficiently refining search algorithms for effective moves rather than strategic intent — consistently started to beat the best humans. (For an inside look at the most prominent first win of a computer against the world champion in the 1997 DeepBlue II–Kasparov rematch, see Kasparov’s Deep Thinking.3) The half-century of competition between man and machine effectively ended in 2005 with Hydra’s crushing 5.5 to 0.5 win against world-class player Michael Adams.

These victories were disappointing to researchers who were more interested in advancing the goal of “general intelligence” than brute-force victories. While the press sometimes compared Kasparov to the folk hero John Henry, the steel-driving man who symbolized the transition from muscular strength to steam engines, during this first chapter of the story computerized chess was simply relying on a different type of strength, and not a very elegant one.

The State of AI in Business

In business, 20 years after machines started beating man in chess, many AI applications are surpassing human capabilities. When computers interact with customers through recommendation and personalization engines, AI generally performs at a speed and scale that exceed human capability. Similarly in radiology, machine diagnoses are approaching human abilities and — with enough training data — becoming increasingly accurate. Many of these applications leverage the strong non-linear optimization ability of AI or recent advances in machine language and vision. Among the most sophisticated applications are self-driving vehicles, where the quest to exceed human abilities is on.

In all these areas, machines solve tasks differently from humans — often still relying on computational power more than inference or insight. Today, AI machines normally complement rather than a replace humans, leading to endless conjecture on how man-machine collaboration might evolve.

Once again, chess has been leading the way and providing clue to what may happen in narrow business applications of AI. The use of computers in chess did not end in 1997.

Promoted by Kasparov, “advanced chess,” in which world-class chess players could seek the assistance of computers during the game, emerged. In today’s business context, this would be called “augmentation” of human work by AI. Later “freestyle chess” — in which man and machine could collaborate in any conceivable way — took hold. The heyday of freestyle chess occurred during the famous 2005–2008 PAL/CSS tournaments on the Playchess server.

The big surprise in the first tournament of the series was that two amateurs from New Hampshire, Steven Crampton and Zackary Stephen, won using three standard PCs, beating computer-assisted grandmasters and human-aided top chess computers. Crampton and Stephen essentially merged human and machine intelligence. Relying on an intensely trained process, either of the two humans or one of the computers took the lead depending on the position on the board.

Both “augmented intelligence,” with AI assisting people, and the Crampton and Stephen’s “cyborg” approach have often been quoted as blue-prints of a powerful man-machine collaboration in business.4 5 And indeed for roughly ten years such combinations have arguable played the highest quality chess. However, one of the big lessons is that, in the zero-sum game of chess, machines start taking over more and more tasks. Eventually, humans, while still having some superior insights, can no longer cope with the speed of the machine and contribute meaningfully to a real game, explaining why advanced chess competitions have gone out of fashion.

At the same time, all serious chess players rely on augmentation by machines in preparing for traditional chess tournaments. In the professional world, one can foresee similar developments. In radiology, for example, machines will likely move from offering assistance, to second opinions, to first opinions. In the long run it will conceivably only make sense to keep a “human in the loop” as a common-sense control.

Some business areas, such as the auctioning of online advertisements or algorithmic securities trading, have already reached the automation stage. In many of these instances, machine logic is still clearly distinguishable from human logic, relying on superfast but primitive statistical ratings. Many observers believe that this is the steady state in man-machine interactions. Machines will provide scale and speed, while humans will offer insights and training.

This conventional wisdom was upset on the chessboard in December 2017.

In its quest for general artificial intelligence, DeepMind, a company owned by Alphabet, has revived interest in games as a learning ground for AI. AI itself has also significantly evolved with the increasing mastery of efficient deep learning. DeepMind’s core interest is in “reinforcement learning,” in which computers do not receive explicit external feedback until the end of the game. Reinforcement learning had early successes in Atari games, but the most widely publicized win had been DeepMind’s conquest of Go, a popular game in Asia, which is played on 361 intersections and more complex than chess, despite its simple rules.

DeepMind’s victory in Go included an interesting twist that is relevant for future stages of AI. AlphaGo was originally trained on the best human Go games until it beat the European champion in late 2016. By then the machine had run out of top-level games for further training. The ingenious authors of the program solved this roadblock by letting AlphaGo play millions of games against itself. In March 2017, AlphaGo defeated Lee Sedol of South Korea, a top player, and later in the year the Chinese champion Ke Jie.

These victories contained an important lesson for many AI applications. If programmers can create a virtual training environment, in which the machine no longer requires human input, AI can enter a hyperlearning stage.

However impressive its victory, AlphaGo was still built on extensive training by humans and in depth optimization to the specifics of the game.

In December 2017 DeepMind took a step toward more independent intelligence when it introduced AlphaZero, a program that was taught only the rules of Chess, Go, and Shogi, and learnt the games exclusively by playing against itself. AlphaZero proceeded to win against the best machines: Stockfish in chess, AlphaGo in Go, and Elmo in Shogi.

We already knew that machines played well, so what is interesting about the “better mousetrap” demonstrated by AlphaZero? Let us again focus on chess, and what might be the final lesson it teaches us about AI. Stockfish competed comparatively well. Over a match of 100 games, AlphaZero won 25 with white pieces and 3 with black pieces, with the remaining 72 games ending in a draw. AlphaZero also profited from potentially unfair advantages, such as a superior Google cloud environment, and special rules. The machines, for example, could not rely on libraries of opening moves, and all moves had to be completed in one minute. These limitations prevented Stockfish from using many of its specific strengths.

Nonetheless, two aspects of AlphaZero’s performance shocked chess world — and have potentially broad implications for business:
• AlphaZero learned to play chess in the incredibly compressed time of just four hours by playing 300,000 games against itself. No training by humans or other machines was involved.
• Its style was strikingly human. The typical machine logic traces of the brute force approach were missing. Indeed, AlphaZero evaluated about 1,000 times fewer moves per second than Stockfish (about 80,000 to about 70 million), and its play was often intensely strategic.

While we have yet to see further matches, there is little doubt that AlphaZero has ended the days of cyborg chess: Its human-like style — combined with its speed and staggering lack of flaws — renders a collaboration with people pointless.

The Danish chess grandmaster, Peter Heine Nielsen commented in a BBC interview: “I always wondered how it would be if a superior species landed on earth and showed us how they played chess. Now I know.”

Clearly, chess is played in a relatively simple and well-defined environment that hardly resembles the complex and often muddy real world. But there are a few undeniable insights:
• In well-defined environments, the actions of machines — even besides their speed — can become very human-like. A machine can act creatively and strategic and display strong judgment of connections and potential advantages that only play out after a comparable long time frame.
• In virtual environments of self-training machines, computers can enter a hyperlearning phase with learning cycles occuring at machine speed — about one million time faster than human neural speed.

All this might seem irrelevant for the business world, where companies are still struggling with successfully applying AI to seemingly simple real world processes or offerings. But it might pay to watch out for disruptive approaches: Virtual environments for hyperlearning can be envisioned in the real world. Flight simulators already are quite good at training human pilots and might be adapted to train machines. Also, recently popular generative adversarial networks (GANs) aim to create virtual learning environments where algorithms can train one another by solving simple problems — such as creating and discriminating false pictures of faces.8 It is imaginable that the engineering of simple machines or parts can be hyper-learned in such virtual environments, creating a decisive competitive advantage.

We should not succumb to simplistic extrapolations or fears about the real world. Man continues to be unique in our being able to “go meta,” leaving the constraints of the environment and reframing the task. But inside well-defined domains, including many functional business tasks, machines will play increasingly large and relevant roles. We need to challenge simplistic claims — often based on current performance — of the future role of AI, and the ‘obvious’ work split between man and machine. It might be hard for anyone who does not play chess to appreciate what just happened with AlphaZero. And it might be the very last lessons chess can teach us about AI. But it is certainly an important one.

Philipp Gerbert is a Senior Partner at BCG, focusing on the impact of AI in business. He is a physicist, holding a PhD from MIT. In his youth, he was the German Youth Champion in chess and played Kasparov in the Youth World Championship.

Daniel Schlecht is a partner at BCG, leading Digital and AI in Energy. He is an International Master in chess and played against the current chess world champion Magnus Carlsen in 2004. He is an electrical engineer, holding a PhD from the RWTH Aachen.
=========== An Ensemble Approach to Large-Scale Fuzzy Name Matching ===========
Databases are a part of everyone’s life — from swiping our cards for the morning coffee to posting photographs of a weekend brunch; from checking bank balance to making an online purchase — we access databases all day long. Changing business requirements and the evolution of the Internet have led to new types of databases, including tightly coupled relational databases, document storage systems, key-value stores, and wide-column stores. As consumers adopt newer data formats such as structured, text, images, video, audio, and machine logs, enterprises observe the intersection of business requirements and evolving data formats and, in response, integrate newer algorithms and technology frameworks into their ecosystems.

As the amount and types of data continues to increase exponentially, multiple challenges have emerged, most frequently as a function of the scale and noise in data. One such challenge is Approximate String Matching or Fuzzy Name Matching in which, given a name or list of names, the goal is to find out the most similar name(s) from a different list. The domain of Fuzzy Name Matching is not new, but with the rise of mobile and web apps, social media platforms, new messaging services, device logs and other open data formats, the nuances of data have grown, making the challenge of name matching increasingly complex.

To appreciate the problem, imagine that you are the owner of an international logistics company. Your delivery team receives several thousand packages every day. For each one, they must scan the package’s bar code and check the delivery details via an online portal and then enter name and address details on a GPS service to begin the delivery process. After closely observing this process, there is a significant delay in the dispatch time because of issues caused by manually entered information such as sender notes or system entries. Some of the common issues the team in charge of entering this data may encounter on a daily basis include:
• Companies with varying prefixes or suffixes: PT Indo Tambangraya Megah, Tbk vs. Tamban graya Megah, Indonesia.
• Names that have been abbreviated: MSFT Corp vs. Microsoft Corporation.
• Names that have a region’s name accompanying the core name: Suzhou Synta Optical Technology.
• Languages and scripts that vary across regions: Colgate vs. 高露洁 (gāo lù jié)
• Names in the system that have been misspelled: Giordano International Limited, HK vs. Gordano Intl’ Ltd., Hong Kong
• Information that exists in the database in varying formats: Key Products — Iron Ore, Copper vs. “…deals with export of iron ore and copper…”
• Names with extra or missing whitespaces: Chu Kong Passenger Transport Co Ltd. vs. ChuKong Passenger Trans. Co.

If there were only few hundred such records, your team might possibly be able to match the names manually, comparing every string with all the other strings and selecting the similar ones. But your logistics company deals with millions of these records. Manual matching would be not just impractical, but unimaginable. (For a more rigorous formulation of the problem, please refer to [1], [2] & [3].) These strings of text are your only unifying point for these records, so correctly matching similar strings is vitally important. Duplicate customer records can cause poor targeting (repeated customer names), weak delivery (search results gone wrong), or wrong marketing (different products sold to same customer, or the potential of spamming).

Even though the problem of matching these strings is almost omnipresent — and highly critical to customer service — increasing variability and complexity in textual data continue to make string matching a daunting challenge. While there is an abundance of search tools on the market, name search is a different beast, and requires a fundamentally different approach.

In the following article, we will outline a way to tame that beast. The “ensemble” approach to fuzzy name matching delivers the kind of precision you need to avoid customer problems, and does so at an enterprise scale. We used this approach with a BCG client, in this case a large corporate bank. Before we started the process, the bank required a team of 10 to perform the string matching on a dataset that was 100 times smaller than the bank’s complete customers database.

The bank’s existing process of manual inspection was very good at returning very high-quality matches. The problem was that after the database team filtered the names for quality, the sales team received only tens of customer leads. After we implemented the ensemble fuzzy name matching engine, the results met the same standard of accuracy as the manual process — but increased the number of quality leads 500-fold and produced this increase within a month of the engine’s launch. With the matching engine in place within a total project time of just three months, the bank enjoyed a dramatic leap in the number of leads, required fewer head count to achieve better results, could more efficiently allocate valuable data-scientist time, and experienced lower operational expenses. This article will provide a detailed description of how our BCG team achieved all of these results for our client.

The problem of string matching is not limited to a specific industry. From e-tailers that must match millions of incoming search queries with product catalogs, to large government organizations that must match names and addresses for use cases such as identity management and watch-list tracking, a large-scale fuzzy matching engine is a modern enterprise necessity. In a global setting, the increasing vernacular content and vocabulary flexibility across languages and dialects means that fuzzy matching engines must deal with a host of complex issues, including:

5. Prefixes and suffixes: AJO Technology Company Limited vs. AJO tech Private Co., Ltd.

9. Truncated letters and missing or extra spaces: Chu Kong Transport Company vs. ChuKong Transport Co. Ltd.

The challenge of fuzzy matching (and related studies around Entity Resolution, Record Linkage, and other issues) has confronted companies and academia alike for several years. Thanks to the growth of Python and related machine-learning libraries, multiple approaches to tackle such challenges now exist. The problem is that most of the known frameworks (both standalone and hybrid) are either suited to a specific use case or require significant customization before they can be deployed in an enterprise environment. Large organizations such as Amazon, Google and Microsoft have invested significant time and effort into building engines that look for patterns in queries, match the patterns with the search or user context and then provide results or suggestions. Less highly capitalized organizations, though, are still trying to master the art and science of performing approximate string matching at scale. Some of the most common approaches organizations leverage to perform string matching include (more details here):
• Common key method for phonetic or information similarity: The crux of this approach is to reduce the strings to a key based on their pronunciation or language semantics. Some of the most common algorithms used in this approach include Soundex, Metaphone, Double Metaphone, Beider-Morse.
• Edit-distance method: This method is one of the most frequently used approaches to tackle the fuzzy matching problem, and comes as a standard module in most of the analytics/BI platforms that support Data Processing/ETL options. The basic approach of the algorithms that belong to this method is to look at how many character changes (number of character insertions, deletions, or transpositions) are required to get from one name to another. Industry standards such as Levenshtein distance, Jaro–Winkler distance, and Jaccard similarity coefficient fall under this method.
• Statistical-similarity approaches: A statistical approach takes a large number of matching name pairs (training set) and trains a model to recognize what two “similar names” look like so the model can take a set of two names and assign a similarity score. These statistical approaches work extremely well for severe issues, and can also support names across different scripts. The drawback is that they have a high barrier to entry, since preparing the training dataset with matching names requires significant manual effort — and significant time.
• Word-embedding methods: Nuances in datasets are not always related to the content: They can also arise due to the context in which the information appears. For example, “drug” and “pharmaceutical” mean the same thing in the context of pharma companies, but regular fuzzy matching approaches based on phonetic, edit distance, or lists would not capture this similarity. This problem is typically observed when dealing with organization names instead of proper nouns, and grows more difficult as the number and length of words in a string grows. Word embedding — the numerical vector representations of a word’s semantic meaning — can capture these kinds of differences by looking at the similarity between the mathematical representation of two words. If two words or documents have a similar embedding, they will be viewed as being semantically similar. For example, the embedding of “woman” and “girl” are close to one another in terms of vector representations, which means they would be considered semantically similar. Applied to organizations, the word-embedding method can recognize, for example, that JDR Drugs and JDR Pharmaceuticals are most likely the same company.
• Miscellaneous: Apart from the commonly used approaches highlighted above, companies may also leverage custom algorithms built upon one or more of the above methods, or they may use other techniques such as common-key or short-text-clustering methods.

Several comparative experiments have been performed to assess the suitability of algorithms ([4] and [5]), and in almost all the algorithms no one model is able to resolve all the issues. Even though the performance of each algorithm varies based on the context of the problem, the language used, or volume and runtime requirements, most of the name-matching issues can be summarized as:
• Inability to handle multiple scripts: Most of the known algorithms are bound by the constraint of scripts. Some are suited only to Latin, while others might cut across scripts. But most of these algorithms can handle only one script at a time. As such, organizations that deal with multi-lingual data sources or channels cannot use these algorithms as-is.
• Trade-offs between recall and precision: While approaches based on common-key or list-based methods offer a very high recall, their precision is extremely poor in high-variance corpora. Statistical-similarity approaches yield very high precision as well, but perform much more slowly and require a large volume of high-quality training data.
• High compute and runtime: The computing requirement for current state-of-the-art techniques continues to grow exponentially. Even though this issue can be managed fairly easily, runtime remains a significant concern, especially for e-commerce companies whose customers demand instant results and instant gratification.
• Lack of inherent feedback: The designs of most name-matching approaches have one commonality: Over time, they all restrict automated improvements. Heuristics can be applied to accommodate specific cases where a particular algorithm doesn’t perform well, but such fixes can very quickly lead to extremely complex and time-consuming designs.
• Difficulty interpreting stop words: Organizations typically curate custom stop words to clean strings before performing matching operations. Even so, many stop words can play a critical role in the semantics. In most cases, organizations make the call to include or exclude a stop word, based on the approximate proportion of matches affected.
• Handling multiple joining keys: Some organizations may deal with datasets involving multiple keys, each formed from a string field such as customer name and address; or name, description, and other product attributes. In such cases, the engine must be flexible enough to handle different string-matching pipelines, and then aggregate the score across each attribute to derive a unified score and return a list of matches.
• Unranked candidates need a heuristic layer to identify best matches: In many cases, the expectation of the user is to receive not just a list, but a ranked list that identifies the “best match,” “second-best match,” and so on. Ranked results are typically resolved using manual rules or thresholds that are fine-tuned over time. However, slight changes in the input-data distribution or similarity threshold can disrupt entire logic delicately predicated on the training dataset.
• Contextual similarities go unmatched: All the content-based methods fail to look at co-occurrence or contextual similarities and thus fail to identify similar entities, such as “drug” vs. “pharmaceutical.” Even though these differences occur only for the secondary strings in names (common nouns are not affected by these issues), recall of the overall method is extremely poor.

In our recent project, the large bank mentioned above wanted to accelerate its growth in highly competitive markets (those in which the average competitor size was significantly larger than that of the noted bank). One of the use cases driving the overall vision was to identify new leads for their solution suite of more than 50 products. In order to generate leads, our BCG team compared more than 200 open, social and third-party data sources. From these, we shortlisted 45 data sources including internal datasets such as transaction, profile, and historical leads. Since the client’s focus was on the APAC markets, several data sources were available only in regional languages such as Chinese, Thai, and Bahasa. Our end objective was to prepare a data mart that could fuel insight generation and profile identification for more than ten million companies, and then to help our client shortlist and prioritize the leads.

There were five key challenges in building such a data asset:
• Identifying same companies across datasets: This was the core challenge in the entire design, and is the subject of the bulk of this article. With a volume of data that spanned multiple formats, we faced a formidable challenge in identifying the same companies across different scripts. To do so, we implemented a number of sub-modules:
• Normalize company names and addresses across all data sets and bring them into a standard format.
• Remove noise from names in terms of prefixes, suffixes, stop-words, regional addendum and other parameters.
• Account for language differences among separate pieces of company information presented in Latin, Mandarin, Thai, Bahasa, and other regional languages.
• Choosing the most meaningful information for each record in the dataset: Our second major challenge was interpreting the usefulness of a particular field in terms of completeness, coverage, usability and correctness. This was so challenging because almost all the datasets had the same information present across multiple fields: Key executives vs. key management, shareholders vs. key shareholding executives, buyers/suppliers vs. supply chain network.
• Processing text across each individual dataset to normalize the fields: Each of the 45 datasets had its own nuances:
• The datasets contained information in varying text formats: In some datasets, key products existed as keywords (copper, iron ore), while in others they existed as sentences (“…deals with export of copper, iron ore…”).
• Addresses were incomplete, pruned or incomprehensible in several places. For example, “234-xxx, Cecil Street, Alexandra Ro” or “Patin Majar, 23/245/xxx, Tanjong Pagar, Central Sing.”
• Text was stored using different delimiters (Commas, Pipe and other special characters such as ‘, ‘“, ‘-’) and was provided by vendors in different formats (JSON, CSV, XLSX, SQLDB, MongoDB, etc.).
• Aggregating information from different data sources: The client expected the master database to host more than 1,000 fields of information for more than 10,000,000 companies. These fields included details such as company profile, address, key executives, shareholders, financials, key buyers, suppliers, product descriptions, competitors and the latest industry developments. Once we had linked all the identical companies, our next step was to bring necessary information from all the data sets into an aggregated data mart.
• Building an audit and feedback mechanism: Since the client expected the framework to generate leads in a self-sustained manner, one of our key tasks was to develop an information audit (quality of the input data, leads and lead information displayed to the user in the presentation layer) and feedback layer that would continuously enhance the logic and improve the quality of leads.

Since we knew the constraints and solution frameworks available, the best way for us to build a large-scale fuzzy matching system was to “go hybrid.” In the course of a process that spanned three months for end-to-end development, we iterated more than 50 different components and tested them across a wide range of use cases (boundary conditions, overflows, format exceptions etc.) before developing the final fuzzy matching engine, which consisted of five core modules:

Module 1: Language detection and transliteration: As highlighted earlier, the data cut across multiple geographies and scripts, rendering most of the existing algorithms unusable without customization. Since the nuances varied across languages, our first step was to identify the language of an incoming string and generate translations in other languages. We tested multiple approaches to perform the language/script normalization. The modules we developed performed following tasks:
• Detected the language of the string passed: We used two core libraries, langdetect and spacy, as a foundation, and built a custom wrapper around them to improve the accuracy of mixed-language strings.
• Normalized languages: A small number of data vendors provide company names and addresses in multiple languages (for example, EMIS provides names of companies in Chinese and Thai, as well as English). Others, such as CapIQ, make names and addresses available in only one language. It was critical for accuracy that we made sure the available information could be mapped across all languages, so we built a transliteration layer that could make information in one language available across all others. To create this layer, we built a 3-stage pipeline:
• Plain-text translation using Google Translate API for Nouns: For data sources with information in only one language, we used Google translate API to translate each entity into the other two relevant languages. For example, we would convert a company name in traditional Chinese into English and Thai and add the conversions to other corpora.
• Syllable-based maximum matching for word alignment: We used a combination of forward and backward syllable-based maximum-matching approaches for transliterating long names and addresses. Our primary idea with this approach was to create a mapping between English and other languages’ syllables by preparing a lexicon to identify potential matches, and then use a combination of forward and backward maximum-matching algorithms. Finally, we would use a ranking logic to prepare the transliterated text.
• Preparing word/character embedding for multi-lingual comparison of secondary words (Company, Optical, Electricity etc.): Only 30 percent of all the shortlisted vendors provided company details in multiple languages. Therefore, it was critically important to build an offline dataset that could enable comparison of cross-language strings. We followed two mechanisms to solve this problem. First, we leveraged the 30-percent datasets capable of multiple languages (EMIS, for example, provides names of companies in both Chinese and Thai, as well as in English). Then we prepared custom word2vec models to generate relationships between words and characters in different languages, and used those similarity matrices for transliteration. Our second mechanism was to use some of the pre-trained models (here, and here) to perform word- and character-level segmentation and matching. We then used the final embedding we had obtained for matching strings and Named Entity Recognition.

Module 2: Pre-processing textual data: We cleaned the text for whitespaces, falsely parsed text, and special characters using a standard pipeline that tackled different fields using a combination of these parameters:
• Tokenization performed at a word, segment, or character level depending upon the script
• Special characters treated using a custom list prepared for each language
• Stemming using Porter Stemmer as default, with Regexp Stemmer for non-Latin languages
• Stop word treatment: We deployed into the engine three unique sets of stop words:
• A dynamic word/character importance calculator. Depending upon the script, we tended to pick words for Latin and characters for Chinese and Thai languages to derive the top-n words in each corpus per attribute (name, address, email and key banking products). (Please note that, for the sake of simplicity, we have used both words and characters interchangeably in the following sections.)
• List of standard prefixes/suffixes/geographies in Chinese, Thai, Bahasa and English languages
• Replacement words: We fed some replacements into the engine to normalize the content (e.g. coltd as co. ltd.), and identified replacement keywords based on exploratory analyses using word frequency/co-occurrence and business heuristics.

Module 3: Named Entity Recognition & Classification (NERC): One of our key challenges was to identify entities in the names and descriptions that could help link companies across the supply chain and establish buyer-supplier relationships (with respect to the products and services). Our objective was to use this established flow of goods and money to identify high-value leads (more connections, more product lines, etc.). NERC is a process of recognizing information units such as names, including person, organization and location names; numeric expressions including time, date, and money; and percent expressions from unstructured text. Our goal was to develop practical and domain-independent techniques that would enable us to automatically detect named entities with high accuracy. Since none of the existing libraries (NLTK, Spacy, SciPy) provided NER tagging out-of-the-box for our use case, we built a custom NER engine to classify information about company and product description into fields as Product, Company Feature, Service offered or “Other” attributes. In order to generate the vocabulary for standard products and services, we gathered information from Global Industry Classification codes such as SIC (Standard Industry Classification) and NAICS (North American Industry Classification System), and regional standards such as HSIC (Hong Kong Standard Industry Classification)

Module 4: Fuzzy Matching: We performed the actual matching in two stages; a low-precision hashing pipeline and a high-precision computation pipeline:
• Hashing Pipeline: Our objective for this first pipeline was to identify almost-definite matches and prepare hashes that could be used in the second pipeline for more accurate (but, slower) matching. We assembled four separate threads to run in parallel and perform following operations:
• For the first thread, we performed HDBSCAN clustering on company names and address (separate corpus). HDBSCAN is an extremely fast clustering approach that uses density of vectors based on the input strings to create clusters. In order to generate the matrix for clustering, we created n-grams at a character level (for non-Latin scripts) and bi-grams and tri-grams at a word level (as default). We used a tf-idf vectorizer to prepare the input matrix for the clustering exercise, and an extension of HDBSCAN to create different clusters such that the number of clusters was heuristically identified based on the vocabulary size.
• For the second thread, we leveraged a wrapper around the default Fasttext clustering approach. We then made two modifications in the wrapper to optimize speed and accuracy, indexing the dictionary entries by the n-grams contained by them to allow for a fast dictionary lookup, and limiting the matching process to the words that had at least one n-gram in common with the other words.
• We then performed Phonetic Hashing using NYSIIS and Double Metaphone to generate a 2-pair hash for each string. We chose these algorithms because they create an encoding for English words. For example, the Double Metaphone outputs a primary encoding and a secondary encoding.
• For auxiliary matching (products and services) we used a combination of NERC and Word embedding to create hashes for fields with products/services descriptions. The NERC block generated a list of relevant keywords per company, which we then passed on to the embedding engine. A Word2Vec wrapper computed a score for each word within the list and compared it element-wise with the items in pre-computed matrix to generate two outputs: Common elements from the paired list and common elements from the overlapping score.
• Processor for stage-2 pipeline: This consisted of a rules-management engine that performed basic matrix multiplication of the output matrices from stage-1 pipeline to generate the final hash for stage 2. This exercise tested for pairwise occurrences across the approaches and prepared a conjoint hash in 2 steps:
• If, for example, string A and B were paired as matches in cluster 2 in approach 1, cluster 4 in approach 2 and cluster 7 in approach 3, then the 1st hash was prepared as “247.”
• The final hash for each string was prepared using 2-bit combinations of the previous hash (e.g. ‘24’, ‘47’, ‘27’ in the above case)
• High-Precision Pipeline: Two ensembles ran in parallel for each of the hash obtained from the previous step. The number of parallel threads were dynamically adjusted in each run based on the number of unique hashes and the average size and variance of companies allocated across different hashes. The first ensemble leveraged a combination of four different edit methods, one chosen per string type based on the length of string, number of relevant words and script. The second ensemble leveraged a Machine Learning Pipeline and was used to test for disambiguation of false matches.
• Ensemble 1: We ran four different edit methods in parallel using the multiprocessing module, and computed the net score using a base-weighing approach formulated through the use of one or more of these logit functions:
• Post-processor: We used a dynamic threshold for each pair to come up with a strict, approximate or failed tag. The threshold was a function of cross-validation per batch and a randomly selected chunk that a small team then validated manually (within a month of our training the model, we were able to reduce the head count needed for this task from 4 to 1). For each pair, we multiplied scores from stages 1 and 2, and ensemble-1 and rank ordered their log scores. We created two boundaries (strict-to-approximate and approximate-to-fail) in the entire distribution, and passed through to ensemble-2 each entity in the “approximate” bucket.
• Ensemble-2: This was a statistical model layer we designed to perform a hard check on the approximate matches generated from the previous step. The ensemble used a combination of Support Vector Machines and Logistic Regression (based on string length) and yielded final outputs. We used several steps to train the model:
• First, we prepared a custom feature builder that generated more than 30 features using the labeled data for matching. We created the training data using seven datasets that allowed common referencing through the website name and email details (exact string match). We used approximately 11,000 companies with a balanced mix of issues (abbreviations, prefixes/suffixes etc.) to build the features and subsequently train the classification models. Some of the important features prepared in the exercise were:
• Detection of out-of-order components (PT Indonesia Batin Technology Pvt Ltd vs. Batin Technology, Indonesia, Pte Ltd., PT)
• Root text in address (identified using a list of regions and matched using a list method)
• Model training: We prepared the model stage using Random Forest, GBM and XGBoost, with Hyperparameter Optimization, using the standard sklearn’s GridSearchCV. We tested different loss functions based on the script and text length. We made the final selection based on the average precision and its deviation across runs, altering the default scoring method in GridSearchCV to suit the business requirements.

Module 5: Feedback Layer: Since the pipeline encompassed both deterministic and probabilistic approaches of string matching, we needed a feedback layer that could advise the team on potential modifications and trigger alerts in case of severe imbalance. To do so, we created an automated descriptive reporting layer across three modules, and dispatched the reports to the team, which then reviewed those reports and tuned the pipeline parameters accordingly. The three modules included:
• Named Entity Recognition: Scheduled delivery of most imbalanced entities (similar probabilities across most varying classes) and volatile entities (entities that change their assigned class across consecutive runs).
• Clustering parameters: Performance summary across known matches and mismatches, along with a report on cluster performance metrics across repeated runs (perplexity, Silhouette, etc.).
• Thresholds for approximate and definite matches: List of most frequent terms at the edge of the threshold, and their scores across each individual model (method).

The final stack with workflow can be seen below (Fig. 1). Please note that, for confidentiality, we have hidden some of the additional pipelines such as ETL, normalization, and feature generation. A weekly schedule has been deployed for training the core models, with threshold tuning performed once every two weeks. The ruleset validation and re-tuning (whenever required) is done by the data scientists. The entire workflow is automated using Luigi. Memory management for large matrix computations is performed using Dask. Python’s core logging module is used along with smtplib to generate automated pipeline-performance reports. We also used Flask (a lightweight Python framework for API development) to create the serving layer that can handle thousands of requests per second (easily scalable to up to 7,000 queries per second in the current format), which represents a substantial volume compared to the typical demands of a corporate banking environment.

In addition to dramatically increasing sales leads by a factor of 500, our design for the large-scale fuzzy name-matching engine also met our client’s goals in terms of both speed and accuracy. The design achieved a precision of 0.99 on the same-language test set and 0.96 on cross-lingual test set. In both instances, we captured a recall of over 0.92. In a typical daily scenario, the engine performs matching for more than 5 million strings in less than two hours, and while we didn’t perform a manual validation on content beyond 8,000 strings, our precision in each review has been from 94 to 99 percent.

In addition to these bottom-line benefits, our three-month client engagement also resulted in a number of insights that we believe are of great value to any company that is intent on solving the puzzle of Approximate String Matching or Fuzzy Name Matching.
• As clichéd as it may sound, no single name-matching method can address all the nuances found in textual data. Most methods will accomplish 80 percent of a design solution. It is the final 20 percent that requires experience and ingenuity.
• The road to such designs can be extremely challenging. At first, every thread in the pipeline seems daunting — the long run times to train the model and perform stress tests, the exceptions after every component in the design is changed, and the manual chore of going through the match lists to adjust thresholds and identify new methods to adopt. Surmounting these ongoing challenges requires considerable patience and dedication.
• It is critical to invest effort up front to design a good test/validation data set — one that encompasses all possible variations. It is equally important to maintain sufficient sample size to accommodate probabilistic methods.
• Managing small blocks such as stop words, threshold adjustment and clustering parameters takes significant mental energy. The boundaries where manual intervention is required are the most challenging. Successfully meeting this challenge requires a strong collaborative community of business stakeholders, native speakers and data scientists.
• Creating ranked lists is no small task. While in some cases it may be sufficient to identify the universe of all possible matches, the need for rank ordering takes the complexity a few notches higher. A prior understanding of cases in which ranked/unranked output was required can significantly improve the design process.
• Setting up automated tests at the start can save a great deal of subsequent effort. As observed in large software projects, complex ensemble methods like the one described above need significant structure and readiness across all pieces. Our design included more than 150 automated tests, ranging from small unit tests for assessing the language to large integration tests across engines. Even though tests can be set up at any time during the process, doing this work earlier in the process has tremendous benefits during debugging, scaling and user testing.
• Non-English strings (especially Chinese) were a bottleneck until the final week of development. Up until the end, we spent a significant amount of time implementing Chinese/Mandarin matching methods, testing bleeding edge libraries like Jieba, prepared large, custom word2vec embedding, and iterating over Google translate. These are valuable tools, but we note that there is huge potential to further improve the precision of all these methods.

References (provided wherever applicable in the texts)
• A hybrid approach to fuzzy name search incorporating language-based and text-based principles : https://journals.sagepub.com/doi/10.1177/0165551506068146
=========== A Better Way? Forecasting with Embeddings ===========
There are a wide variety of models and tools designed to tackle time series forecasting problems: ARIMAX, exponential smoothing, Kalman filters, RNN, and LSTM, to name just a few. But the time series forecasting techniques most data scientists usually use to leverage historical data don’t always yield the level of granularity desired, and their implementation can be complicated or even impossible in certain scenarios.

There might be a better way. We find that a feedforward neural network with embeddings layers constitutes a straightforward and interesting non-recurrent deep learning architecture that provides excellent forecasting results and shows clear benefits, especially when:

1. The historical data available is limited. Traditional time series methods need multiple seasonal cycles to perform optimally. In practice, that means a model must be trained on multiple years of data — which are frequently not available.

2. There are many distinct, but related, time series. When generating multiple related forecasts — for example, forecasting sales at each of a retailer’s stores — traditional time series techniques (e.g., ARIMAX) need to be applied to each store independently, making it hard for a single model to leverage the data generated by all stores at once such as system trends, shopper behavior during holidays, etc.

3. There are many high cardinality categorical variables. Common techniques for handling categorical variables such as dimensionality reduction or one-hot encoding can be time-consuming, computationally complex, and may require extensive model regularization.

For instance, in a multi-store example, the model can learn from a mix of exogenous and auto-regressive features. The embedding layers allow the model to learn from distinct stores’ time series at once by embedding the store IDs, or to encode categorical features in a meaningful way (e.g., holidays, weather, geography, etc.) in a lower dimensional space to extract valuable information.

An example of forecasting with embeddings

Let’s take an example of a classic sales forecasting problem for a large retailer, where we have access to weekly sales data from thousands of stores. We can easily manipulate and augment the data to obtain the following feature set:
• Time-related variables (e.g. month, week of year, day of week, day of month)

Each data point is a pair x(i), y(i) where y(i) is the weekly sales and x(i) is the feature vector.

Some of the features listed above are categorical with high cardinality. For instance, x_storeid takes as many values as there are stores in the dataset (e.g., thousands of values). Using a classic encoding method such as one-hot encoding might not be very effective, as it blows up the dimensionality of the input feature vector and greatly increases its sparsity.

One way to deal with such features is to use embeddings. As the TensorFlow team notes, “an embedding…stores categorical data in a lower-dimensional vector than an indicator column.” If the feature is in ℝᴺ , an embedding is simply a map from ℝᴺ → ℝᵐ where m is much smaller than N. The corresponding vector in ℝᵐ is called the embedded vector, and it is much denser and smaller than in its original ℝᴺ space.

For those familiar with models such as word2vec, it’s exactly the same concept. Note that the dimensions of the latent space in ℝᵐ is one of the model’s hyperparameters. In other words, the choice of m will impact model performance and as such, needs to be chosen carefully.

Embedding layers’ weights and biases are initialized randomly and then learned with the rest of the neural network’s parameters via backpropagation, just like any other layer.

Their optimized weights and biases minimize the error on the task the neural network is trying to perform — in our case, to predict sales. It could be that if two holidays are similar in terms of how they impact sales, their embedded vector representations will be very close to one another in the latent space ℝᵐ. In some ways, the learning algorithm is allowed the flexibility to transform the input it’s given for certain features into something more meaningful in order to minimize its prediction error (i.e., loss).

Let’s take a concrete example. Assume there are only six mutually exclusive holiday states: no_holiday, columbus_day, independence_day, Christmas, labor_day, new_year.

We can build and train an embedding layer for this feature from ℝ⁶ to ℝ³ such that the sparse one-hot representation of independence_day might be x_holiday = [0,0,1,0,0,0], but -for instance- its embedded representation as a dense vector in ℝ³ is [4.2, -0.8, -1.8].

Once trained, we can extract the weights of the embedding layer and visualize the mapping, as shown here:

As we can see, Columbus Day and Independence Day are closer and therefore more similar to one another in the latent space than No holiday.

In its simplest form, the neural network architecture used in this problem is shown here:

In considering the architecture, it’s important to keep in mind the following:
• Not all features require embeddings; each one should be treated on a case-by-case basis (e.g., normalization, encoding, binning, etc.)
• The embedding features are implicitly one-hot-encoded before being embedded; that’s why they appear as one-dimensional in the input layer.
• Deep learning architectures tend to perform better when fed unstructured data without much feature engineering. For example, feeding in the past 0,1, …, n weeks’ worth of sales and letting the learning algorithm build the autoregressive features it needs from the raw data (e.g., rolling mean) to minimize the loss.

The parameters θ of the neural network f are trained via the usual optimization problem, which is aimed at minimizing the cross-entropy between training data and the model distribution:

If we let:

we then recover the mean squared error loss:

The above loss is then optimized via stochastic gradient descent. For more information, you can reference the book “Deep Learning” by Ian Goodfellow, and Yoshua Bengio, and Aaron Courville, from where the equation was taken.

Putting it to the test

Using a publicly available data set of weekly sales from a large retailer, we trained and tested such architecture and obtained solid results.

As a benchmark, we used simple ARIMA(3,1,0) models fitted on the time series of every individual store/department and running a one-step forward prediction. We retrained those models every time a new data point was observed.

We also trained and tested a random forest to assess the performance of a simple off-the-shelf, non-linear model against the same dataset. The metrics below are computed on unseen test data pulled from about 180 stores/departments over a period of two and a half years:

More interestingly, the power of the embeddings are highlighted when we compute the performance metrics exclusively on the holiday data points, which tend to have erratic weekly sales behavior:

Deep learning has traditionally been seen as an effective tool, but one whose application was mainly limited to specific prediction and classification tasks related to images, sound, and text data. When applied to time series and forecasting, recurrent architectures (e.g., RNN, LSTM) are the preferred ones.

But when there are numerous high cardinality categorical variables, the available historical data available is limited, and/or there are a multiple distinct but related time series, a simple feedforward architecture with embeddings can offer an easier, more effective — and more efficient — forecasting tool than traditional time series techniques.

Its efficiency becomes especially relevant when generating predictions for uncommon days such as holidays, where traditional methods tend to be more challenged, and may require dedicated models and tuning to make them work.

For additional insights, be sure to check out fast.ai, where Jeremy Howard and Rachel Thomas, along with Sylvain Gugger, write extensively about the power of deep learning. Another good resource is Howard’s video series on YouTube.
=========== Giving your Algorithm a Spark ===========
To illustrate when and how a PySpark UDF can be applied, we built an example around the New York city taxi cab dataset (all trips in 12/2018 where customers gave a tip). We cleaned the data; derived some features like weekday and hour of the trip, trip distance, and trip duration; and transformed the tip amount into three categories: Low, Normal and High. Finally, we grouped trips by their pickup and drop-off “taxi zones” (New York has around 260 such zones) and proceeded only with those (pickup, drop-off) routes, in which at least 500 trips occurred.

This gave us roughly 4.5 million trips to work with, in about 2000 groups. The resulting data looks as follows:

Now, imagine we want to compute something individually for each group — a process that is computationally expensive. For example, every taxi driver likes a good tip, so let’s compute where and when the driver can expect one. For this we could train and test an expensive classifier (like SVM) that aims to predict the tip category based on the other attributes. (SparkML currently does not support non-linear SVM as shown here — so a complete migration to Spark is not trivial.)

Let’s assume we have defined this computation as a Python function built for local execution. It covers all steps from data scaling up until validation and, when done, returns the measured accuracy as well as the built classifier object:

Usually, we would now use this function over all data, like this:

This computation takes around 60 minutes on a computer (standard 2018 laptop), since SVMs complexity is quadratic to the input data.

Fortunately, this is an ideal candidate to leverage a PySpark UDF!

We start by defining the PySpark UDF, which encapsulates everything train_and_test() is currently doing. For that, we need to serialize complex Python objects and encode them as strings so they can be passed to PySpark. For this purpose, we define the following two helper functions (using the base64 and pickle packages):

The UDF function that PySpark will use is declared as follows:

Note that we simply reuse train_and_test(): All we have to take care of is the conversion of our feature matrix into a numpy array (which we do inside of the UDF to avoid the need to also encode it before loading it to PySpark) and the encoding of the results (accuracy score and trained classifier object).

The PySpark UDF can then be registered by pointing to the previously defined Python function and declaring its return type:

Starting again with the Pandas data frame “model_data_df”, we can train and test the classifiers using the PySpark UDF like this:

Running the modified code on our computer (locally on Spark, not even in the cluster!), decreases the runtime for the code block by 80% (12 minutes instead of 60) — and this includes the time to convert the data from a Pandas to a Spark data frame and back. Since there are about 2,000 models to build, the potential to further increase performance by more parallelization on a cluster is much larger. On the other hand, the logic inside the UDF can then be tweaked to get better model results — i.e. doing a more expensive parameter search.

There are some limitations to using PySpark UDFs to move an algorithm to Spark. If a lot of data needs to be exchanged between the Python and Spark processes, this creates an overhead and the combined runtime for data transfers can outweigh the runtime improvements from parallelization. This applies generally to problems that are more I/O bound than CPU bound.

The approach works particularly well when there is a set of self-contained steps to execute, and when the computational cost is high compared to the need for data exchange. For example, Spark UDFs can be a great choice for:

- Time series: Such as for forecasting costs of parts or individual accounts. Each row needs to contain the time series data for on part or account.

- Applying a trained ML model: Such as for determining churn probabilities. Each row contains the complete feature set of a customer.

- Fitting a function: Such as for determining the reliability of a part or machine (predictive quality). Each row contains information about the failure history.

- Market basket analysis: Such as for analyzing items shopped per each transaction.
=========== Analytics Roadmap to Personalization ===========
A data scientist, on the other hand, would approach the marketing calendar as an optimization problem. To solve it, she would choose to maximize an objective function — for example, incremental revenue from sales, net of discounts — by algorithmically selecting the right sequence and set of parameters within a universe of options. Those options include:

· Timing and duration of the tactic

· Degree of difficulty of subsequent reward related to the tactic

· Channel(s) used to deliver the tactic

· Creative assets used to promote the tactic

· Verbiage used to sell the tactic

This universe of options easily becomes too vast to be optimized via advanced analytics alone. Indeed, that is the issue most organizations need to solve for: selecting the right sequence and set of parameters.

Finding an approach that solves for such an extremely broad and multidimensional problem is a core challenge. Twenty years ago, we simply would have deemed the problem unsolvable. Today, we potentially have the right toolkit and computing power, but available AI methods still depend on “having seen what good looks like.” And that means we need a considerable amount of data to get to a viable answer, ideally from controlled experiments.

The hunger for experimental data is pervasive. In computer vision, neural networks require thousands of (labeled) cat pictures to accurately recognize another picture of a cat. In pricing, groups of homogenous customers need to be presented with different prices in order to derive the optimal level of price discrimination (creating what’s known as “elasticity curves”). And in marketing campaigns, different journeys have to be deployed in order to figure out which journey will work best going forward. As a result, if an organization has only launched the same A-B-C-D-E sequence of campaigns over time, it will never know if D-A-E-C-B is actually a better option, because it has never seen it before.

So, organizations test, via controlled experiments. The limitations of testing are the same regardless of what’s being tested: time and cost. Going back to our A-B-C-D-E example, it takes time to test the best permutations of five different tactics in the market. Even with weekly campaigns, 120 permutations (5 x 4 x 3 x 2 x 1) would entail months — or more likely, years — of testing, depending on the level of parallelization. There’s also an opportunity cost of not nailing the optimal answer right away. Beyond the sequence, organizations might also want to understand the optimal parameters at a tactical level (e.g., What’s the best version of Tactic A for each customer?), which requires further testing — and with it, more time and more cost.

True AI-enabled optimization is possible, and it already exists in the market. It requires rigor, structure, and persistence, mixed with a hefty dose of simplifying assumptions.

Indeed, the goal is to simplify, and to stratify the learning over time.

If personalized marketing can be treated as an optimization problem, the key is not to solve everything at once. From a marketer’s perspective, the problem is broken down into three steps:

1. Optimize the sequence of tactics over time: Should it be A-B-C-D-E or D-A-E-C-B?

2. Optimize the tactic today: Regardless of what happened in the past, should Jane receive Tactic A or Tactic B today?

3. Optimize within the tactic: Once we’ve selected Tactic A for Jane today, what’s the right combination of reward and difficulty within Tactic A that we need to use?

Our recommendation is to invert the problem-solving order by leaving the sequencing problem for last.

1. Determine the right combination of reward and difficulty required to optimize each tactic for each customer/subset of customers.

2. Define the right tactic today, and maybe add interactions of channels or other contextual tactics to the testing strategy over time.

3. Ramp up sequencing tests, starting with small, tenable problems (A-B-C vs. B-A-C).

Let’s assume that Tactic A is a discount for buying a bundle of products. The discount is the reward, while the bundle of products is the difficulty, or the task required to unlock that reward. For example: Buy jeans and tops (the difficulty of the task) and get 5% off (level of reward).

Both the difficulty and the reward in Tactic A could be hyper-personalized, because customers have individual tastes (e.g., Jane reacts to a bundle of jeans and tops, while Amy reacts to a bundle of jeans and jackets) and triggers (e.g., Jane needs a 10% discount, while Amy needs a 20% discount).

Since there are infinite permutations to be tested at the individual level, we simplify, by testing a finite number of reward/difficulty combinations at the segment level, and by hyper-personalizing the so-called “last mile.”

In the world of ML, deep learning, and AI, segmentation is still our friend. Why? Time and cost. Most businesses do not report having more than two interactions a year outside of their top 1% customers. Apparel, luxury, airline, entertainment, and hospitality are all examples of industries where the median interaction is around 1.6 per customer per year. That’s not enough volume to learn and optimize for each individual. But we can learn and optimize across segments, and define — in broad strokes — the best combinations of reward and difficulty.

Optimize the combination of reward and difficulty

An experiment grid breaking down reward and difficulty could look like this:

We segment the population, each segment gets exposure to different variants of a single tactic, and we learn what sticks. The process never ends.

After two or three experiments we understand whether high-spend/high-engagement customers react better to a 10% discount or a 15% discount, and so on. Experience, data analysis, and rigorously designed experiments will help us define

· The right matrix volume: e.g., Should we use 3x3 or 2x2?

· The right axes: e.g., Should we focus on engagement or tenure? Spend or profit?

· The overarching philosophy: e.g., Should we be optimizing for spend, margin, or response?

So, what exactly are we personalizing? We’re personalizing the last mile, or what that the combination of reward and difficulty means to each individual customer.

Last-mile personalization is where ML comes to the rescue. For example, being asked to buy a pair of jeans and a top might be a medium effort for Jane, but a hard effort for Amy, for considerations other than price. The last mile is about translating what a “medium difficulty” bundle of products means for Jane, and for Jane specifically.

ML gives us a leg up by producing a vast range of scores that can inform us of an individual customer’s propensity to:

Routinely deployed ML approaches are often ensemble models of decision-tree algorithms that provide a propensity score to answer a binary question (e.g., Will Jane buy product X in the next two weeks?) Each propensity score is (more or less) a probability, so if Jane has a propensity score to buy jeans in the next two weeks of 0.72 (or 72%), we loosely read it as a high probability of her buying jeans in the next two weeks. The higher the propensity score (as close to 1.0 or 100% as possible), the higher our confidence that she will actually buy the product.

Once we have our propensity scores figured out, an interpretation of the difficulty matrix could look like this:

An easy bundle is comprised of a product with high propensity (more than 70%) and a product with moderate propensity (less than 40%). Since Jane’s product bundle with more than 70% propensity is likely to be different than Amy’s, we are ultimately creating different recommendations within the same spend/engagement segment of the population. Jane’s product bundle is, in other words, personalized.

Putting it all together

The beauty of this approach is that we are experimentally personalizing an incentive in a timely and cost-effective manner. Which means Jane will receive a message that is truly 1:1.

Let’s revisit the logic. Jane is a high-spend, high-engagement customer. We run two consecutive pilot experiments (for consistency of results) on a subset of the population, where all customers like Jane receive either a medium difficulty bundle with a 5% discount or a hard difficulty bundle with a 10% discount. By measuring against a randomly drawn control group, we conclude that the medium difficulty bundle with a 5% discount generates a higher lift. So, we launch a medium difficulty bundle offer for a 5% discount to all customers like Jane. While the discount (reward) is the same for every Jane-lookalike, each actual bundle will be highly personalized. Jane will be asked to buy jeans and tops, whereas Amy will be asked to buy shirts and blouses. Why? Because that’s what ML “propensities” suggest: Jane has a high propensity for jeans and tops; Amy, for shirts and blouses.

So, while segment-level experimentation helps us learn the right reward, ML helps us translate the concept of “difficulty” into actions. For an overview of the entire Personalization Stack, refer to my article here: How to Build a Personalization Stack

Experimentation, some caveats to keep in mind

Simplification also means we are imposing an opinion on what to test, intentionally reducing the universe of options for the sake of time and cost. Other question marks remain: should a “hard difficulty bundle” be more difficult? Should both products in a bundle below a 20% propensity score? And is 10% the right target?

Just like our algorithms, marketers learn over time, and adjust their experiments based on prior results. These are important elements to keep in mind

Experimentation never ends; we can always learn more, add more complexity, reinforce prior findings. For example, we can test three levels of permutations instead of two, add different channels to the mix, add exogenous factors, create parallel campaigns, etc.

Experimentation is not free; we commonly hear that “experimentation is expensive,” but that’s not the right lens, either. Running controlled experiments is the only path to data-driven personalization, the premise being that we’re spending money now in order to get a higher return later. That money is “spent” in two ways:

· Opportunity cost of keeping customers outside of marketing (control groups receive no marketing and tend to spend less)

· Cost of discounts/incentives required to learn the optimal levels

Experimentation needs to produce measurable results; proper experimentation is designed with statistical significance in mind so that it leads to irrefutable answers — a campaign either generates a return, or it does not. If it generates a return, discounts and allowances are not “costs,” but merely investments with an expected multiplier. The typical return on a marketing dollar using personalization is around 2–3x. Not a bad deal.

Since experimentation is measurable, organizations should consider the cost of experimentation as a predictable investment with an expected and predictable return.

Experimentation to predict “lift” can become extremely complex; while ML has become democratized, doing it right is still challenging. Typical model implementations predict behavior with all else being equal. If Jane never waits for a promotion to buy a new arrival product, her propensity describes her typical behavior without a treatment (incentive). But her propensity might change when the discount is introduced. In an ideal world, we would have not propensity models but “lift models” that predict behavior in relation to receiving a particular treatment. For example, if Jane has a 70% propensity to buy jeans, but a 95% propensity to buy jeans when they are on sale, our take on the bundle difficulty might change as we would find the incentives on jeans to be overly cannibalistic for Jane.

ML is as informative as the experiments through which the models learn. While prior purchase history is a great first step to train a model, more sophistication is required over time to build response models that factor in context and incentives.

Defining the right combination of reward and difficulty of Tactic A for Jane is the first step in the personalization journey, and would typically take anywhere from four to eight months. The second step is to understand whether today Jane should receive a bundle offer at all. We know the best Tactic A for Jane, but is Tactic A the right treatment for Jane today? Having an answer implies we already know the impact of the other tactics on Jane (i.e., we have experimentally tested Tactics B, C, D, and E and we have good directional insight into what works best).

Depending on the size of the target market, this stage comes nine to 12 months into a personalization program, for a number of reasons:

· It takes time to get an accurate read on experiments, so we often need to sequence them instead of running them in parallel

· The more tactics at our disposal, the longer it takes to cycle through all of them

· We also want to include seasonal patterns, as winter incentives might require a different setup vs. summer incentives, for example

Provided we are comfortable with the quality/quantity of experimental data for all the tactics, there are other, practical considerations beyond the data science realm to be aware of. One is that organizations typically cannot execute different tactics for different customers at the same time. For example, our data shows that today Jane should receive Tactic A and Amy should receive Tactic B, but the marketing execution platform is not organized to launch two distinct campaigns at the same time. Not surprisingly, it takes longer for an organization to update processes and platforms than to learn how to optimize tactic selection via controlled experiments.

Another consideration is that a personalization program should start with a forward-looking diagnosis of all related technology and analytical solutions and processes to ensure that resulting insight are actionable, and not stymied by issues of organizational or platform readiness. Provided the organization, technology, and processes are in place to personalize the timing of executing a tactic at an individual level, the optimization logic is a combination of the following ingredients:

Prior experimental data by segment. For example, ranking the effectiveness of a tactic for all customers that look like Jane. At this stage, we have all the input we need, as we have cycled through all the tactics two or three times and we can rank them by objective function (e.g., lift, response, sales, etc.)

New experimental data on interactions related to a particular tactic: The question we try to answer is whether customers react better or worse to concurrent offers. Marketing departments have several different options at their disposal; they can push certain communication, use incentives for other communication, overlap in-store and online experiences, etc. For example, learning that customers like Jane do not usually respond to push marketing (e.g., for a bundle offer) after being treated with a triggered offer (e.g., a post-purchase incentive that only kicks in after a transaction) might impact our decision of whether to trigger Tactic A or to suppress it altogether because it’s ineffective or irrelevant.

Business rules and a brand-specific marketing philosophy. This is the most impactful element when it comes to selecting a campaign. We obviously don’t want to always send Tactic A to Jane, nor do we want to spam her, forget about her, or send her irrelevant messages. To that end, organizations develop a rich set of rules that typically fall into these four buckets:

When we put it all together, our experience leads us to conclude that simplification is the right approach. Technical hurdles, business guardrails, context, and budget limitations for experimentation suggest that business rules typically dictate 80% of the final sequence. The most common reason is that organizations only have so many tactics at their disposal, so anti-repetition and suppression logic shape most of the answers — sometimes simply cycling existing offers is good enough. More importantly, return on investment is often elsewhere. Completing the first step (reward/difficulty matrix) is paramount; beyond that, if faced with the choice of expanding to a new channel (e.g., from email to app/web), adding a new tactic, or hyper-personalizing tactic selection, the last option will hardly ever provide the best return on investment.

Organizations that have successfully completed the second step have relied on complex experimental frameworks coupled with the development of response models and lift models. While deep learning approaches such as long short-term memory (LSTM) have moved the needle in this space, the real shift is around transitioning from simple propensity scores to conditional propensities that factor in context (e.g., customer-specific information, habits or explicit feedback gleaned over time) and engagement. This is more a DOE (Design of Experiment) exercise than a simple algorithmic upgrade — proven ML techniques paired with properly designed tests is often a compelling first foray in this space.

In the first two steps, we blended learning at the segment level with hyper-personalization at the individual level (the ML layer). We also saw how processes and marketing execution can pose technical challenges. Finally, we discussed that tactic selection at a point in time is mostly about inference on incremental behavior (e.g. treated customers spend more than untreated customers, all else being equal).

Our desired end state is the ability to understand the right sequence of touchpoints — be they incentive-based or not — that maximize Jane’s lifetime value with the brand. Lifetime value is the only factor that we’re looking to maximize over the long term. And it is possible, as techniques are being increasingly solidified in both academia and industry. Again, there’s a need for simplification, but also for a philosophical shift in how we allow our algorithms “to learn.”

Traditional ML and DL (deep learning) applications fall under a category of techniques called “supervised learning,” which require “labeled data” to learn over time. But what does that even mean?

It means we are “learning” how to minimize an error, in this case, the error on a prediction. For example, I present a picture of a cat but the model concludes it’s not a cat. We know it’s wrong, and we tell the model so, by providing a number of cat pictures that it can learn from. That’s the “labeled data” — prior information we already identified as correct. The more labeled data we feed a model, the more the model has room to improve and reduce the chance it will make an error the next time it’s presented with a picture of a cat. These techniques work, but have two limitations. They require labeled data, and they require a significant amount of it — more than people realize.

The parallel in marketing is that we have multiple interactions with Jane where we send her (experimentally designed) offers over time; she responds to those offers, either by making a purchase or by ignoring them; and the model keeps learning until it cannot reduce the error rate any further. But we do not have that luxury; statistically, Jane will churn or unsubscribe before the model has even begun to have an opinion. That’s where we introduce a different approach altogether, called Reinforcement Learning (RL).

RL starts with a blanket opinion about the outcome of different options; for example, our A-B-C-D-E tactics. The opinion could be that it assigns each of them a 20% conversion rate, then starts testing what works (i.e., converts) and what doesn’t. The testing is optimized via a sampling mechanism that tries to converge as much as possible towards tactics that garner the highest response. So, it will iteratively try all tactics once, until one tactic — say, Tactic A — delivers a conversion. At that point, it will start to sample Tactic A more often than the other tactics because it empirically knows it to be (more) successful. After iterating several times, it produces a ranking according to which tactics converge often, which tactics converge rarely, and which tactics never converge at all. Add business rules on top and, voilà, you’ve solved the sequencing problem in marketing personalization.

The point of this article is not to explain RL, but to lay out the business implications of when and how to employ such techniques to maximize the return from personalized marketing.

Optimizing a sequence of actions can be and is solved routinely with traditional ML techniques and proper design of experiments. However, the beauty of RL is that, for the most part, it does not require labeled data, much less a trove of prior data. So why do we even bother with the first step, which does require labeled data for ML? A number of reasons:

· Order of operations. We cannot optimize a sequence until we have optimized the individual components. Step 1 allowed us to create “the best Tactic A for Jane,” so that should precede any decisions around whether Jane should receive Tactic A at all.

· Talent. RL is a relatively novel approach, whereas, from a talent perspective, ML competency is more pervasive. The average data science team — provided it even exists — will often be comfortable developing an ML layer but will have little to no experience with RL techniques.

· Time and cost conundrum. A more technical consideration is that RL still requires a considerable amount of iterations for it to converge to a viable answer. As such, its first applications were developed online, where traffic is high. There are different approaches we can take to mitigate this requirement, such as employing different sampling techniques or reducing the universe of options to optimize for (e.g., optimize some campaigns but not all of them), but the best approach is to use ML to fast-track the initial implementation, which is the irony of the process.

Let’s go back to our example. RL started with a blanket opinion as to which tactic to deploy, assigning 20% to each of the A-B-C-D-E tactics. But we know, through experiments, that Jane responds more to Tactic A, and through ML we know that the right Tactic A is medium difficulty, 5% discount. This experimental knowledge means we can significantly accelerate the initial opinion-generation task by leveraging the learning of our first step. RL will continue to sample and cycle through the other tactics, but will converge much faster than if we had never experimented before. That’s why it’s Step 1 in our analytical roadmap, because we’re better off starting there.

The conclusion is that while it’s true that RL learns over time and adapts to change, we get much faster results by completing Step 1 before Step 3 vs. trying to solve for two problems at once.

AI-driven personalized marketing is already a reality. Companies approaching it for the first time should view it as a very unique journey that requires the following components to be successful:

Discipline. Building intelligence requires a sequential roadmap with limited room for shortcuts because currently cognitive science still assumes “the brain must have seen what good looks like.” So, embrace experimentation and follow the proper order: Optimize within the tactic, then optimize the tactic, then optimize the sequence of tactics.

Patience. You can’t learn multiplication before you’ve learned times-tables, and if learning times-tables takes 12 months, invest the time; bear with it. Personalization programs deliver ongoing improvements but expect a two-to-three-year journey to exploit their full potential.

Cross-functional talent. Marketers conceive the curricula and the tactic, data scientists translate them into experiments and intelligence, developers adjust the channels and construct the journeys, data engineers build/maintain the data pipes, finance instills rigor to fund the journey, and legal sets the boundaries of what’s appropriate. So before seeking the best-known AI guru in Silicon Valley, build a balanced team that can bring together different parts of the organization, with a focus on an aptitude for change management.

The benefits of using advanced analytics to power personalization

Increased engagement, deeper loyalty, and a measurable uptick in return on their marketing efforts are some of the core reasons marketing organizations are employing advanced analytics to power their personalization efforts. With them come additional benefits:

Unique data. Investing in experiments builds a unique, invaluable asset that cannot be found elsewhere on the market. While run-of-the-mill ML is getting democratized (not so the advanced applications), the curated data powering those engines cannot be purchased or borrowed; they can only be created internally.

Ability to fund the journey. Outsize returns materialize in at least 18–24 months, but personalization programs can (and do) lend themselves to quick wins and fast turnaround times. We have empirically measured jumps of 30%-40% in net incremental revenue from hyper-personalizing a subset of existing segmented tactics in 3-month mini-programs.

First-mover advantage. The sooner organizations start using advanced analytics, the sooner they start digging the moat that separates them from the competition.

Indeed, using advanced analytics to drive personalization is still a relatively new phenomenon. But it will become standard operating procedure. Organizations that manage to make it a core part of their marketing efforts, will be able to reap the subsequent rewards.
=========== Bring your Python code up to speed with Numba ===========
Python is the programming language of choice for most data scientists. It supports multiple programming paradigms, is available for many operating systems, and features both dynamic typing and automatic memory management. Python programmers enjoy a smooth learning curve and comprehensive libraries, for scientific computing (NumPy), data analysis (Pandas), machine learning/deep learning (Scikit-learn, Theano, TensorFlow), and visualization (Matplotlib, Seaborn). 

It’s also open source, and free.

But Python is an interpreted language, and compared to languages like C and Fortran, which require a higher level of software programming skills, is slower. As such, with its ease of use comes a much lower execution speed. 

And as I learned firsthand, that means less time to iterate the design of the features and algorithms, and to test new ideas that will give your business the competitive advantage it needs.

To mitigate performance issues, most of the aforementioned libraries mix portions of code written in such low-level languages with that written in Python itself. Unless the whole pipeline in a data science project is leveraging existing optimized libraries, however, performance is lost. To be able to experiment with more cases, process higher amounts of data, increase the rate of development iterations, and scale up, a speed boost is necessary.

To get that additional speed, among the options a data scientist manager may consider are hiring experienced software developers to convert the codebase to another language, buying more processing power, and moving to a distributed system (with the associated cost of acquiring specialized data engineering skills).

A better solution is using Numba. The just-in-time (JIT) compiler allows you to capture speed benefits while retaining a development pipeline that, with the addition of just a simple function decorator, is entirely written in plain Python. It enables, in other words, the best of both worlds.

Is Numba the way to go for me?

When it comes to Python, there are a number of alternative approaches available on the market aimed at increasing the speed of execution, among them Cython, PyPy, Shed Skin, and Pythran. 

Assuming that:
• Your code includes a lot of numerical computing, and perhaps uses the NumPy package
• You are willing to stay in pure Python (i.e., you don’t want to learn a new language/syntax nor ask other coworkers to do so)
• You don’t want to spend a lot of time installing a compiler and making the compiling pipeline work

…then Numba is the right choice for you.

As you may have noticed, execution speed isn’t even mentioned in that list. The reason is twofold. First, assuming you are able to make the other packages run properly, they will all speed up Python to roughly the same degree; any meaningful difference among them would be a result of the specific case and, most likely, counterbalanced by the time spent optimizing the original code. And optimizing for the specific architecture on which the code is executed is something that Numba already does for you. Second, unlike other packages, Numba doesn’t require the help of a skilled software developer; indeed, it requires little to no additional effort on the part of the data scientist or her collaborators, and as such, has no impact on the team’s productivity.

Finally, from a strategic perspective, it is critical that you move the fully developed code base to a technology that is widely supported, maintained, stable, and optimized. While the official 1.0 version has yet to be released, the Numba team takes stability very seriously, and tries to keep core functionality consistent between releases. In a phone call with the Numba development team, they described their current target for the 1.0 release to be mid-2019. As to infrastructure, every new version of Numba is tested on Win32, Win64, Linux, Mac, Power8, Arm64, and Arm32 systems crossed with four different versions of Python. So not only will Numba most likely be supported on your system, it will most likely be supported on all possible systems to which you may want to deploy your code.

Given its native integration with NumPy, Numba is ideal for accelerating math-heavy computational portions of algorithms in code that, having applied vectorization wherever possible, still take a long time to run. I use it in features engineering, the development of pricing models, and signal processing, all with calculation times of minutes instead of hours. And as the time I spent in front of the screen waiting for the calculation’s output decreased, my productivity increased accordingly.

Currently, Numba neither supports widespread packages, such as Pandas, nor error management and logging to produce robust code. So, what is the best way to integrate Numba into your development workflow while retaining the same coding speed, quality standards, and interface flexibility?

There are three steps that you’ll need to take: one, profile the code to find the time-consuming portions that are the best candidates for JIT compilation; two, isolate them in separate functions; and three, add the @jit decorator and enjoy accelerated execution speed.

Let’s go through the process starting from a function that computes the sales of a transactional database averaged over a time window specified by the user.

The dummy transactional database is a Pandas DataFrame comprised of three columns — day, sales, and sku_id — which report the day in which a number of sales occurred for a specific item marked by sku_id, respectively.

The following code snippet generates the dummy transaction database by assuming 10,000 transaction entries over 30 days:

Next is the function avg_sales, which allows you to specify the start and end dates through which the sales are averaged:

To identify the bottlenecks in your computational pipeline, profiling is the way to go. Packages like cProfile and profile are two options; both provide a detailed view of the operations where most of the CPU time is spent. Be sure to closely examine the profiler ranking to identify any CPU-bound problems, as I/O-bound operations (like read/write operations to the disk) are unlikely to benefit from JIT compilation.

The image below is the output of a SnakeViz profiler.

SnakeViz is a free profiler that relies on the cProfile package. Its output clearly shows that Panda’s package to access Data Frames, indexing.py, which is included in the main for-loop, is the main contributor to computation time.

The next step is to encapsulate the computational kernels in separate Python functions, each of which will be jitted:

Now that the computational kernel has been isolated, we are ready to accelerate it using JIT compilation:

In the image below, Numba was compared against a naïve Python implementation and Cython, a popular choice when code speedups are sought.

As you can see, a significant speed increase was achieved. It’s worth noting, however, that Cython competed with Numba’s performance only when the various tricks described in this tutorial were used. The faster Cython implementation required learning a new syntax to add static typing for inputs, to remove safe checks (e.g., valid bounds and negative indexes checks) and, of course, to generate and compile an extension module.

According to the authors of Numba, typical speed bumps range from 2x-5x when dealing with simple NumPy operations to 100x-200x when it’s pure Python code involving loops. Not too shabby, given that we’ve only added a one line of code to the decorator function!

And that’s not all

There are a couple of other Numba features that you may want to check out, such as universal functions and filters through the stencil decorator.

Notably, Numba can release the global interpreter lock with a simple flag, allowing it to fully leverage the computational power of multicore architectures so common nowadays and thus solving what is perhaps a major limitation of the Python language. Increasing the computational power of a virtual machine (and in the process, reducing the expenses associated with running it) is impactful only when you can benefit fully from it, and that’s exactly the case with Numba.

And much like OpenMP and Cython, Numba also supports the parallel for construct, an effortless syntax that allows seamless integration with existing code. Last but not least, Numba supports GPU computing, which is particularly relevant for things such as array computations and Monte Carlo calculations, where massive parallelization is possible.

If you want to go further…

As you can see, Numba is the best option for solving a broad range of common problems, but is especially good at allowing you to boost your speed in Python without the need of a dedicated software engineer on your team — all while retaining the quality of your code.

For any data science manager looking to go further with Numba, a lot of useful documentation is available here. And here you’ll find a new and rapidly expanding repository of Numba use case examples.

Finally, if you’re looking for an overview of Python code optimization in general, I suggest “High Performance Python: Practical Performant Programming for Humans”, written by Micha Gorelick and Ian Ozsvald.

The Author would like to thank Ian Stokes Rees for his helpful suggestions on the article and Stanley and Siu from the Numba team for their availability to discuss and review the technical contents
=========== Airtunnel: A blueprint for workflow orchestration using Airflow ===========
As part of our work at a data science consultancy, we build and operate systems to assist in various analytics projects. These are often large-scale projects involving thousands of jobs per day processing hundreds of data assets, along with sizeable developer teams collaborating on workflows. In the following discussion we capture some of the learnings on workflow orchestration for (big) data processing.

For any workflow¹ that might be repeated more than a handful of times, one should typically consider automating the process. In this article we will argue that for reasons of productivity, repeatability and documentation you need some degree of automation via workflow orchestration. To be clear:

Productivity in this context refers to the ability to easily and efficiently perform a series of steps as many times as required. The IT saying to “automate and modularize early-on” is especially true for workflows.

Repeatability relates to executing tasks in the exact same order and in a consistent manner. Data processing applications, for example, require repeatability to make sure that findings can be reproduced.

Documentation for a workflow, in this context represented as the code or configuration that orchestrates it, is as deterministic as it can get and therefore a powerful tool for knowledge sharing and transfer. This form of “documentation” does not — and should not — exclude human-readable documentation that enhances understanding of the workflow.

A good example of the need for workflow orchestration is an ETL process. Often an ETL process will be repeated daily and, as such, requires consistency. Failing to maintain consistency can corrupt decision making, business processes, and other business tasks. (Note: Unless there is certainty the above is a one-off event, orchestration should be considered from the start.)

An ETL process workflow often includes the following steps:

3. Performing a pre-defined list of transformations such as deduplication and correction of known data issues.

4. Loading the final data into the target system.

Another workflow example (not involving data), relates to building, testing and deploying software (cfr. CI/CD). This is another often-repeated, multi-step process that requires constant scrutiny over how the different steps are performed. This process typically involves checking out the code from version control, performing quality control, compiling artifacts, running tests, inspecting the results, and deploying to multiple environments. Given this complexity, a manual approach would be very tedious and prone to error. Furthermore, lack of clear workflow orchestration can significantly hinder the ability of multiple developers to work together on a single application.
=========== Novel TabPy Approach Connects the Power of Python to the Front-end Ease of Tableau ===========
If you are familiar with TabPy, then you know it is a powerful solution for those who want to use Tableau as a visualization tool, but require a computational core that is beyond Tableau possibilities and have a team of data scientist available to work on the back end. TabPy is also useful if you have developed a Python application and want to streamline the process of having the GUI shown in Tableau.

These use cases can be accomplished using basic TabPy capabilities. For more sophisticated uses, however, I have developed the novel approach I am about to describe. This approach is designed for a number of use cases, including:
• Those who want to have a real-time user interface, one that minimizes the processing time and delay between a parameter change and updated visualization.
• Those who want to show (several different) aggregation levels on the same Tableau dashboards, but need to perform all the calculations using the most granular-level data.
• Those whose backend calculations rely on more than a single data source or database.

My novel TabPy approach makes it possible for people without deep data-science skills to quickly build and deploy interactive dashboards.

As a result, non-technical audiences can interact with data in a deeper, more meaningful way and experiment in real time with various scenarios. Based on a real-time Python backend, this approach enables users to capture the best of two worlds by leveraging the wide availability of powerful data-science techniques.

Tableau provides a powerful and comprehensive platform for data exploration and visualization. The variety of its visualization methods, as well as its built-in geo coding that enables geo-analytics application, have contributed to make Tableau a powerful tool for business intelligence and descriptive analytics. This wealth of capabilities is readily accessible via an extraordinarily easy-to-use interface, making Tableau a very popular tool as well.

What limits Tableau’s use to visual analytics is the fact that it is fundamentally a data visualization platform rather than a programming language. As such, it does not natively offer access to powerful, modern data-science techniques. Such techniques are typically available as open source packages available to a broad audience, written and maintained in commonly used scripting languages such as Python.

To overcome this limitation, Tableau developed TabPy, a Python package that enables it to execute Python code on the fly and then visually display the results. Built on a Tornado process, with communication between the two tools using REST APIs, TabPy enables the quick deployment of advanced-analytics applications. This combination of a backend written in plain Python and a frontend based on Tableau makes it a very powerful way to complement descriptive analytics with predictive and prescriptive analytics.

The Unique, But Limited, Benefits of Basic TabPy

To some extent, TabPy’s split approach provides the best of two worlds: class-leading data visualization capabilities backed by cutting-edge, always up-to-date, data-science algorithms. By effectively splitting accountabilities, this basic approach provides important resources benefits as well: Front-end designers can focus on the visualization portion, while scarce data-scientists resources can focus entirely on the science behind the data analyses. And given Tableau’s accessible learning curve, these front-end designers do not necessarily need software engineering or data-science skills. As such, TabPy puts this combination of powerful data analysis with straightforward data visualization within the reach of the most diverse business organizations.

On its own, Tableau is capable of deriving insight from data. Basic TabPy can dramatically enhance the power of those analyses. Used together, Tableau and TabPy go well beyond the scope of data visualization and into the realm of interactive dashboards, in which final users can tune multiple parameters to evaluate real-time impact.

With TabPy, whenever a parameter is changed, the dashboards are immediately updated. To do so, TabPy leverages an input/output approach in which both the data aggregated according to the current visualization and the tuning parameters are transferred to Python. Python then processes the data and sends the results back to Tableau to update the current visualization. So far, so good.

In many scenarios, basic TabPy provides an elegant solution. It falls short, however, when the input-output paradigm is broken. This can happen for a number of reasons, such as when the full underlying data set is needed for the calculation, but an aggregate measure is shown in the dashboard. The same problem can occur during more advanced interactions, when multiple aggregation levels are shown at the same time. It can also be difficult to leverage multiple data sources in a single calculation, when the exchange of full databases increases the transfer overhead between Tableau and Python and can harm the responsiveness of the dashboard.

The novel approach that follows can overcome all such limitations, leveraging the power of Python so users can unleash the full TabPy potential to create visually compelling, highly interactive Tableau dashboards.

To explain this novel approach to TabPy, I have designed an example of complexity reduction through product-portfolio optimization. The subject of the optimization is a B2B retailer that has experienced growth mainly through mergers and acquisitions (M&As). Because of this inorganic growth, the retailer faces a great deal of complexity, operating on several markets and having a portfolio composed of a thousand SKUs (Stock Keeping Unit) that are divided into several categories and subcategories. To further complicate matters, the SKUs are built in different plants.

The company’s senior management wants to increase margins by removing the least profitable SKUs, but is willing to continue selling lower performers to maintain a certain market share. They are also willing to keep the manufacturing plants running above a target asset utilization, knowing that to decrease utilization too far would negatively impact the fixed costs base of each plant.

The data available in this example consists of an SKU-level database in which yearly volumes, costs and revenues are reported. The SKUs are organized in Category and Subcategory hierarchical levels:
=========== How to Build the Personalization Stack ===========
Inputs are pieces of information about the customer and the product. We want to collate every relevant piece of information about a customer’s past and current behavior, with clarity around the journey over time (including the disambiguation and de-duplication of IDs) and every touchpoint along the way (whether they involve inquiries, browsing, purchasing, etc.). We also need a well-structured view of the product hierarchy, ideally enriched with SKU-level profitability (not just price) and product features/descriptors that can be used to run lookalike and clustering techniques.

This layer captures raw data but also transformed data coming from other components of the personalization stack. For example, it can capture a customer table with basic ID info (unprocessed data like personally identifying information) enriched by processed data such as individual responses from prior campaigns and predictive scores of individual propensities (to buy, to engage, to fade, to trade-up, etc.).

An inference is a conclusion informed by past behavior and/or a prediction of future behavior. For example, the recognition that Jane Doe and Jane Smith are the same person, her last-time value or progress within a specific journey, or the belief that she’ll buy product X in two weeks but is unlikely to open emails within the next month.

The last one is the realm of machine learning applications, in a sub-layer known as the ML layer.

Now, is machine learning a form of intelligence or a simple form of inference? Semantics aside, machine learning in isolation is hardly prescriptive and doesn’t fully inform the next best action. We’d need more elements of the personalization stack to surface prescriptive recommendations (and I’m generalizing here, for simplicity).

Intelligence is the ability to prescribe a specific action in order to maximize a desired behavior. That prescription, or “next best action,” is ideally in the form of a tactic (e.g., offer a discount on a bundled purchase), a context (e.g., after a 5-second ad impression), and a channel (e.g., online, followed by a reminder email). Some non-obvious considerations:

>> Marketing ideates the tactics. The more codified, modular, and structured those tactics are, the more tractable the optimization problem becomes.

>> A basic next best action is to rank each possible tactic by expected value. The probability of conversion multiplied by the profit generated by the purchase is what’s known as the “expected value.” Ideally this is ranked for each customer, over each channel, and across each context. However, that can easily yield an unmanageable number of permutations, which will require simplifying many assumptions.

>> The most compelling optimization approach is ongoing experimentation. Machine learning can’t optimize every permutation algorithmically. The secret sauce is iterative controlled experiments — think ongoing A/B tests on steroids. And the experiments require some form of segmentation (e.g., group A, which receives three levels of discounts, is then compared to control group B, after which the winning discount level is selected). Yes, to personalize we still run segmented experiments. That’s the irony of statistics.

>> Business rules play a very significant role. Examples include eligibility, anti-conflict, and anti-repetition rules that often override the original ranking by expected value. As the business rules layer becomes more sophisticated, modularity, UI, and performance at scale become more mission-critical than refining the ML layer.

If this sounds complicated, that’s because it is. Personalization is not just about any differentiated touchpoints; it’s about the optimal differentiated touchpoints for each customer, at each point in time. Optimal implies maximizing an objective function (e.g., lift, conversion, spend). To that end, marketers need to concurrently deploy strong inference (ML and features), rigorous experimentation, and a reliable business rules layer.

Once we have defined the next best action (who we are targeting and what touchpoints we need to use), we’ll need to figure out how were are going to deliver those personalized touchpoints. This is also what’s known as the marketing execution layer. The main ingredients are:

>> Offer schema that translates the marketing tactics into a structure that a platform can ingest, for example, a table with offer type, duration, level of discount, number of hurdles and/or steps, level of difficulty, etc.

>> A formula for creating the offer, such as code that takes a customer’s information and converts it into the data schema described above

>> A configurable workflow engine to create customer journeys (e.g., “If customer does X, send offer Y; if the customer takes no action for two days, send a reminder email”)

>> A workflow monitor that checks the status of each live workflow (e.g., “Has customer A received the offer? Has customer A made a purchase? Is it time to send a reminder email?”); the more complex and contextual the workflow, the more advanced and real-time the state monitor engine needs to be

>> Ways to surface the action, such as an API, digital assets, an electronic service provider, etc.

To contextualize, below is a logical flow that shows how each component is activated in the process of understanding the customer, defining the next best action, and building/surfacing that action. (See Figure 3).
=========== Adopting Data Science Solutions for Business: Balancing Complexity, Accuracy and Interpretability ===========
The use of data science across companies and industries worldwide ranges from non-existent to advanced. Even inside companies that actively use data science, the way it is applied can vary dramatically. Many top tech companies rely heavily on data science in their day-to-day operations, while other more traditional companies such as grocery chains, CPG, or IG companies focus more heavily on human capital. And within all of these companies, some functions such as sales rely more on qualitative approaches, while teams such as IT or scheduling and revenue management for airlines may focus much more data-driven decision making.

Bringing data science to clients always presents very interesting challenges, not the least of which is the notion of complexity and accuracy versus interpretability. Depending on where they are on their journey to advanced implementations of data science, clients may be more or less willing to sacrifice some measure of complexity for the sake of clarity or understandability. On the low-complexity end of the scale, you run into the challenge of whether overly simple models actually bring value to your client. On the high complexity side of the scale, users often are confronted with “black box” issues whereby the complexity of the models makes it impossible to interpret and understand what happens between the inputs and the outputs. This “black box” concept raises some interesting human challenges: How much do you trust your models? At what point do you lose credibility? How do you help support and validate your model’s recommendations to your business users? In the end, it all boils to down a single and critical success metric: adoption. The most significant challenge is oftentimes balancing complexity and accuracy with interpretability to achieve adoption, and then managing the side effects of striking this balance.

As the data science arm of BCG, BCG GAMMA is often sought out to devise data-driven solutions for clients in very specific fields. We must always find ways to balance each client’s technical acumen, expectations and capabilities with our own desire to deliver an optimal solution. In this post, I would like to discuss how to think through this balancing act while working with clients to help them solve specific problems, as well as to share a number of specific examples.

Over the past decade, data science has taken on a prominent role in business. Google, Amazon, Apple and Microsoft use it to develop very visible applications such as digital assistants. Uber, Lyft, Didi and others use it to create ride-sharing applications. And companies such as Waze use it to develop route-guidance optimization applications. The increasing availability of data, the lower cost of sensors and IoT solutions, and the increase in computation power have all made data science applications like these possible.

Given the success of these applications, many companies are looking to data science to improve their top- and bottom-line performance. The potential applications are endless. On one end of the spectrum are churn-modeling tools such as defection detection, or prediction engines to calculate when customers might leave your company and move to a competitor. At the other extreme are financial-modeling applications, HR solutions, and machine-failure prediction tools. But no matter what the intended application, the question I often have to grapple with is whether our potential clients are looking to data science as a solution looking for a problem, or truly looking to solve an actual, day-to-day business challenge.

I recently engaged with a large retail bank that wanted to improve its product pricing. The bank’s goal was to provide its customers with the best possible price so it would have a profitable operation, while growing and shielding itself from potentially revenue-negative customers. In the course of my interactions with the bank’s leadership team, it became clear that they were trying to achieve two very different goals:

It didn’t take long to discover that the second goal was in conflict with the first. Why? Because machine learning is a field that lends itself to a specific set of applications. Furthermore, while the term “machine learning” encompasses a large class of models tasked with making inferences from data, it is often confused with very specific applications, such as random forest classifiers or ensemble models.

The bank’s leadership team was looking for a solution that would leverage this latter class of models. Our assessment was that, for this particular task, a time-series approach rather than an ensemble model was better suited to their needs. I have found that, even in use cases where ensemble models could theoretically be used, a more traditional logistic regression approach (which, by the way, is also machine-learning) could behave much better.

This dichotomy between expectations and model performance quickly turned the process of optimizing the bank’s pricing into an uphill battle. To get beyond this impasse, our team decided to place more emphasis on educating our client team about how each type of model applies to different situations, and why the use of time-series solutions or logistic regressions was better suited to their specific needs.

Solving the problem — then, explaining the solution

In another example, I recently worked with an energy company that was trying — among other efforts — to improve its customer retention at the end of contracted service periods. Our team’s analysis of the problem led to the use of a classical random forest classifier, a machine-learning approach that leverages ensemble models. In this specific instance, our client’s other goals did not conflict with this specific one, so we were able to get quick buy-in to our approach and algorithm selection. To do so, we chose to fully leverage the power of random forest classifiers and, in fact, used more than 400 features in the model. We followed the usual approach to building such machine learning models by training the model on numerous data subsets, followed by validation on test sets. We also performed forward and backward tests to prevent seasonal issues from affecting the quality of our models. In the end, we landed on a very accurate model to predict churn.

We faced another challenge when it was time to explain the model to our client counterparts. We had difficulty explaining the methodology in ways that could be understood by a team at the very beginning of its data science journey. Eventually, we were able to get buy-in from the client team, but the “black boxiness” of the model created additional confusion when the team asked us to provide traditional regression metrics and parameter estimate values for evaluation. Obviously, these ensemble models do not output such parameters, so we could not provide them to the client. Instead, we relied on partial-dependency charts to demonstrate some of the relationships.

This communication challenge highlights what can happen when a company moves quickly from little or no use of data science to advanced machine learning solutions. It further suggests that, in this case at any rate, it might have been more prudent — and probably just as effective — to propose a more traditional approach to data science solutions. Had we used a more interpretable logistic-regression solution to this churn model, we might have been able to bring the client along the data science journey faster by providing them with intermediate anchor points in the data.

When implementing data science solutions, it is important to take into consideration both the client’s underlying goals for the solution — and their ability to understand and relate to the solution you are working on. In the first example I shared, a large part of the effort centered on first educating the client about the value of the solutions we were proposing, and then on applying the right solution to the problem. In the second example, had we been quicker to recognize where our client was in their data science journey, we could have taken a slightly different direction that would have solved their immediate problem — and allowed us to gradually enhance their data science skills.

The strength of the foundational data — and of the business users

Another area of great importance when considering data science solutions is the state of a client’s data environment, processes, and tool sets. If the client does not have good data available, then you will not be able to build a successful model. This may sound incredibly obvious, but it is not always apparent to internal users. For example, a leading truck manufacturer asked us to help improve its pricing and sales approaches by leveraging internal data along with some external, commercially available data. We quickly found that the quality of the in-house data was much worse than we anticipated. The main problem was that instead of using unique customer identifiers, the company relied on manual alphanumeric entries for each customer. This meant that a single customer who made ten purchases could have ten different name entries, with variations to each entry. If the variations were slight, we could fuzzy match their names relatively easily. However, in many instances the names were completely different from one transaction to the next. Unfortunately, similar issues also existed in the external commercial data the client had purchased. These kinds of data issues are not new, but they must be evaluated early in the process so that everyone understands from the start what is and is not feasible.

Another critical piece of data science adoption revolves around the actual users of the solutions created by data science teams. This takes us back to the black box solution I mentioned earlier. When I worked in the Revenue Management (RM) department of a major airline, we leveraged a variety of data science tools developed in-house or by outside vendors. The users of these tools (that is to say the members of the RM team I was on) were generally highly educated and advanced users. Their degree of skill was due, in part, to the historical legacy of RM within the airlines and to the fact that the group was an entry point for the airline’s future talent. As a result, the team had access to very detailed training programs with thorough explanations of the algorithms and methods used in revenue management. This preparation gave the RM team members enough confidence in the systems to understand their outputs and be able to debug 80–90% of their questions regarding counterintuitive outputs.

As the field of revenue management continues to evolve, however, it is becoming increasingly difficult to debug some algorithms. For example, Bayesian forecasting, which relies on continuous updates of the model parameters as new data comes in, can be very difficult to interpret when counterintuitive results appear. As a rule, though, RM teams across the industry are sufficiently trained to understand the systems they use every day, which has allowed for unparalleled adoption of RM solutions — at least in this sector of the economy.

For the truck manufacturing sales team, on the other hand, we devised a slightly more rudimentary solution. The solution leveraged a smaller number of parameters and allowed us to communicate our approach to the entire sales teams in more simple terms — but still retained sufficient accuracy and complexity to make the results meaningful. This transparency was critical to our ability to get buy-in from the sales team — the primary users of the tool. What we absolutely wanted to avoid was that a single outlier recommendation might make a team member think that the tool was unreliable and outputted “bad” recommendations. To that specific end, we conducted group training sessions to share the model parameters with the entire sales team, and invited their comments, feedback and thoughts on the impact of these parameters. We also made sure the data relationships were intuitive enough to be easily understood by everyone on the team. And as we approached the end of the engagement, we followed up with one-on-one meetings with team members to address any lingering questions or doubts.

This approach drove significant team adoption, but despite our best efforts there were a few hold outs — team members who felt uncomfortable with our approach and chose not to use the tool. They made this decision despite the strong mandate from their own leadership team to use the tool, and clear performance improvement among other team members who did so.

These examples illustrate the importance of calibrating your solution to your user group. Only by taking this step can you hope to get widespread adoption of your solution. After all, what good is a superb algorithmic solution if nobody uses it because they don’t understand it or trust its outputs?

There is more to good data science than good algorithms

My experience implementing data science solutions for numerous clients has taught me that the following items are key to ensuring the success of your solutions and implementations:

1. Know your client’s goals and aspirations, as well as their technical ability — and their reasons for wanting to implement data science solutions in the first place.

2. Understand what is technically feasible and applicable given user capabilities.

Once you have a clear view of these two key aspects, you may need to help your clients align their goals with their capabilities. Once they are aligned, though, you should be able to set a clear path for the use of data science within the company. In the end, it’s all about client adoption of your solution. This is a matter of balancing complexity and your desire for an optimum solution, with the realities of what you will be able to actually implement — and with what users will be able to understand and adopt, given their current environment and capabilities.

I want to leave you with one final example of such tradeoffs. During my time at a leading enterprise-pricing software provider, the company’s science team created a wonderful pricing-optimization solution based on a very detailed segmentation approach. When this solution was first released in the early 2000s, it was significantly ahead of its time. However, once the tool was in market, the science team realized that while there was nothing wrong with the solution, it was creating significant user adoption challenges: It was just too complex to be well understood by the majority of clients, most of whom were just embarking on their pricing journeys. To support user needs, the team simplified the solution to speed adoption and then, over time, gradually added technical complexity back into the tool as clients became more educated in the art and science of pricing.

When I think about the relationship between adoption and complexity and interpretability, I often visualize the chart shown below. At the far left, the graph shows that overly simple models will result in little adoption: Nobody wants to use a demand forecast that consistently forecasts the average of historical data. As you move to the right across the graph, the complexity increases (so, consequently, does the value of your solution), and there is an exponential increase in adoption — with a small amount of variability around the mean adoption (orange line). As you move farther to the right, complexity increases to the point that it starts hurting mean adoption rates. Along with this decrease in adoption, a large increase in variability around the mean appears, which reflects the willingness of more advanced companies to adopt complex solutions. You will notice that I have not included scales on either axis of this chart, since these values can be very fluid as businesses become more accustomed to data science solutions. Furthermore, there is no single value that applies to any given business.

Where your next client falls along this curve will depend on their own journey to data science, their goals, and how well you are able to help ease them down the path. Ultimately, the calibration of the axes will depend on your client’s technical acumen and the strength of the foundational data — and on your ability to strike the right balance between complexity and understandability.
=========== Accounting for Uncertainty in Predictive Optimization ===========
In some cases, this result can be strengthened to a strict inequality. See Ito, Yabe and Fujimaki (2018), Proposition 1, for an example.

In the situation described here, we had unbiased predictors of the possible outcomes available. Often in predictive optimization that is not something that comes easy, as we are often looking for predictors of decision outcomes, not statistical estimators in the traditional sense.

Say, for example, that the decision outcome can be modelled as a function f of our decision dᵢ and a random variable e, meaning that aᵢ = f(dᵢ, e). Even though we might have available an unbiased estimator for the mean of e, say ê, it is in general not the case that f(dᵢ, ê) is unbiased for f(dᵢ, e). (The unbiasedness property holds when f is an affine function. In that case, sticking with our earlier notation, we could write âᵢ = f(dᵢ, ê) and have E[âᵢ] = E[aᵢ].) And, of course, the decision alternative that maximizes the expectation of f(dᵢ, ê) can be quite different from the one that maximizes f(dᵢ, e).

In the next section, we’ll share an example of this and illustrate the consequences of disregarding uncertainty.

Now, we’ll illustrate the problems of disregarding uncertainty in a model of markdown pricing. Through simulations, we can build intuition around the problem and measure the severity. Finally, we will also show how to take uncertainty into account to better solve the optimization problem.

Within retail, markdown pricing is a common example of predictive optimization. Retailers often have products that will expire from the assortment due to such factors as product replacement, fashion trends, seasonal elements, or sales performance. The problem for the retailer is to find the right discounts to maximize revenue given stock levels at the onset of the markdown period.

Markdown pricing problems typically involve setting prices for a large number of products across multiple markets. At its core, a typical markdown optimization for a single product in a single market can be broken down by finding the discount d* in a set of possible discounts D that maximizes revenue R(d):

Revenue is computed as the discounted price (1-d) · p multiplied with sales quantity:

Sales quantity at discount d is the minimum of demand at discount d, y(d), and the retailer’s stock S. A challenging aspect of real-world markdown pricing is finding a reasonable model for how demand y(d) varies with the discount d. For the sake of making our point explicitly, let us assume that demand is perfectly described as the product of a baseline demand e and an uplift factor u(d):

Further we assume, again for the sake of simplicity, that the uplift factor is described by an exponential curve with scaling factor c:

Pulling together the markdown model in a single expression, we get:

In reality, a model like this contains multiple sources of uncertainty. We typically do not know the exact functional form of the relation between demand and discount, nor the parameters involved. Furthermore, there is often uncertainty regarding stock levels at the onset of the markdown period (S), since discounts need to be set some time in advance. Predicting stock levels and demand at different price points in order to choose the optimal discount constitutes a predictive optimization problem.

To keep this exposition short, we will introduce only a single source of uncertainty, the baseline demand e into the model above. For numerical illustration we fix the stock level and uplift parameter respectively to S = 20 and c = 4.5. We model baseline demand as a lognormal random variable with mean 8 and variance 90.

Here is an R implementation of the markdown model:

So, let us say we now have available an unbiased estimator ê, meaning that E[ê] = E[e]. It is tempting to plug the value of ê into our markdown model and treat the problem as a deterministic optimization:

Proceeding in this way is very common in practice and, as we discussed earlier, there can be many good reasons why one would want to use this method as an approximation. However, this procedure now introduces two sources of error.

First, it disregards the sampling variability in ê. Plugging in an estimator does not, in fact, turn the problem into a deterministic one, since the dependency on the data sample means that our estimate is itself a random variable. Second, non-linearities in the objective function cause a discrepancy between the true maximized value and the deterministic maximized value.

In our numerical examples, we will illustrate only the second effect, and show how the minimum operator is biasing the deterministic optimization — and how one can overcome the problem. We therefore assume that there is no sampling variation in our estimate of the mean of e. The methods we show here, however, can also be used to account for sampling variance.

Solving the deterministic problem is straightforward. The optimal discount is 9%, corresponding to a revenue of 10.9. ( This model can also be solved analytically, provided we allow the discount to be real-valued.The optimal discount is then given by d* = min{1 - c ⁻ ¹, c⁻ ¹ log(S/e)}). Here is our solution implemented in R:

Since our estimator for baseline demand is unbiased though, we need to ask if our estimate of maximum revenue is also unbiased. And once again, the answer is no, as can be proved using the same arguments about Jensen’s inequality and the convexity of max- and min-operators as we used in the introductory example.

Although the solution d = 9% is suboptimal, it may still provide a worthwhile approximation of the objective value. Let us therefore use simulations to check the quality of this solution.

We see that the actual mean revenue in our simulations was merely 7.0, which is approximately 36% less than what the deterministic solution led us to believe. In this case, it is clear that the deterministic optimization exceeds by far the revenue we could expect from the markdown. At the same time, there is little reason to believe that the deterministic optimizer (9%) is maximizing the actual expected revenue one gets when accounting for uncertainty. How then should we account for uncertainty in this kind of model and set prices according to the true optimal discount?

We can find the optimal discount by ­performing stochastic optimization. One useful method for stochastic optimization, which builds naturally on the simulations we performed above, is the Sample Average Approximation (SAA) Method.

When using the SAA method, we start by simulating a large sample B of possible realizations of e. The idea is then to approximate the expected revenue at a given discount, E[R(d, e)], by the sample average:

It’s important to be aware that in our notation we have made explicit the dependence of baseline demand in the revenue function by writing R(d, e) instead of R(d) as we did above.

We now have a way to (approximately) evaluate the expected objective function at any discount. From there, we can apply a range of standard optimization algorithms to find the discount choice that maximizes revenue. In our case, the problem is restricted enough that we can simply perform an exhaustive search over the set of possible discounts, to get:

The SAA method is intuitive, easy to implement, and often yields good results. Under certain mild regularity conditions, one can also prove that the approximation error converges to 0 as the sample size increase. More details and an overview can be found in e.g. Kleywegt et al. (2002).

Here is our implementation using 10,000 simulations of the SAA method for the simple markdown model:

We see that the true optimal discount is 23%, which gives an expected revenue of 7.6. Using this true optimizer rather the suboptimal decision d = 9% leads to approximately 9% higher revenue. This is a typical example, in the sense that a) the deterministic optimization overshoots the expected value of the objective function at optimum, b) the objective function at the “deterministic maximizer” is substantially below the expected optimum, and c) using a maximizer attained by performing stochastic optimization gives an objective function value somewhere between that of a) and b).

A more realistic markdown model built on the concepts discussed above will typically involve several added layers of complexity, such as multiple markets with different demand elasticities, complicated stock constraints caused by supply chains with multitudes of stock-holding units and replenishment plans, dynamic price updates over time, and much more. In our experience, the core concept of a “baseline plus uplift model” is fairly robust and performs well in many situations.

The main point we want to make is that bias can be introduced if one naively treats a predictive optimization problem as a combination of a forecasting/estimation problem and a deterministic optimization. Whether the bias is large or small is difficult to ascertain without running simulations.

There are a number of situations in which simulations can be hard to implement. In the example of markdown pricing, we assumed a particular parametric model (a probability distribution) for baseline demand e. In many applications it might not be easy to identify a good parametric model for the uncertain factors. In these situations, we recommend using a bootstrap procedure to evaluate the uncertainty in relevant optimization inputs. If the uncertainty is large, or if the optimal decision is highly sensitive to variations in the input, one can proceed by using the bootstrap as a sampling device for a SAA-based stochastic optimization.

There are other situations where the computational cost of implementing a full-scale simulation is prohibitive. In these cases, one can adopt a statistical view and estimate the bias through scaling down the full model in such a way that the scaled-down model is representative of the full one. In the case of markdown pricing one could, for example, implement simulations on a random subset of products. Solving such situations thoroughly and effectively can often involve quite bit of ingenuity and out-of-the-box thinking, but in our experience — it is well worth the effort.

Smith, James E. and Winkler, Robert L: “The Optimizer’s Curse: Skepticism and Postdecision Surprise in Decision Analysis”. Management Science, 2006.

Ito, Shinji and Yabe, Akihiro and Fujimaki, Ryohei: “Unbiased Objective Estimation in Predictive Optimization”. Working paper, 2018.

Kleywegt, Anton J., Alexander Shapiro, and Tito Homem-de-Mello. “The sample average approximation method for stochastic discrete optimization.” SIAM Journal on Optimization, 2002.
=========== Data Science Python Best Practices ===========
Ten years ago, any quantitative PhD with a few statistics courses under their belt and an inkling of writing code could call themselves a data scientist — and not be entirely wrong. I should know — I was one of them. But as the field of data science has evolved, this is no longer the case. It’s not enough to be able to put together a few models in a notebook. Today, data scientists work iteratively in teams to build stable, scalable, and extendable data science products which drive core revenue-generating activities for their companies. The following guide fills a gap in the existing literature by focusing on data science software engineering practices required to build effective data products.

As an introduction, I suggest reading Andrew Fowler’s excellent post that outlines three key data science best practices: command line executability, YAML configuration files, and model testing.

In this guide, I extend this foundation to a full suite of Python data science best practices:
• Keep code and documentation together

Save time and sanity by accessing hierarchical YAML config files using Box. This package allows for dot-based access to YAML parameters, rather than Python’s traditional dictionary syntax; e.g., you will write instead of

Pip install and Box up this config.

Then, in the rest of your project, use “.” notation for easy access:

Best practice: analogously to , there’s a . Here’s an idiomatic way to use it with config files to facilitate reuse and modularity of functions/methods.

The above function looks for in . If it is not present, it checks , and if that’s missing as well, sets a default value (200).

Making all parameters in your package configurable has three advantages:
• You can quickly adjust the ETL, model, and other pipeline components by changing a YAML file rather than editing code.
• Your collaborators and users can make substantial modifications without changing (or even understanding) the underlying code.
• Your package can be readily integrated in automated pipelines that dynamically modify configuration parameters before launching your package.

For example, here’s a section of the config defining training parameters:

Now we can use to dynamically import and instantiate the specified sampler and classifier models.

Then, the Sampler and Classifier can be changed simply by modifying the config file, without changing any code.

In the above config file, we have different DB paths depending on the environment: dev or prod. Use environment variables to determine where you are, and parse the config accordingly. For example, let’s say the variable is either or , depending where your package is run.

This has the additional advantage of removing the need for (and or ) when accessing config parameters. Now you can just call instead of .

Here are three related best practices: (1) save your environment variables to the config (line 10 above) so that you can easily access them through the config without having to each time; (2) never save plaintext passwords or credentials in the config or in your code— use environment variables for that; and (3) save the environment variables in a file (add it to ) and load them using dotenv.

Poor design is the original sin from which subsequent difficulties in extending, modifying, and understanding your program arise. Learn clean design — it’s one of the highest-value time investments for a data scientist.

Plan the design and align with teammates by writing the docstrings for your key modules, classes, and functions prior to coding them. These can be reviewed (PR them!) and revised with the full team, so that everyone provides input and gains understanding of the whole package architecture, not just the pieces they are responsible for coding.

Of course a lot will change as you and your team develop. Nevertheless, initial planning will set you on a smoother development and refactoring trajectory. As Eisenhower said, “plans are useless, but planning is essential.”

Unlike statically typed languages, such as Java and Scala, Python does not require type declarations. Since Python 3.6, however, they can be added to your code using typehints.

Typehints serve two purposes: documentation and static checking using mypy. Using typehints obviates the need for documenting types inside docstrings. Additionally, the package reads typehints and, if you run it prior to execution, it’ll reveal elusive type errors that crop up intermittently and are hard to detect without static checking.

Think of mypy as a fast, “free” way to detect many issues (or at the very least, inconsistent typehints) without having to perform manual testing. Some IDEs, including Pycharm and VS Code, also check typehints statically and highlight inconsistencies as you code.

Keep code and documentation together

Keep documentation version-controlled together with code. For docs to be useful, they must be updated together with the code they’re documenting. Write documentation in markdown and render as clean, navigable HTML using Sphinx¹.

And, remember those comprehensive, Google-style formatted docstrings you’ve been assiduously writing before coding? Add them automatically to the documentation using Sphinx autodoc.

Managing dependencies while developing multiple packages in Python can be a chore. Ensure you and your collaborators have the same development environment by using virtual environments, which encapsulate and isolate both the Python interpreter and dependencies of your package. Two common virtual environment options are conda and venv. Pick one —it doesn’t matter which. I prefer venv, since it’s built into Python 3.3 and above.

Create the virtual environment using venv and install dependencies as follows:

Above, we create an environment with Python 3.6 interpreter, add three dependencies (more precisely, at least 3 — they each may have other dependencies pip will install), and then write the list of current dependencies into a file. This file, and not the env directory itself is added to git. Then your collaborators can recreate the same environment by initializing a new virtual environment as in line 2, activating it as in line 3, and executing in order to install the required dependencies.

Using conda is analogous, except that instead of , an file is normally used to save dependency information. You can find the details in the official documentation.

Modular components are easier to reuse, debug, and extend. Ensure that key parts of your pipeline — including data sourcing, preprocessing, training, evaluation, and prediction — are standalone.
• Source: Pipe data from multiple original sources into a staging DB (e.g., for simple applications, sqlite), separate files (e.g., images for training a classifier), or denormalized tables saved as pickle, Parquet, feather, or other formats.
• Preprocess: Read from staging DB/files, use asserts to validate key staged data properties, engineer features, transform, and save to new DB table(s) or files.
• Train: Read from files, use asserts to validate key preprocessed data properties, define model and training scheme (e.g., cross-validation), train model, and save (e.g., as a pickle, or h5 for TF/Keras models).

Each step should be governed by configuration parameters. If only a few configuration parameters are needed (e.g., paths to a few source files and an output file), provide them as arguments to a function wrapped with a click CLI (see Andrew’s post for details). If many parameters are needed, use a separate YAML config, with its path given as an argument to the click CLI wrapped function.

A clean, logical directory structure will reduce the cognitive load involved in finding, understanding, and changing functionality — both for you and for your collaborators and users.

Here’s a directory structure I recommend. This structure is for a project named supermodel. The files are for the most part examples — yours may be different.
• The root directory is not a Python package (it has no ), but the same-named directory , and its subdirectories, are Python packages.
• Unit tests (more on those below, in the “Unit test, a lot” best practice) are in (from the root) and are organized in directories and files mirroring the structure . For instance, functions of are tested in .
• contains a call to the function from , with the loading requirements from , like this. To simplify your life, avoid duplication of requirements in and .
• should load and Box your config file(s) and add in any environment variables, as described in the sections above. Then, in all other modules, you would import the config Box as .
• calls the main() function in , which is the single entry point for all functionality (see below “Use a single entry point” best practice). It enables the user to execute the package in a standard way: (with optional arguments following).
• Do you find it too confusing that the root directory and top-level package directory have the same name? If so, change one of the names (e.g., scikit-learn has root directory “scikit-learn” and top level package “sklearn”. In Pandas, on the other hand, both are named “pandas”). Personally, I avoid multiplicity of names, so I use the same name for both.
• This structure is not meant to be exhaustive, or set in stone. Depending on the nature of the project and the approach, you may have multiple top-level packages (e.g., a separate, extensive ETL or reporting process), directory for abstract classes if taking an object-oriented approach to the pipeline, a separate directory for integration and end-to-end tests, etc.

A key principle of good design is to wrap complex business logic in separate components connected with simple APIs. Having a single entry point is an application of this principle to your whole package: it’ll make it simple for users and collaborators to access package functionality and use it in their workflows².

The entry point should be the package itself: (to use the name of the example package above). To allow this, create a like this:

Then in your you would define two functions: which is decorated with click to process command line arguments, and , which is called from and defines the execution of your package. (Why separate and ? The former can’t be imported, due to the click decorations; the latter can.) Here’s an example from

The imports in lines 3–6 and the tasks dictionary can be refactored using the config file and (see the “Make all parameters configurable” best practice section above). Remember to not include in any of the other files in the package. After all, you want to be the single entry point to all package functionality.

Waiting to connect all the components of your pipeline — from source data ingestion to delivering the resulting predictions — to the end of the project is a recipe for disaster. By creating a complete dummy pipeline you will kill two birds with one stone: first, de-risk the external system integration challenges and second, create a “walking skeleton” on top of which you’ll develop the real functionality piece by piece. The walking skeleton will allow you to test that the pipeline executes successfully while you are developing.

What does the initial version of the walking skeleton entail? It varies by pipeline step (see the “Decouple, decouple, decouple” section above):
• Source: For each data store from which data is sourced, query a small sample of the data (not saved anywhere). Instead, write a dummy data table/file to the designated staging DB/bucket/directory for preprocess to pick up.
• Preprocess: Read the dummy table/file written by source. Don’t do anything with it. Write dummy preprocessed data for predict.
• Predict: Read the dummy table/file from preprocess. Write a dummy prediction in the format you expect the real prediction to have to the destination where it’ll be read by downstream systems (e.g., a Pandas dataframe with the expected columns saved as a csv to the agreed-upon S3 bucket).

Notify downstream users of your model to verify that they can read the dummy predictions and that the format of the predictions is suitable. As you develop, rerun the pipeline to incrementally test each additional piece you implement. In fact, schedule it to run in the same way the final product will typically be run, such as using a daily Airflow DAG run, cron job, or event triggers. By doing so, you’ll also be able to verify that all required external systems — data sources and predictions destination — are reliably accessible over time.

Python provides extensive logging facilities. By logging, instead of using print statements, you can specify (1) a dynamic log format, indicating where in your package the statement originated, and (2) different levels of severity, from debug (lowest) to error (highest). You’ll also be able to flexibly channel your logs to files, standard out and other streams, and even email.

While you don’t need to know Python’s full logging functionality, you should master the basics and use them consistently in your packages.

Configure your logger using a YAML configuration file, like this one.

Then, in your (after you load the YAML), set up the logger based on this configuration:

In each module, set the logger name to the module name in the beginning of the file, following the imports (line 5 below):

Yes, you have to do it in every file. But it’s a small price to pay for clear logs that indicate which module each log message is coming from.

Strive to write pure functions — functions that take arguments, process them, and return the result without changing state³. Look askance at all functions which return None; 90% or more of those should be I/O write functions.

Pure functions are easier to reason about, reuse in your pipeline, and unit test.

Write unit tests for all functions that encode business logic and perform data transformations. Isolate in small functions — and do not unit test — I/O, graphing, and orchestration (i.e., functions or scripts that execute your pipeline in the intended order). Also, do not unit test functions which thinly wrap external (e.g., Pandas or Scikit-learn) functionality — those packages have their own suites of unit tests. If the wrapper contains substantial logic, consider separating it. For example, split a wrapper that sets up model architecture and then trains into two functions: model setup (test this) and model fitting (do not test — sklearn/TF function). The thicker the wrapper, the greater the benefit of testing.

Use statements to validate necessary assumptions about data and key calculation results: if the assertion is false, your program will halt and display the assertion message. This will save the user time and frustration tracing an error raised later in the program due to, for instance, faulty input data⁴.

Below are examples of three types of assertions:
• Assertions of necessary properties of data inputs (line 8) — columns, value constraints, dtypes, etc.
• Assertions about the environment (lines 11, 12) — environment variables, key package versions, availability of external resources.
• Assertions about results (lines 19, 20) — prior to returning a result, check that it makes sense.

Especially during rapid, iterative model development, you want to know which preprocessed data, saved model, or results were generated by what code.

There are a few ways to accomplish this:
• No versioning: This is a valid option if the modeling project is simple and running the pipeline is very quick (i.e., you can always checkout a previous commit and quickly rerun to generate all outputs).
• Git hash versioning: In your saving functions (for saving preprocessed data, models, results), append the git hash of the current commit using GitPython. For example:

4. Full-featured data versioning: Use a system such as DVC.

For many purposes, options 2 and 3 are sufficient.

The above practices are a lot to take in all at once. The good news is that you don’t have to. Add them one at a time to your repertoire until you develop familiarity and comfort. They will serve you well on your journey from Jupyter-notebook slinging data science cow(boy|girl) to a clean-coding, product-developing professional.

Thanks to Andrew Fowler, Brad Allen, Cloves Almeida, and Akos Furton for providing feedback on an early draft of this guide.

[1] Markdown is simpler to write than reStructuredText, but it doesn’t capture the full functionality of Sphinx. For more complex documentation, use reStructuredText.

[2] In this guide, I’m using “entry point” in the general sense of a method for executing your package. I’m not referring to the entry_points argument in setup.py. You would not normally use those entry points in a data science project.

[3] Do you need to keep state in an organized way (as opposed to having a bunch of dicts and DataFrames floating around)? Use classes/objects. Within classes, use the decorator to designate utility methods that don’t need to use class/object attributes, and make those methods pure.

[4] Caveat: if you do not have control of how your package is deployed, and it may be deployed by an inexperienced devops engineer who thinks that Python is C++ and decides to use "optimization” flags, then using assertions in your code is dangerous because they are skipped by the optimization. In this case, use conditionals and raise exceptions instead of using assertions. Or you can provide explicit instruction on how to correctly deploy the package. (Explanatory note: unlike in C++, the optimization flags in Python provide close to zero improvement in execution speed. Don’t use them, don’t let friends use them.)
=========== Preparing the Supply Chain for the Next Disruption ===========
COVID-19 has put supply chains for all industries under enormous stress. From the sourcing of raw materials to the distribution of finished products, supply chains are at the frontline of this crisis. But this is not likely to be the last pandemic or global crisis. How supply chains are currently coping — or not — provides some valuable lessons for facing similarly widespread disruptions in the future.

Many firms have taken steps to digitize their supply chains. But many haven’t focused on global supply chain visibility and planning. Faced with the current calamity, companies are frantically diving into supply chain data, creating one-off models to assess risk. As a result, there is very little real understanding of what may happen within a matter of days — which supplier may shut down next, where production may need to stop, or where the next spike in demand will occur.

No wonder, then, that we are where we are today, with supplier delivery and production delays galore, empty shelves of essentials, and pandemonium at retail stores. But it doesn’t have to be this way. With a comprehensive supply chain AI strategy in hand, companies can help avoid similar issues the next time around. Here are some strategies to that end.

First, companies need to take a good look at the digital technologies that they’re currently using to manage their supply chains. Off-the-shelf AI solutions that help with visibility and planning have been around for a while. There are many offerings for specific use cases such as demand management, supply planning, or control tower visibility. But because each offering has its limitations, none is able to provide a comprehensive solution that supports both planning and visibility in both good times and bad.

On one hand, commercial control tower offerings are typically able to provide visibility across the supply chain. But because they don’t provide enough insights and capabilities for day-to-day decision support, they are not the optimal investment in the absence of black swan events. Additionally, these control tower offerings often lack the simulation capability that will allow managers to shift from monitoring what is happening today to predicting what will happen tomorrow.

On the other hand, many supply chain planning tools are great at answering demand-forecasting and inventory-planning questions, but they are not flexible enough to solve for internal and external uncertainty. Their predictive capability is limited to time-series forecasting algorithms, which are quite useful in modeling history to predict variability under business-as-usual cycles. But these same algorithms are insufficient during global disruption because they do not allow for the introduction of internal and external environment variables that are essential for making predictions in crisis situations.

We have supported clients across industries — including pharmaceutical, steel, and consumer goods — in building the initial elements of such a solution. These bespoke elements are essentially visibility and optimization layers that sit on top of standard enterprise resource planning systems to provide transparency and insights for day-to-day supply chain management. But they can be further enhanced to deliver sustained value and better support in times of crisis.

Getting the right technologies in place is only the first step — the organization piece must also be addressed. All too often, functions such as procurement, manufacturing, logistics, and quality assurance operate in silos, frequently without a cohesive plan to face adversity. While technically the COO’s office provides oversight, there is no clear owner of exchange to exchange (E2E) supply chain visibility and risk resilience. And the potential of advanced analytics to predict and prevent supply chain failure lies largely untapped.

Companies should set up a global team consisting of supply chain strategists, execution drivers, and technologists (including data scientists and data and software engineers). Reporting to the COO, the team should have three primary responsibilities:

· Augment business-as-usual activities across the supply chain. The team should focus on incorporating advanced mixed-modeling techniques that allow nontemporal variables to be introduced. These techniques will help improve the prediction and management of demand and supply during crises.

· Create analytics models. In addition, the team should leverage data and insights from these predictions to develop models that can simulate the sequencing of impact during unforeseen global crisis scenarios.

· Devise guidelines. The team should also put together playbooks that can provide guidance on how to manage the changes to business operations that will be needed in the face of such scenarios.

Real time (or at least near real time) E2E monitoring of the supply chain is imperative. Now more than ever, companies must set up a control room with live tracking of global suppliers’ health, inventories, cost of goods, production plans, shipments, demand, and external risk hot spots. As the workspace of the global control center team, this room would function as a hub for supply chain optimization during normal operations and as a supply chain command-and-control center in times of crisis.

This capability is analogous to the control room where BCG GAMMA teams have built and deployed AI solutions to optimize tail assignment and crew scheduling for an airline client. These solutions are particularly designed to manage unexpected disruptions, such as delays.

A supply chain control room will require investments in a variety of solutions: track-and-trace technology, including radio frequency identification tags on containers; Internet of Things relays for manufacturing lines; and real-time data processing. It will also be necessary to set up lines of communication and data transmission with suppliers and distributors worldwide. But companies should avoid investing in everything at once. They should first figure out which technologies they already have, what’s missing, the value that the new solutions would provide, and which vendors to procure them from. That will go a long way toward ensuring that these potentially large investments are sound ones.

For example, an industrial and construction equipment company provided its global supply chain control room with visibility into suppliers’ production plans, sales and operations plans, capacity commitments, and inbound flows, as well as into suppliers’ financial, geopolitical, and operations risks. During the 2008 financial meltdown, this visibility proved critical for managing the supply chain and developing contingency plans to mitigate the impact of the crisis. The company did more than survive — it improved its supply chain response time, reduced inventory costs, and increased its market share. This visibility also enabled the company to weather the disruptions caused by the Midwest floods of 2010 and the Japanese tsunami of 2011, when hundreds of suppliers and numerous transportation hubs were out of commission.

Given the critical importance of timely decision making, AI solutions need to cover the E2E supply chain as well as be more granular and near real time than in the past. The challenges posed by today’s interconnected global supply chains call for nothing less.

Use a Digital Twin to Predict and Cope with Disruptions

It’s unreasonable to expect AI to predict when the next global crisis itself will occur. But it can help create a digital twin of the supply chain, a simulation of all the complex systems in the actual supply chain. The digital twin can be used to model what would happen if different disruption scenarios unfold. A well-designed model can help predict demand spikes and supply shocks and recommend what steps are needed to ensure preparedness and resiliency.

In times of crisis, the digital twin can be used to compare the long-term impact of different action plans, making it easier for companies to make good decisions. With the help of advanced forecasting, firms may even be able to detect early signals for what the shape of demand will look like when the crisis stabilizes. Incorporating production planning will make it possible to optimize schedules by ensuring continued supply to minimize delays and maximize financial efficiency. Recommendations coming out of such models may include inventory diversions, customer allocation strategies, and changes in suppliers. Such actions will allow an E2E supply chain to adjust faster as things start getting back to normal.

To leverage the digital twin’s full potential, the control center team will need to be on the lookout to capture internal as well as external data, including how COVID-19 has spread across different parts of the world. It’s also a good idea to extend benefits of such AI-based modeling to suppliers and distributors in the supply chain. Setting up a symbiotic relationship will allow the entire network to benefit from these efforts.

Ultimately, for any of this to work, companies will need to establish a culture of disruption preparedness. That means acknowledging the critical role that the control center plays — not only in developing crisis management playbooks and handoffs for all of operations but also in managing change across the supply chain. To shape management policies that are robust, the control center should partner closely with safety and quality functions. Empowering the control center to optimize daily operations will ensure that when a crisis arises, the organization will know how to work with the control team for best results.

The COVID-19 pandemic is not likely to be the last crisis that supply chains will face. But if companies take the opportunity to leverage its lessons, they will have a chance to reduce the damage of the crises that are still to come.

Kartik Pant is a lead data scientist, BCG GAMMA, in the Washington, DC, office of Boston Consulting Group. You may contact him by email at pant.kartik@bcg.com.

Pepe Rodriguez is a managing director and partner in the firm’s New Jersey office. He leads our global work in the digital supply chain. You may contact him by email at rodriguez.jose@bcg.com.

Olivier Bouffault is a managing director and partner in BCG’s Paris office. You may contact him by email at bouffault.olivier@bcg.com.

Duane Weeks is a senior advisor in the firm’s Chicago office. You may contact him by email at weeks.duane@advisor.bcg.com.

Boston Consulting Group partners with leaders in business and society to tackle their most important challenges and capture their greatest opportunities. BCG was the pioneer in business strategy when it was founded in 1963. Today, we help clients with total transformation — inspiring complex change, enabling organizations to grow, building competitive advantage, and driving bottom-line impact.

To succeed, organizations must blend digital and human capabilities. Our diverse, global teams bring deep industry and functional expertise and a range of perspectives to spark change. BCG delivers solutions through leading-edge management consulting along with technology and design, corporate and digital ventures — and business purpose. We work in a uniquely collaborative model across the firm and throughout all levels of the client organization, generating results that allow our clients to thrive.

For information or permission to reprint, please contact BCG at permissions@bcg.com. To find the latest BCG content and register to receive e-alerts on this topic or others, please visit bcg.com. Follow Boston Consulting Group on Facebook and Twitter.
=========== Lagrangian relaxation can solve your optimization problem much, much faster ===========
Today’s advanced analytics techniques are providing businesses with new insights and opportunities that can improve decision making through optimization. We were recently presented with one such opportunity when a food retailer asked us to develop machine learning and optimization solutions to help the company establish prices that would optimize profit and reduce waste. After initial experiments and tests in real stores, we decided to scale the solution to large markets, which offered greater rewards, but presented new challenges. The main challenge was that on this larger scale the models to solve were significantly larger, which caused our Gurobi-powered optimizer to struggle. This phenomenon is sometimes referred to as “the optimization curse.” The optimization curse refers to the fact that as optimization problems become larger, they get exponentially harder to solve, and is described very well here.

In this article, we will explain how a well-known optimization technique, Lagrangian relaxation, helped our team reduce optimization time by a factor of more than 10 while improving the quality of the outcomes. This technique was very helpful to us, and we believe it can help others who also work on optimization topics. In particular, it can be applied to multiple use cases for which one needs to solve large MIP with coupling constraints, such as in scheduling or stock allocation problems.

Before we jump into content, though, we would like to thank Professor Pascal Van Hentenryck for helping us understand Lagrangian relaxation, as well as for everything he taught us about optimization.
=========== Anti-patterns in Enterprise Data Sciences ===========
What is the market not telling you?

Nassim Nicholas Taleb, in his book “The Bed of Procrustes” noted that “people focus on role models; it is more effective to find anti-models — people you don’t want to resemble when you grow up.” While Taleb’s quote is commonly referenced across journalism and social media today, it isn’t as applicable to real life as we might think — at least not to real business life. Rather than seeking out contrarian voices, industry leaders tend to be overly influenced by mainstream thinking. Consider all the searches for those SEO-optimized webpages that perennially comprise page-one search results, reinforcing the same timeworn, made-for-PowerPoint sets of ideas over and over again. “Establish a data lake,” they instruct business users. “Invest in data sciences.” “Compute and experiment at scale.” And on and on. It is staggering to consider the number of companies fundamentally built on such “best practices”, their leaders devoutly believing that in doing so they will roll out inevitably trend-setting products and features and win the battle for customers.

Thinking in the fields of Data Sciences and Artificial Intelligence (AI) has been no different. A quick search for “best practices in machine learning” returns more than over 200 million Google results. In fact, “machine learning” has been the poster child of enterprise success stories for the last decade. It has led to the astronomical rise in data scientist salaries and demand. With data being the new rocket fuel, data science has become the hottest career path of the new century. Fueled by the astronomical amounts of money raised by start-ups, these corporate stars boldly leverage Machine Learning to drive AI-based insights to the heart of the enterprise. In the 2019 MIT SMR and BCG Research Report, fully 90% of respondents agreed that AI represents a business opportunity for their company, while the lack of it was deemed a strategic risk. “What if competitors, particularly unencumbered new entrants, figure out AI before we do?” quoted one of the respondents.

There have been multiple forces pushing AI’s growth, including its ability to provide greater visibility into consumer behavior and demand, an increase in the accessibility of information, technological ease and progress in the AI field — along with a much better understanding of how to integrate Data Sciences and Analytics into everyday operations. Enterprises are also increasingly focusing on a lifetime-value-centric approach, which represents a departure from purely retrospective approach of understanding and rewarding customers. Instead, companies are embracing a more forward-looking approach that leverages predictive and prescriptive components of the analytics spectrum.

But as encouraging as this all sounds for AI enthusiasts, many statistics offer a very different perspective. For example, the MIT SMR and BCG research report also highlights that seven out of 10 companies surveyed have reported minimal or no impact from AI. Of all the companies that have made some investment into ML and AI, less than 40% can correlate business gains in the past three years to these same investments. If these tools are so transformational, if market dynamics are more or less equally exposed to every company, and if everyone has access to the best practices in processes, technology, and workforce management, then one has to ask: Why are there only a handful have great AI stories to tell the world?

A quick scan of industry reports points to hundreds of factors causing these poor returns on investment. With respect to AI, companies are said to be “not doing the right things” or “doing things wrong.” When one digs into these vague charges — when we put on our data scientist hats and boots and start digging into the nuances of how enterprise analytics run on the ground — a clearer picture emerges. Upon this closer inspection, some very interesting patterns emerge that are consistent across industry verticals, geographies, and business functions. These patterns are repetitive and easily reproducible across functions. Avoiding these patterns would seem to be an almost “natural” solution to persistent AI-related problems. Together, they comprise what might be called industry “worst practices” and emerge across different organizations as, in effect, “anti-patterns.”

Seen from the 30,000-foot level, these anti-patterns include:

· Failure to pay attention to customer behavior across the buying journey

· A focus on evolving first instead of evolving fast

· An overemphasis on model building over model optimization

· A failure of inclusivity in AI and ML team building

Rather than stumbling on the nitty gritty mechanics of implementing AI and ML, companies more often fall victim to program-level design flaws and inefficiencies. These failures prevent many companies from ever realizing the full value of these costly and otherwise quite powerful technologies. But the good news is that these anti-patterns are, in the main, organizational problems that, once addressed, can unlock computing power and generate significant return on investment. Let us take a closer look at each of these worst-practice anti-patterns and how to avoid or correct them.

The etymology of “average” goes back to 700 BC, but the adoption of its more “action-oriented” definition began in medieval and early modern Western merchant-marine law contracts. These contracts had to do with cases in which a merchant sailing ship encountered stormy weather and had to jettison some cargo to make the ship lighter and safer. In that circumstance, the contract specified, merchants with goods onboard the ship had to pay an equal share of the value of all the goods lost overboard — rather than being libel only for the cost of their own goods thrown overboard. From that point on, this concept of “averaging” was adopted by British insurers, creditors, and merchants and popularized the idea of spreading losses across whole portfolios of assets. Soon thereafter, the concept of averaging applied to all manner of things both quantitative and otherwise.

In marketing sciences, averages have been used not only to ease consumption of quantitative details, but also to solve for technology constraints. Since organizations did not yet have the technological and operational capabilities to analyze customers and their behaviors at an individual level, they used the concept of averages to simplify their understanding of the marketplace, and then build and execute marketing plans on that understanding.

However, the difference between theoretical and practical implications of averages have diverged extensively in the past decade. Averages have very high usefulness across domains, but they do not do a good job of telling us the “why” and the “so-what” of what companies do. With the advent of varying dynamics in individual preferences and behaviors, the traditional approach of creating marketing segments and reporting average performance metrics has become passé. Open-source advancements in technology, operations, and interpretation provide rich mechanisms to understand and act on the basis of individual — not average — preferences. This ability to mass customize has becomes one of the core differentiators for companies that build scalable, real-time product and personalization capabilities. Meanwhile, those companies and their PowerPoint slides depicting segments and averages are quickly slipping into irrelevance.

A decade ago, it would have been unheard of to base business planning on individual-level insights or personalization. It is now standard procedure among leading enterprises. With appropriate data modelling and amid a plethora of algorithms, the ability to cluster the elements of individual journeys can empower us so much more that the use of “averages” to drill down into behaviors and understand the “why” behind that customer’s actions, interests and intent. Understanding buyer motivation reveals those critical intervention points that enable us to build personalized offers and experiences.

2. Not Differentiating Between Customers and Their Journeys

A lot has been written about The Golden Circle, a theory that places great value on an organization’s ability to understanding the “why” behind what they do. The concept’s originator, Simon Sinek, encourages leaders to look at each problem with the same lens — that of the fundamental purpose of the organization. Once that lens has been defined, strategic objective can be formulated, followed by tactical planning to build and then execute that strategy.

This progression seems simple. The problem is that the simpler things get, the more difficult they often are to correctly execute. With respect to marketing sciences, the adoption of the Golden Circle gets even muddier as we cut across the circles. This is not only because of nuances created by complex technology landscapes, or because the existence of so many algorithms can paralyze enterprises with indecision. It is also because of our inability to clearly define exactly what it is we want to understand about each customer.

In the myriad of digital customer interactions and brand expositions, behaviors metamorphose into highly dynamic facets, leading to different personas. A customer making a purchase on a mobile handset will go through different exploration trajectories, interact with different layers of the business ecosystem and exhibit altogether different behaviors than another customer who buys via their television. The characteristics of these journeys continue to diverge as the product categories do the same. For example, the path to purchasing a mobile handset will vary significantly from that of securing a home loan. This variation is only magnified by the use of different platforms.

Standard marketing-segmentation approaches look at the “average behavior” of a customer. In doing so, they fail to capture the subtleties around journeys, instead yielding segments that look good on PowerPoint charts but are rarely actually used to design marketing programs. Undaunted, many organizations keep working with these “average” segments, shaping their marketing strategies and building new offerings around them. When the campaigns and experiments are run, the results may seem good. But as we drill down into the data, the improvements and the benefits begin to fade.

Since business KPIs are easier to interpret in terms of customers (for example, new customers acquired in the last 3 months or increase in average spend per customer), the problem definition and measurement have to be done at the customer level. But there is no singular “level” because, as customers exhibit different behaviors, these varied personas emerge. Since targeting is done to alter a specific behavior, the development and execution must be done at a granular level of behavior per persona.

As an example, for a use case that deals with predicting the propensity of a customer to buy a new product, one of the objectives could be to move the customer to make a purchase, thereby increasing the measurable KPIs of product holdings and revenue. The development of the predictive model to achieve this must be designed around the observed persona-behavior level. For instance, all customers who are sensitive to price and promos and have low product awareness but a very high digital affinity will be a part of one behavioral segment. The targeting must also be done with respect to the customer journeys. If the customers in a behavioral segment spend more than 20 minutes online and browse a new product category and have a potential lifetime value exceeding $20,000, then the best model might trigger an EDM that offers a 10% discount during weekend mornings when they are most likely to be browsing.

Theoretically, the three separate tasks of defining, building and executing on a predictive model may seem easy. But the missing linkage between these stages is not only one of the most common anti-patterns observed across enterprises, but also the one that can have the biggest impact on topline.

3. Not Moving from “Data strategy” to “Feature strategy”

The notion of “Big Data” surged around the beginning of last decade, with major publishers and influencers speaking about the need for organizations to get on this wave early and strong. Those who failed to do so, it was said, faced the prospect of long-term failure. This message was incessantly pushed across all touchpoints (albeit in varied conventions and formats) and led to many new analytics offerings including Hadoop, Spark, and Deep Learning. Analytics and technology services companies consulted to help companies assemble the right technology and analytics stacks, and to understand how to develop the “right” data strategy. This resulted in many companies dumping their data assets into a pool, creating never-ending block diagrams and connecting processes to showcase all the rich information they had. The effort could take anywhere from 6 months to several years as new roles were created, governance became a highly regulated process, and everyone became so excited about all the benefits to come. But they weren’t excited for long.

All the major companies today have already adopted Big Data analytics in some form or the other and set up a Data Lake as a part of their initial, foundational exercise. Few knew what to do with these large bodies of data, with a very limited portion of these enterprise data assets being used in any significant way. In the process, several roles were terminated, people either transferred to different divisions or simply became lost amid burgeoning roles and hierarchies.

And yet, even amid all this turmoil, small teams of data scientists at some companies kept working on their standard use cases, leveraging data from the data lake to generate “features” and “signals” to help them understand the “why” behind what they were doing. These companies that increasingly focused and built capabilities around a “feature strategy” saw their data lakes continually updated and leveraged, while other companies saw the value of their data assets dwindle away.

This concept of a “feature strategy” includes thinking about the “why” before dumping assets into the pool. This is a process of developing use cases that drive growth, and of analyzing the capabilities required to build engines to support these use cases. This, in turn, leads to the creation of specific features and data assets to fuel these engines. It also includes building and inferring customer journey properties and augmenting use cases with information beyond traditionally used averages and derivatives (rations and deltas). Over time, the top-down approach of feature strategy also provides a way for businesses to make the case for increases in “better data,” as opposed to simply more “big data.” In the process, it becomes possible to measure the benefit of individual data sources and fields and to track their adoption by different teams and functions.

For anyone seeking advancements in their analytics journeys, it is crucial to move away from data lakes to what might be referred to as continuously evolving “data rivers.” At the same time, now is the moment to move from a data strategy to a feature strategy that sheds new light on not just creation, but consumption.

Significant advancements have been made in experiment design and Omni-orchestration that reach across multiple journeys, devices and touchpoints. But very little has been done to optimize what to measure or what “implicit feedback” means in the design process. There are two key factors adding to the confusion. First is observer bias and self-serving bias in experiment design, wherein companies shape their experiments to align with an expected outcome. An example of this might be the way an automobile company chooses features, build models and designs audiences to sell a familiar design. The second factor is the mindset of data science teams that prompts them to maximize the lift over baseline but not necessarily minimize the difference between the best that is possible and where they are at that moment in time.

Traditional experiment-design methods have been devised and leveraged solely as “comparative frameworks” to understand the incremental value (A versus B), and not differential value (“what is the most I can make from an experiment”). For example, if a campaign shows 300% improvement over baseline, does it mean the campaign has done well? Definitely. But does it also mean that the business done as well as it could? Maybe. We are better off maximizing the benefit over what we can see versus what we cannot see. In a healthy experiment design, getting a lift over baseline (or, control or holdout) is the basic measure of success. But for the purposes of acquiring feedback and enabling continuous improvement, it is critical to have a gold standard — a North Star that guides your experiment design.

The fact is that, when it comes to possible improvement, a lot is often left on the table. Two related thought experiments are gaining traction in the industry. Both tend to create a paradigm shift in how experiments are approached. The first experiment is the idea of reinforcement learning. This is a more technical idea that has an established process and strives to balance exploration and exploitation to minimize, over time, the differential between “the best a design can do” and the “current state of the design.”

The second thought experiment requires a change in mindset, pushing teams to think of the performance of their experiments with respect to this clearly defined, aggressive notion of the “North Star” goal. In this framework, even if a predictive model yields good market results (that 300% over the baseline model), the experiment will be deemed an approximate failure if the delta relative to the North Star is too large.

This approach to measuring success is, of course, highly debatable. But it does serve to enforce appropriate focus on the design and rationale for experimentation, thereby increasing the integration of business and sciences in large-scale marketing campaigns. If organizations need a good experiment-design framework, the mandate should be to build a strong scientific foundation, put rigor around the rationale for the definition of “North Star” within the organization, create a set of measurement levers around that North Star, and use cutting-edge practices such as reinforcement learning to achieve that gold standard.

5. Not Considering That a Fast Mover Often Trumps a First Mover

The ability to establish a trend within a product landscape has always been a much sought-after skill among both established and startup companies. To help companies grab the high ground, there are more than 7,000 established platforms and vendors in the Martech space alone (as indicated by chiefmartec in their 2019 report), and the list is expected to grow anywhere from 5–7% annually. Even though these service-provider offerings are growing and increasingly focused on gaining first-mover advantage, it is a different playground altogether for those seeking this service.

Marketing science is a process involving several components and some companies have set the foundation for designing and scaling them. But the key is not to aspire to be first mover, but to get the process right and learn to evolve quickly. Each trend in the market is like a living organism: It changes every day, adapting to its ecosystem and modifying itself to stay relevant.

Customer 360, for example, was on the receiving end of the hype in the mid-2000s and, to this day, remains for many a must-have. Even so, companies both big and small continue to struggle with the tremendous breadth of this concept — and with correctly implementing it. Building, using, and continuously evolving a customer 360 is comprised of three separate, organically improving processes. Companies that stop once their 360 is built and in use — but neglect to continue evolving it to match an ever-changing customer landscape — soon likely find themselves lagging behind. More than building a data product itself, it is important to focus on speeding up its usability, reproducibility and usage. Emphasis must be on demystifying the offerings for all those involved, and on building a common vocabulary within the organization.

6. Not Predicting and Investing in Winners

Ask data science teams across organizations about the validation approaches they take when building machine learning products and solutions and they’ll probably start talking about techniques like cross-validation, RMSE, loss metrics and so on. Very rarely will they emphasize business metrics such as profit, reduction in cost to acquire new customers or digital transition rates. This focus on technology is in not ubiquitous, nor is it technically wrong. It does, however, raise the issue of optimization-driven development. Companies are dedicating a great deal of resources to automate the process of model development and insight generation, achieving this by parallelizing the crude effort required for steps such as model selection and feature engineering. But once the model is built, very rarely do they put additional effort into optimizing the “consumption” of the model.

To illustrate this point, let’s say that a company builds five different propensity models, one for each of the five products they sell. The goal is to identify the next best product for their customers, and to end up with similar propensities across all products. In most of our observations, people tend to use plain heuristics and rule-based mechanisms to identify the “best 1 out of 5” products that should be recommended. Rarely do data sciences team apply the lens of business optimization over these raw propensities to identify this “best” offering. If, instead, they took the raw likelihood of purchase and leveraged information about operational costs and margin associated with each product, they could recommend the product that would best optimize lifetime value or profitability.

Similar conditioning is observed in the construction of marketing campaigns. Since the cost of marketing is very low, the thinking goes, there is no need to bother with optimizing the targeting mechanism. Companies either target the masses through a series of drip-marketing tactics, or use a model heuristic (top 1 or 2 deciles) to run experiments and measure performance and benefits. Frankly, such approaches are lazy and inefficient, leading to content fatigue (too many touchpoints), poor benchmarks (too many or too few people) and the rewarding of inappropriate audience segments. Propensities without optimization (for model, audience, offerings, etc.) is a grave mistake for organizations that envision building large-scale data products.

In India, there is a concept of Vasudhaiva Kutumbakam (originally from Sanskrit: वसुधैव कुटुम्बकम्). This phrase appears in Maha Upanishad, a foundational Hindu text. It consists of several words: “vasudhā,” or earth; “ēva,” or indeed; and “kutumbakam,” or family. The phrase literally means “the world is one family” and has several interpretations across Indian heritage and beyond. It also has relevance in the world of business, in the context of enterprise-analytics setups that host several teams, functions and roles. In this context, it applies to the entire process from creation to consumption of analytics.

Over the last decade, there has been a sharp rise in the amount of literature and products expounding on the importance of the collaborative, automated and cross-functional development of analytics. This approach was first popularized as “agile development.” Now we witness the rise of automated machine learning. At the same time, self-service BI/analytics platforms have come a long way. And everyone is debating, misusing, or confusing everyone else about the differences (or, lack of) between Machine Learning and Artificial Intelligence. Technologists have started strengthening their claims (and businesses have started believing them) that large-scale problems, whether well-defined or not, can be solved simply by incorporating the latest and hottest solution or platform.

The kernel of value, however, resides with enabling the consumption — and not just the delivery — of the insights derived from these tools. A team’s ability to consume this information is only as good as its ability to facilitate the exchange of information. From an external point of view, this seems like a minor problem. Internally, people tend to debate that the gap between a fantastic creation and mediocre consumption is simply due to an interstice within the organization. What these same people fail to realize is how deep this interstice really is, believing that it is the result of technology gaps rather than design fallacies.

This all leads us back to Vasudhaiva Kutumbakam and the topic of inclusivity. Companies that excel at innovation inculcate the practice of “analytics inclusion,” defined as the inclusion of processes, people, and collaboration that drives better information visibility across all stages of development. In such a paradigm, the data scientist is kept in loop about the broader vision and business outcomes, and senior management is kept in the loop about the data and processes. One might doubt that senior executives having anything significant to add to questions of data or models. But the purpose of inclusivity is not necessarily to problem solve, but to extend visibility across all layers. It doesn’t always mean more people must be involved across processes, but it does always tend to open new communication bridges and create environments in which people are able to ask more and better questions, identify the owners and the hits and misfits, and lead to designs that represent the objectives of key people across the organization.

Solving Anti-Patterns May Not be Sexy, But It Delivers Results

Building data products and large-scale experimentation workbenches is a difficult but clearly solvable task — if people are equipped with the correct toolkits and aware of worst practices and how to avoid or correct them. They must learn to step away from the concept of averages, and leverage new data capabilities to work at the level of individual customers. They must learn to focus on the behavior of customers as they move through the buying journey. They must learn to acquire not just more data, but better data — the right data. They must hold their experiment designs to a higher standard, learn to evolve quickly, optimize for long-term value and profitability, and think clearly how proper team composition is intrinsic to overall success.

Moving away from these anti-patterns requires a change in mind-set and culture, and it can take a lot of effort to alter long-standing corporate cultures. The changes may not be as “sexy” as some of the things that come up in Google searches or are touted in the current analytics-space market and, as a result, may be met with resistance by people who are swayed by the latest shiny object. Nonetheless, with appropriate, consistent top-down messaging within the organization, the successful implementation of these changes invariably becomes the core differentiating element. Fixing these anti-patterns enables large-scale data and analytics integration in the company. In this day and age, that must be a guiding bottom line.
=========== Topological Data Analysis and Guacamole Cats ===========
Change one pixel of a photograph, and you can fool even the best computer-vision algorithms. The solution: Building models that don’t fall for those tricks, and that can generalize far beyond what you’ve taught your algorithm.

This article is laid out in four sections, which take us along a journey to discovery: description of the problem → description of the tool → how this tool can solve the problem → further implications.

Note: An in-depth mathematical treatment of some of the ideas presented here can be found in our published research paper on IEEE (alternatively, on arXiv).

It’s another bright sunny day in San Diego and you’ve decided to walk downtown and grab some lunch. On the way there, you come to a four-way stop. Unbeknownst to both you and the self-driving car barreling down the street toward the intersection, the stop sign just ahead has been altered:

While the pithy comment spray painted on the sign doesn’t hide from you the fact it’s still a stop sign, the story might be completely different for the car. The algorithm running the autonomous vehicle could misinterpret this object in any number of ways: It could see this stop sign as a cat or an airplane or, if it’s an especially smart car, just an “other” object.

The fact is that you can fool current state-of-the-art computer-vision algorithms by changing a single pixel in a photograph (that’s right, a single pixel!). Of course, this kind of alteration would be completely imperceptible to a human. We’ll refer to these kinds of examples as “adversarial examples.”

Here’s an actual adversarial example constructed by researchers at MIT, in which we see NN outputs that show the probability distribution over classes for the input images:

This article presents a novel approach and application to constructing robust classifiers that are not easily fooled by adversarial examples. More broadly though, this approach could be used for any type of classification problem that require some forgiveness in either the data or the parameters of the model. Said differently, if you need an algorithm that can generalize far beyond what you’ve taught it, our TDA+ML approach might work well for you. This approach has the potential to be much less biased than current ML algorithms. These algorithms tend to do a good job of learning precise mathematical structures. But they have a hard time generalizing beyond this. (We’ll leave a discussion of the ethical concerns inherent to ML algorithms for another article.)

The approach presented here is based on research I did while working on my physics PhD. I presented my research at the IEEE ICML 2019 conference, and it was published in the proceedings. In particular, this approach is based on tools from topological data analysis (“TDA”) and is used in conjunction with conventional machine learning methods. As a consequence, this approach has the potential to be much less biased than current ML algorithms.

Topological data analysis, which is based on algebraic topology, can identify significant global mathematical structures that are out of reach of many ML methods focused more on the specific structure. Without an extensive mathematical background, it can be difficult for readers to follow a purely mathematical explanation of TDA. I will, instead, opt for a more intuitive exposition. For the mathematical details, please see:
• Edelsbrunner’s & Harer’s text on computational topology (sidenote: check out the front cover of the text)

The novel combination TDA+ML results in a robust algorithm that resolves some fundamental issues present in other current state-of-the-art methods. We’ll analyze this performance in the context of noisy image classification.
=========== A Data Scientists’ Guide to Building Credible Risk Models — Part 1 ===========
Advanced machine learning (ML) algorithms are making steady inroads into critical aspects of banking such as fraud detection and collection. Meanwhile, predictive modelling in credit risk management continues to rely primarily on traditional approaches such as Logistic Regression (LR) and, to a lesser extent, General Additive Models (GAM). ML has the potential to significantly enhance the performance of LR-based risk scores, but its use in credit risk modeling has been limited by hesitancy among risk managers to adopt ML. This reluctance may be explained by several reasons:

· Regulatory discomfort: Low comfort levels among regulators with ML models, mostly attributable to limited interpretability and explainability

· Limited understanding of advanced ML: Lack of understanding among business stakeholders of the analytical aspects of ML and how it differs from conventional techniques

· Algorithm focus over risk-outcome focus: Overemphasis among data scientists on utilizing the latest ML techniques at the cost of fully understanding risk aspects

In this first of a three-part series, we will address this third reason and explain how ML practitioners can develop predictive models by keeping fundamental risk considerations on the forefront.

To do so, we focus on two key facets of risk models that can help make ML models more acceptable to risk practitioners: Model Design and Holistic Model Validation.

A substantial amount of time at the start of the design process should be spent understanding the precise risk problem the model is expected to solve. How exactly the model is to be deployed and used also needs careful consideration. All too often, though, the ML algorithm choice is disproportionately based on a “newer is better” credo or with a myopic focus on higher accuracy. An overemphasis on accuracy as the ultimate measure of model performance is a sub-optimal and potentially risky practice — and at times can reduce the potential business benefit derived from the model.

While accuracy is a critical facet of a risk model, it is not the only one. The following model design features should be considered, while maintaining a balance between model accuracy and other risk considerations:

· Event Definition: Vital to credit risk modeling is the process of “defining the event.” Since “higher GINI” is often the predominant factor in model selection, modelers sometimes choose a shorter performance period when building the risk model. When other factors are held constant, a model designed to predict for a 9–12-month performance period is likely to have a much higher GINI than a model designed for a 15–18-month period. But if 50–60% of defaults happen outside the shorter model-designated event observation period, the model will have limited business value. The temporal decay performance of such models would be poor, giving the model less value in the real world.

Alternatively, the event and the event observation period may be mathematically determined by calculating roll rate (precision) and capture rate (recall), then finding the optimal intersection point between the two. The performance period ultimately chosen for the model should be aligned with the underwriting and collections team, over and above the mathematical optimization.

· Handling imminent defaults and high-risk populations: In any banking scenario there are always borrowers on the brink of defaulting. Such high-risk populations can be easily identified without the need for models or algorithms. And they can easily be excluded from the modelling population using gating rules.

Most modelers use modelling exclusions, such as customers who have defaulted in the immediate past or who are currently in default. Some modelers, however, continue to keep obviously high-risk populations in the modelling sample, which tends to inflate GINI but reduce business value. To be of greater benefit to business users, models should excel at predicting defaults that are less obvious.

· Enhanced use cases of risk score: Until recently, most risk models were used to support only go/no-go credit decisions. With the rising demand for personalization and underwriting treatments differentiated by risk profiles, models are now expected to do a lot more. For example, a well-designed risk model must support allocation of customers to different underwriting work-streams. This can be ensured by creating swim lanes that integrate risk scores with customer journeys, thereby helping to place customers in appropriate treatment buckets. Risk scores must also support pricing and credit limit allocation.

As noted above, the quest for strong models often focuses on increasing “accuracy.” Accuracy is often referred to as a model’s “power,” in this case to differentiate between “events” and “non-events.” The accuracy of a model is measured by different metrics such as GINI (ROC, CAP), KS, and Precision/Recall. Accuracy is, of course, a prominent attribute of a model, which should be balanced with model stability and interpretability. But the efficacy of the risk model may be more holistically assessed if the following incremental measures of model performance are also considered:

· Rank ordering of events with score: This is a standard practice in risk management, though often overlooked by ML enthusiasts. Rank ordering is good indicator of risk differentiation across the predicted score buckets, and when exhibited across test and out-of-time samples it can also help assess a model’s performance stability. When using models in risk-decision making, the rank ordering determines the specific operating thresholds based on the risk score.

· Temporal decay: The risk differentiation of a score should ideally hold over the entire maturity of the exposure. The model should thus be tested for its predictive power by evaluating events even outside the model-defined performance period. The ultimate reason for building a model is to reduce loss over the life of the loan, so it is important that the model has reasonable predictive power outside the model-defined performance period.

· Reverse bi-variate: “Black-box” models, which may include arcane AI logic and modeling, are considered poor work products by many data scientists who, instead, use “explainable AI” schemes. The transparency of even these schemes is a matter of debate but can be improved using simple approaches, such as plotting reverse bi-variate. Observing the pattern of individual x-variables (predictors) over the predicted probability from ML algorithms enables data scientists to measure the business intuitiveness of each predictor.

Push ML Intervention for the Benefit of All

The current hesitancy among credit risk managers and regulators to rely solely on ML models is likely to endure for the foreseeable future. Thus, the foundation or anchor score in any risk assessment framework will continue to rely on logistic regression output. The performance of the risk-assessment framework can be enhanced, however, by judiciously superimposing advanced ML models over foundational LR model scores. The impact of doing so will improve performance at the margins, but by meaningful amounts — especially for large-volume lenders. Thus, ML intervention should be a standard practice, at least among larger users.

To ensure that the practice of risk assessment reaps the benefit of advanced ML, data scientists should work towards enhancing the credibility and usability of their output. If advanced ML risk models are to pass the justified scrutiny of risk managers, ML practitioners must make the effort to address, in totality, the risk aspects of predictive solutions. By making model design and holistic model performance assessment central facets of their modelling efforts, data scientists can provide end users with tools that provide better risk prediction, more tailored customer service and a stronger bottom line. We encourage you to stay tuned for parts 2 and 3 of this series where we will explore model design and model performance assessment in greater depth.
=========== Model Validation: A Science — and an Art ===========
Learnings from model validation in credit risk for other domains

When data scientists consider the topic of model validation, a few names typically come to mind, such as Adjusted R-Square, Accuracy, Precision, Recall, Concordance/Discordance, GINI and Kolmogorov-Smirnov (KS). In this article, I will address the common pitfalls of GINI, a widely used measure for assessing performance of binary classification models. I will conclude with a discussion about why data scientists should adopt a holistic view of model validation instead of looking at single metric.

GINI is calculated from AUC-ROC curve. It is a graph on which the cumulative percentage of the binary target variable (sorted from 1 to 0) is plotted against the cumulative percentage of the corresponding population. The business world is replete with target variables. They represent everything from the identity of patients with communicable diseases to conduct contact tracing, churners in telco to customers interested in a loyalty program, bank-loan defaulters, fraud transactions to leads for cross-selling.

The example below is from a BCG engagement with a South-East Asian bank. The purpose of the engagement was to help the bank develop credit scorecards for its portfolio of small-to-medium enterprise (SME) loan applications. Due to sub-optimal data collection processes and local regulations, most of the applicants had thin bureau- and non-audited financials. To establish a solid data foundation, the first six months of our BCG team’s work with the bank were invested in data extraction, data collection and manual digitization of reports. (Even with the use of sophisticated OCR of the available scanned pdfs, only 45–50% of the content of these documents was retrieved — despite painstaking correction of human error and elimination of corrupted pdfs.) As a result, our team had to start modelling with a very small data sample (N<2K) of bureau, financials and application data. The major challenges we were:

3. The absolute number of “Bad” events available to train models

Given the lack of reliable data, our team built customized or “bespoke” models to tap into all available data sources and minimize missing value imputation. We developed the models based on the entire available population, using k-fold cross validation techniques. The final model resulted in approximately 40% Mean GINI (the results of in-sample validation). These results were not as good as we might have hoped, but were sufficient given the situation we faced.

The task of out-of-time (OOT) validation was given to the client’s internal validation team. As soon as GINI dropped by approximately 50% compared to development, the internal team determined that the models would be ineffective. Given that we had anticipated OOT GINI to be in the range of 35–45%, we did not expect this result. So we did some digging.

What Went Wrong: The Math Behind GINI

GINI is essentially a rank-ordering metric that measures how accurately a model or scorecard can predict “default” events. The typical approach is to divide the population into 10 bins (deciles) after sorting by descending probability of default (PD), and then calculate the cumulative percentage of “Good” (0) and “Bad” (1) captured as we move from rating classes 1 to 10.

In the example below, we score 1,500 customers (N) with a Bad rate of 3% (in other words, there were 45 “Bad” customers). Without using any model and applying just the law of probability (PD), we can assume that for every 150 customers there will be 4–5 defaulters. When we train the model, we can calculate that the capture rate is 60% in the top 3 buckets and that the GINI is 42%.

Here are the three steps involved in the calculation:

Step 1: Sort by descending order of PD and make deciles/bins.

Step 2: Calculate the number of Good and Bad in each bin.

Step 3: Calculate AUC and KS (difference between cumulative percentage of Good and Bad) for each bin.

In the top 3 deciles, 27 Bad customers out of total of 45 Bad customers = 60% of the total defaulters captured. When you do the math, the model GINI comes to 42%.

For OOT validation, there are 1,200 customers to be scored using the same model. Once we run the model and sort by descending PD, the results are:

One might immediately notice that the GINI drops by almost 50% and, hence, be inclined to disapprove the model — as our bank client initially did. Interestingly, if this model was doing proper justice to the last 2–3 buckets, we would expect no or very few defaulters. Instead, we see similar default rates in the last 2 deciles compared to population default (3–4 defaulters for every 120 randomly selected customers). This could be due to a number of reasons:

1. The model is not working well with new data.

2. Defaulters have counterintuitive parameters, perhaps a combination of low utilization and high past-delinquency rates.

3. Only partial information is available for some features, causing the model to create probabilities based on imputation/treatment (such as for missing value imputation/ Winsorization).

4. The variables may show new trends, leading to fluctuations in the population stability index (PSI).

This is, of course, a model and not a crystal ball. As such, we can’t perfectly predict each and every default correctly. On further investigation we found that of the 8 defaulters classified into the last 2 buckets (lowest risk class), 6 of them either had incomplete data or there was an error in calculation of some transformed variable. If we re-examine the metrics by removing these 6 cases from the evaluation population, we would have only 2 defaulters in the last 2 bins, with a revised sample size of 1,200–6=1,194 (N) and Bad rate 30/1,194=2.5%. In this case, the results would be:

Following this re-examination, the GINI is back to 40+. One must ask: Is it fair to conclude that the revised sample passed the validation test by discarding only 0.5% of the test dataset?

What We Can Infer

This question brings up my final point, which is that, clearly, GINI should not be the only metric on which you evaluate model performance. In general, data scientists understand that while certain metrics may be optimal in some scenarios, they may be suboptimal in others. For example:

1. When dealing with imbalanced datasets such as credit card fraud or patients with chronic diseases where the majority class (0) is ~99% of the population, the metric of “accuracy” would be a naïve measure: Even a blunt model classifying everyone as “0” would have a 99% accuracy. False negatives (FN), especially when they concern someone’s health, can have serious outcomes hence modeller should aim at increasing “recall.”

2. Consider another example, this one involving a test in which predicted criminals are punished. In this example, our null and alternate hypotheses are H0 : Person is innocent, and HA : Person committed crime. Given that an innocent being punished (type-I error; reject null hypothesis when it’s true) is a far worse outcome than a criminal being set free (type-II error; accept null hypothesis when its false); the modeller must focus on increasing “Precision” — he or she must reduce False positives (FP).

These errors are inversely proportional, so one should fix a threshold for type-I error (<=α, level of significance/size of test) and then chose a test that optimizes type-II errors (β, 1-Power).

As we saw when we revised the default model, we could gain a higher or lower GINI (and in this example, the magnitude of change varies by approximately 200%) simply by ignoring six customers. Ideally at the beginning we should lay out our evaluation criterion and define what we mean by success, such as the model correctly identifying the percentage of defaulters with a threshold of a performance metric that exceeds a certain percentage.

Even given the low GINI, when we look at the entire 1,200 OOT population the model still holds value because:

2. There are comparable capture rates in the top deciles for train and test samples (When you compare Tables 1 and 2, the capture rate is still 60% for the top 3 deciles).

Consider the Specific Scenario, But Keep Moving Forward

This points to the fact that it is not always necessary to discard results based on a single metric. In the final analysis, having a model in place is far better than having no model at all — and far better than going back to a legacy “Application checklist”. GINI is excellent at detecting rank ordering and giving an indication of performance but it, like all measures, has its limitations. When it comes to model validation, data scientists needs to adopt a holistic view. Finding the right metric is a matter of taking into consideration all scenarios in play (sample size, event rate, fill rate of variables, data availability, reliability, macroeconomic situations and so on) and finding the best solution to the problem at hand.

If you have questions or comments, please contact me at https://www.linkedin.com/in/karabirupa-dutta-430a7624/.
=========== Hunting Down Hate Speech ===========
How to use data augmentation methods to improve supervised NLP models

Natural Language Processing (NLP) has become increasingly popular in both academia and industry over the past years. Consumers take advantage of the benefits of NLP on a daily basis in their phones (SwiftKey), personal assistants (Alexa, Siri), music playlists (Spotify Discover), or news-recommendation engines (Medium). It is only natural then that corporations would soon follow suit and start building more robust enterprise-level NLP applications in a range of LegalTech, InsureTech, or FinTech engines. This use of NLP increases efficiency and relieves analysts of the drudgery of executing endless, mindless repetitive tasks. But there is a catch.

Many of these NLP tasks are unsupervised and, therefore, rarely lack training data. Some supervised classification tasks such as hate speech and cyberbullying detection, however, require labelled text samples that can be more difficult to acquire. For these tasks, class imbalance of samples is a frequent problem. When trying to identify hate speech on the Internet, for example, there exist many more “regular” comments than abusive comments. As a result, most datasets for hate-speech detection are likely to have a strong class imbalance towards these “regular” text samples.

In other machine learning domains such as image recognition or video processing, class imbalance is commonly dealt with using data-augmentation techniques. Data augmentation adjusts existing dataset samples by applying small transformations to the input, then using these transformed inputs as additional training data. For example, it’s common practice to augment the data of a visual image by rotating or flipping it, since doing so does not change the meaning of the image. In other words, a flipped image of a cat will still be recognizable to humans as a cat. Flipping a sentence, of course, does not work quite the same way, since changing the word order of a sentence also changes its semantics. This problem was also discussed in a previous GammaScope article.

This post suggests three quick and easy tricks to improve the performance of machine learning models for NLP where class imbalance is a problem. To do so, we first need to build some intuition about how text can be represented as a numerical input before we show how to use this representation to augment text data and design more robust NLP systems. The main techniques we will develop are:

1. Word-vector padding alterations and synonym replacement as an augmentation technique for short-text data

2. Natural Language Generation models (with RNNs and Markov Chains) to generate new samples of the minority class

As with all machine learning problems, we must find a way to numerically represent the data that serves as an input for the downstream tasks (e.g. logistic regression, SVMs, or deep-learning models). For images, we can represent a picture as its height x width x color depth per pixel, thus creating a 3D representation. Doing this for text is more difficult, since we don’t have a fixed “depth” representation assigned to each word or character. A naive approach would be to use a lexicon such as WordNet, which assigns a fixed definition to each word. We could then use a binary encoded vector (“one-hot” encoding) that takes a true or false value for each word in a lexicon and uses the concatenation of this vector as an input of a sentence. The problem with this approach is that the size of that one-hot encoded vector becomes unmanageable for large text corpora (vector size would grow with vocabulary size). Therefore, we want a fixed-size representation of words or sentences such that the input for the downstream task remains manageable — even for large text corpora.

The most popular method for this is Word2Vec, which uses the idea of context dependency. Instead of defining a fixed meaning to each word (such as a lexicon would), we “characterize a word by the company it keeps.” In other words, each expression in a text corpus is assigned a fixed-sized vector that represents its meaning relative to the other words in a text corpus. One can achieve this by sliding a “window” over each word in a text corpus. At each step, we look at a current “center word” and try to predict its context words. This boils down to an optimization task to find the greatest probability that, given the current center word, the context words will appear.

In the final implementation, at each timestep t of a given word sequence, we maximize the probability of two words co-occurring in the first log of the equation, and take j negative samples to maximize the probability that the real outside word appears. We also minimize the probability that random words will appear around the center words (full details of this method can be found here). Based on this setup, a shallow two-layer neural network adjusts the word vectors θ using stochastic gradient descent and backpropagation for the gradient update. When we iterate over large text corpora this way, words with similar meanings (i.e. those that appear in similar contexts) end up having a similar vector. Hereby It is common to learn these vector representations on a large text corpus that may be unrelated to the training task, such as the 3bn word Google News Corpus, CommonCrawl, or the Wikipedia corpus. After learning the word vectors, we can assess the semantic similarity between two words by looking at the cosine distance of their vectors.

Using this technique, we have a way to characterize each word in a vocabulary relative to each other, which allows the downstream learning task to abstract away “clusters” of words with similar meanings, as shown in the above. Note that there are a number of adjustments to this simple Word2Vec model, such as Global Vectors, FastText, or Doc2Vec — all of which are unsupervised learning tasks for fixed-size word vector representations.

Having built the intuition of what understanding of language most NLP models use, we can think of ways to use this understanding to generate new text samples. One way to generate text data is to adjust the existing data to take the form of a semantically similar sentence. The primary objective of this method is to change the numerical input to the downstream learning task while keeping the same semantics of the sentence. Once again using the example of hate-speech classification, we would want a racist comment (under-sampled class) to remain a racist comment — but to do so using different words. To generate these slightly alternative samples, we can check for two things for each word in a vocabulary:

1. A cosine distance threshold between all the word vectors in the vocabulary

In the above, the cosine similarity is simply the dot product of any two given word vectors, scaled by the size of the vectors. Two vectors with a higher dot product are more similar and hence more interchangeable with each other. For example, the words “power” and “strength” from the above are likely to be used in similar contexts and, in turn, have a higher dot product. A part-of-speech tag is a grammatical tag we can assign to each word to make sure two words are grammatically the same. In the example below, we can see that “letting” exceeds the similarity threshold but is a gerund, while “let” is a regular verb. Checking for POS-tag equality simply ensures that we don’t make the sentence too grammatically incorrect when augmenting the samples.

In the simple example above, we can see that we augmented the existing samples of the given sentence. How could this help our sample model classify hate speech? Assume the model is trained on multiple, very similar abusive samples where the key abusive word is frequently exchanged with a similar abusive word from the text corpus. The downstream classifier will put a lower emphasis on the individual hate words, but a stronger emphasis on the context words that may not have been exchanged. In this way, we induce the model to learn speech patterns as opposed to individual words. This is useful for this specific learning task, since the model is less likely to be led astray by common permutations users may employ to circumvent lexical hate speech checks, or by people using abusive language without actually saying something abusive (such as by quoting someone who, in fact, is saying something racist).

The threshold in this method is treated as an additional hyperparameter to be optimized. The intuition behind optimizing for the cosine-similarity threshold is that two words must have been seen in sufficient equal contexts such that one can be replaced by the other without confusing the downstream task. I tried this method on a number of hate-speech datasets and achieved a 4–6% accuracy improvement and, most importantly, improved recall by as much as a 25% over the under-sampled class.

An alternative approach to augmenting the minority class is to generate new samples from scratch. You may have read about the Microsoft chat bot that turned racist by accident. How about purposely creating the most racist and misogynistic chat bot possible — one capable of generating new hate speech samples we wouldn’t need to label anymore?

To do this, we could train a Markovian or RNN language model on the training data for each class, then pick a random start word and let the model predict the next word in the sequence. For simplicity’s sake, let’s start by looking at Markovian language models. Recall that a Markov Chain is based on the concept that the next element in a sequence is determined by the current element and its highest transitional probability to any of the previous elements. If we apply this to text, we could map each unique word in a corpus to take a state, and define the transitional probabilities to be the probability of one word being the adjacent word to another. To illustrate this, let’s take the song lyrics of “Imagine” from the Beatles as an example:

Imagine there’s no heaven,

 It’s easy if you try,

 No hell below us,

 Above us only sky,

 Imagine all the people,

 Living for today….

If we map each word in these lyrics to a state, we’ll see that there are some words such as “imagine,” “no,” or “us” that co-occur with more than one word. If we now start a random walk through the chain of states illustrated below, we could come up with a new sample:

Of course, this example is very simple and works much better when applying Markovian language models to larger text corpora where the number of word co-occurrences is higher.

The problem with using Markov Chains is that they are memory-less models, meaning they only take the current state (word) at each timestep into account and ignore the entire previous sequence. This leads to sentences that, in many cases, are grammatically incorrect. An improvement to this method would be to train a Recurrent Neural Network, which takes the current word as well as the entire previous sequence into account. To do so, we could use a Long-Short Term Memory unit (LSTM), which allows us to put either a higher or lower emphasis on more recent timesteps. We could then adjust the LSTM to generate longer or shorter text sequences.

To implement this, one must train a separate RNN model for each class using a W2V embedding for an embedding layer that’s passed into two subsequent 128-dimensional LSTM recurrent layers. The separate outputs from the embedding layer and each LSTM layer are concatenated, outputting a N x 356 feature matrix (N being the number of words). This feature matrix is passed into a final fully connected layer, the output of which is mapped to probabilities for each word in the corpus. The highest probability represents the next word in the generated sequence. After training this on a range of hate-speech datasets (this one and this one), the text generator produced samples, all of which were all similar to those shown below:

Note that the generated samples (such as the second sample) don’t always make sense to humans. This doesn’t always matter for the downstream task, though, since it’ll still be able to recognize more combinations of language patterns and vocabulary from the training corpus. Trying this technique on a number of hate speech datasets, we found that we could further improve accuracy by 2–3% and hate-speech-class recall by 8–10% on our richest dataset. But these generative methods are by no means perfect and, by sometimes introducing more noise, can do more harm than good. For example, having only 100 samples in the minority class won’t be enough to train a natural language generation model. As a result, these generated samples will simply confuse the downstream task.

Let me offer one last remark regarding the design of NLP systems for larger systems when using multiple embeddings (i.e. mapping from words to numbers, such as Word2Vec explained above). In this situation, performance is maximized when all the embedding techniques in the system are aligned. For example, assume we designed a model that trained the sentence-bootstrapping augmentation on the GoogleNews Corpus, the Natural Language Generation model using an alternative embedding technique (e.g. GloVe), and the downstream embedding using a different text corpus on the same technique (e.g. CommonCrawl). Perhaps unsurprisingly, this model would be likely to perform worse than a model using the same embedding and text corpora throughout. This is because each component of the model would have a slightly different understanding of language.

Intuitively, we could relate this to a game of Chinese whispers: If player A (augmentation) has a different understanding of a word than player B (downstream embedding), the output at the end of the game is more likely to be confused. Therefore, we want all components (or players) in this multi-step model to have the same understanding of the words. Usually, after aligning the word-representation techniques and training corpora of the embeddings, the accuracy of the model will increase by an additional approximately 2% across datasets.

There is a fine line between using augmentation effectively and exposing your data-science project to data leakage. While this application is for text-data augmentation, coming up with ways to augment your datasets can be useful to boost performance for a whole range of data types. When doing so, be sure to apply them with care. Note that, in order to avoid possible data leaks, it’s very important to carefully separate the cross-validation folds and make sure that no test-set data is used to train the augmentation methods.

In this post we showcased two relatively simple but tough-to-beat data augmentation techniques for text. There are a few things to bear in mind when using these techniques. First, the fact that, in some cases, the generated samples won’t make perfect sense to humans may not matter too much for the model. As long as the model can abstract away another language pattern or a word used in a similar context, its ability to generalize on unseen data is likely to be improved. Second, these techniques are by no means perfect and there may be many ways to optimize them with regards to the distance metrics or neural network architectures used. One extension would be to use Generative Adversarial Networks (GANs) for the text generation. Another would be to align the distance metrics used for training of the word embeddings and the ones used for augmentation. This solution currently uses Word2Vec, which assumes a Euclidean space and optimizes the dot product during training, while cosine distance is a spherical metric. We would expect this solution to improve significantly if we used a spherical embedding method in the first place.

This blog post was written to accompany the publication of the following paper:

Rizos, G., Hemker, K. & Schuller, B. (2019). Augment to Prevent: Short-Text Data Augmentation in Deep Learning for Hate Speech Classification. The 28th ACM International Conference on Information and Knowledge Management (CIKM ’19), November 3–7, 2019, Beijing, China.
=========== Personalization — where to start? ===========
Personalization programs are complex, multi-year transformations. For this reason, they require substantial leadership mindshare to break the inertia of the status quo, but also a deliberate sequencing of actions and investments. This paper is a practical, experience-driven guide on how to start, and how to prioritize the most critical decisions.

If I were to force-rank in terms of criticality, the two most pressing questions are invariably around value and process:

· What’s the size of the prize?

· How do we get started?

To address these queries we decided to draw from parallels, by analyzing successful, large-scale personalization programs, and collating both the themes that are specific to a specific organization, and lessons learned that apply more broadly. Concretely, how did they size the opportunity? How did they get started?

We will first describe the diverse approaches taken by organizations such as Starbucks, Allergan, a retailer, and a global airliner when they launched their successful personalization programs, subsequently provide a framework to inform the “how to get started” questions.

By 2015, Starbucks was already offering personalization in both physical and digital channels. Back then, like now, you can fully customize your drink. The barista puts your name on the cup, and calls you when your order is ready at the counter. Starbucks Rewards, the star-filled reward and loyalty program, offers an extra incentive through an earn & burn mechanism, seamlessly executed within a digital ecosystem that includes app-based store-locator as well as ordering and payment functionalities.

This success begged the question: how can they pursue even deeper personalization?

The first target was the application of advanced analytics to the already wildly popular weekly emails that Starbucks sends out to nudge consumers towards incremental purchases. That success derived from challenges and games aimed to reward a behavior with bonus stars, a virtual currency convertible into free drinks or food. For example, a customer could be nudged to “buy a Latte, a breakfast sandwich, and a hot tea and win 50 Bonus Stars.”

The challenge was to take this incentive-based personalization to the meaningful individual level. The upside of this hyper-personalization is that you can truly tailor the message for an individual customer, i.e. using the three best products for her. The downside is that traditional marketing processes and channels are inadequate to manage this level of complexity and versatility. A combination of few hundred SKU’s and multiple bonus levels means that even the “three best products” approach has millions of permutations and requires hundreds of thousands of different email copies rather than a few standardized templates. An additional challenge is the onus to prove that this deeper personalization will augment and improve an already well-oiled marketing steamroller in a cost-effective manner, without disruptions or PR blunders.

To pull this off, Starbucks embraced a phased approach — which we refer to as “scale and repeat” — built on two guiding principles: limit the tech burden, and prove the value before making significant investments. The first foray into deeper personalization in 2015 took barely four months from ideation to execution, i.e. from launch to performance measurement). It consisted of three hyper-personalized email campaigns sent to a smaller but statistically significant subset of the population. Once the pilots proved the “incrementality” of a hyper-personalized version of the existing segmented tactics, the mantra has been “scale and repeat.”

· Scale is conceptually simple. You extend hyper-personalization to every campaign, not just three; you address the full population, not a subset; and you use all digital channels, not only email.

· Repeat means to replicate the pilot à scale journey to other areas. A good candidate for Starbucks was the in-app ordering feature. Starbucks personalized the digital experience by developing a carousel of relevant products, similar to Amazon, to complete your mobile order. It expedited the selection, because your most coupled items are visually surfaced, and encourages menu exploration because the most relevant new products matching your tastes are rank-ordered and presented once you swipe the carousel.

The recommender carousel is a strong example of non-incentive based personalization, following a tested development cycle: start small, prove the value, then invest in scaling and hardening the product.

Starbucks has invested over two years into its personalization effort, with an unrelenting focus on measurable lift. The initiative is credited with generating over 8% of top-line sales within the loyalty program. Living this journey end-to-end allowed us to observe the decisions that drove the success. The first material decision revolved around the development of a complex AI platform that, to date, embraces the latest paradigms of algorithmic-driven marketing. Starbucks needed a custom platform tailored to its unique reward program, but acknowledged it was not in the business of building tech. Instead, it spun off the IP into a multi-tenant NewCo in which they retain equity participation.

The second critical decision was to build an organization dedicated to personalization within its loyalty group. Its pilot à scale à repeat roadmap relied on six-month milestones. Every 6-month pod was responsible for bringing well-codified experiences to market, and the subsequent pod would build on the previous one, focusing on new experiences and behaviors. Teams were co-located into a single space (aptly called “Personalization Lab”). They were cross-functional in nature, fully dedicated, and led by a marketing VP with a strong analytical background. And well-caffeinated, of course.

Allergan: The efficiency of building bigger first

Allergan is primarily recognized as the company that developed and distributes the wrinkle-defying injection Botox, one of the most recognized brands in medical aesthetics. Less well known is the fact that consumers (i.e. the end users that visit a provider for an injection) can earn and burn tangible discounts through Allergan’s reward program. Brilliant Distinctions (BD), the reward program, is focused on the end consumer but mostly administered via providers. The next step — similar to Starbucks — was hyper-personalization of incentives towards such consumers. Both organizations fully embraced personalization and built the tech around it, but Allergan pursued different choices, and built a program with greater scale and complexity during early stages than Starbucks did.

In January 2018, Allergan’s long-term ambition was clear: focus on data, build the infrastructure and the technology to harness the data, and then leverage the new capabilities to disrupt and re-invent marketing, starting with the medical aesthetics business. Over the course of the year, Allergan launched a number of digital ventures aimed at boosting acquisition (Spotlyte), discovery of and access to services (Regi) and incentive-based personalization (Allergan Data Labs, ADL in short).

In contrast to Starbucks’ journey (pilot - scale - repeat), Allergan’s blueprint called for a tech and analytics tour de force from day one. This required a longer upfront build. While Starbucks would launch skinny pilots and then scale the tech, Allergan’s tech was developed at scale from inception. While Starbucks went from ideation to execution in four months. Allergan did not even make go-to-market a priority until month 6, placing significant emphasis on “production-ready code.” It dedicated the first three months to building data pipes and to setting up the cloud-based production environment and the core analytics engine. The engine has three common elements: a co-located data layer, an insight layer, a predictive layer around consumers’ behaviors, engagement and preferences. Not your run of the mill pilot.

Allergan launched two waves of hyper-personalized bundle offers and re-activation campaigns, measured against a control group. Early results proved that personalized incentives significantly outperform mass or segmented marketing, with tangible benefits in terms of efficiency (dollars generated per each dollar invested in marketing) and bottom line (return, net of discounts). Early campaigns didn’t require further data engineering, but rather led to re-thinking the MarTech ecosystem, because the analytics engine could power faster variations of marketing campaigns than the execution channels could support.

Allergan’s philosophy also differed from Starbucks in terms of investment, organization, and ownership of IP. Allergan’s investment in tech and analytics was more front-loaded, which is arguably a more efficient allocation when leadership alignment is strong (i.e. more comfortable with longer lead-times to in-market execution). At peak, Starbucks had over 70 FTEs dedicated to personalization while Allergan had around 25 FTEs. The organization in the first months was mostly designed around data engineering and analytics, sitting next to the original marketing organizations. The more substantial organizational disruption coincided with the first wave of marketing execution (around month 6), and led to the formation of Allergan Data Labs as a stand-alone unit. Allergan went outside to bring in a talented VP with acquisition and personalization experience in the MedTech space to lead the charge, build a stronger digital team, and fully leverage the analytics workbench. ADL is now the center of excellence that powers the analytics behind new waves of tailored, personalized experiences.

Finally, all the intellectual property (IP) is internal. While ADL feels like an agile-friendly startup, ownership and development of algorithms for personalization remains in-house, according to a two-phased roadmap. The first step is for ADL to centralize the insights from other digital ventures (Spotlyte and Regi); the second step is to expand analytics-driven personalization beyond medical aesthetics.

In other large-scale programs, personalization was not the catalyst for large digital transformation, but rather the second or third wave of a broader innovation agenda. In our experience, such initiatives usually reflect scale, maturity, or simply readiness. The most recurring argument is that the organization needs to build the foundations first before it undertakes a more complex endeavor such as personalization. The risk, however, is that the “build phase” might never end completely, leading to costly delays and stretched timelines.

A global retailer (Retailer X) has re-written the playbook on how to embrace an ambitious multi-year transformation, with personalization being a pillar, but not the primary driver. In its journey, it has identified four areas of disruption around analytics, and built both the organization and the technology around these areas . Instead of pursuing everything at once, the organization has staggered the execution along an engineering roadmap: every use case needs to build and fund the technology and processes for the next one. By the time it launched personalization, Retailer X had already launched cash-positive initiatives, and developed a number of relevant assets: a centralized cloud-based environment, a common data schema, strong data automation, and a support engineering team to enable new analytical applications.

The first personalization use case mirrored Starbucks’ nimble approach. It aimed to launch a small pilot of personalized email incentives, improve known tactics, and then scale only when uplift is statistically significant. Since the IT team owns the infrastructure at scale, scaling a pilot has more to do with processes and marketing execution than with core engineering work. This has allowed Retailer X to launch several pilots in parallel with a relatively small personalization team of around 10 FTEs. In our experience, this lean set-up is more the exception than the rule, and mostly hides the fact that the engineering and infrastructure work took place under a centralized organization that serves a broader analytics agenda (e.g. pricing, supply chain, etc.). It is also a rare instance of a complex personalization program that is not the sole occupation of a dedicated VP in role.

From an IP development perspective, this retailer has embraced the path of owning only the critical assets, but buying the rest. This is common practice when there is a strong online component, and the incentive is to integrate off-the-shelf solutions. For example: install third-party customer data platforms (CDP) and basic product recommenders, then integrate them with existing workflow engines with proprietary business rules. The core in-house development is around experimentation frameworks (test and learn, reinforcement learning) and more advanced recommender engines and decision models. For instance, Retailer X, has developed a remarkable suite of predictive algorithms to serve the most complex forms of hyper-personalization: surfacing a product that completes a prior purchase, the best new product that could be of interest, a product more amenable to be purchased full-price, and so on. The company continuously updates and enriches the suite as new open-source algorithms are brought to market, either by testing a stronger algorithm (e.g. a more accurate ensemble model) or by embracing a new technique altogether (deep learning instead of machine learning approaches.)

Starbucks, Allergan and Retailer X have each followed a tried and trusted approach to embracing analytics for personalization: first optimize within the tactic (e.g. right price, right product), then optimize the choice of tactics based on context (e.g. send a discount or a reminder offer today?), and then finally optimize a whole sequence of tactics. I have written extensively on the topic here.

The global airliner (we’ll call it Airline X), has taken a different path, with a successful 12-month implementation of an advanced AI system that optimizes the correct sequence of marketing touchpoints. The intelligence is focused not just on the individual tactic (what is the best route and discount combination for each customer?), but rather on optimizing the rank-order of subsequent tactics: is sequence A-B-C-D-E better than sequence D-C-A-B-E?

The decision to tackle the most daunting analytical problem head-on is sensible under a number of conditions. The first condition is to have enough content to cycle through. Airline X meets the condition because it has a vast range of offerings — including own-brand and third-party credit cards, various insurance products, miscellaneous merchandising and leisure-related products — that goes far beyond air transportation. The second condition is that the fruition channels (Email, Web, App) are mature enough to accommodate the level of personalization — individually tailored timing, channel, and content — produced by the AI platform. The third condition is that there is enough historical data around customer-level response to the different marketing messages — alternatively, that the organization carves out enough time for experimentation to overcome the cold start problem. In other words, the AI platform needs prior data to understand what “good” looks like (possible but usually unlikely), or a process to create that experimental data (often cleaner but more time-consuming).

Meeting all three conditions is a testament to an analytically mature organization. Creating no-regret marketing content and augmenting the MarTech eco-system can be accomplished in months, but creating longitudinal experimental data to optimize entire sequences or journeys has no finite timeline — models converge successfully in months (and they did) but the learning process never ends, and most organizations are not comfortable with that burden. This consideration is not an argument for complexity at any cost, nor a statement that personalization is a distant accomplishment years in the making — it’s not, and it wasn’t for the Airline X. The key difference is the extent to which the sequencing matters.

Starbuck’s gamification does not necessarily require sequence optimization. Customers expect to play a game every week, and thus a simple rotation suffices as long as the weekly game feels relevant and personalized. Differently than Airline X, Starbucks therefore did not require years of experimental data to develop and reap upsize financial benefits from a cutting-edge AI system. In Allergan’s case, the sequence matters, but to a lesser extent, because most aesthetic treatments are regulated in terms of frequency (90 days need to lapse after a filler). In the airline business, the context matters (think pre- and post-flight incentives), but relevance of communication during a purchase hiatus matters even more. Outside out frequent business travelers, capturing the attention in spite of infrequent flight purchases is like honing a dialogue, which is a sequence of words.

Airline Xs execution strategy has been less idiosyncratic than its analytical roadmap. It launched a pilot in eight months to prove value, and only then invested in scaling and hardening the solution into a “continuous improvement” steady state. The IP rests in the proprietary engine that optimizes the sequence, whereas all the other components of the MarTech stack (tagging, workflow manager, etc.) are integrated from mainstream solutions. Again, we find the notion of cross-functional, co-located teams, with around 90 FTEs coming together into a mini organization within the organization, split into different squads: “marketing technology”, “marketing and content”, “engine build”, “engine operations”, “measurement” etc.; this has been indeed one of their largest programs, blessed by broader alignment and support: jointly sponsored by Exco members representing airline and loyalty, with senior stewardship from analytics.

Distilling a common recipe — what we can learn from others

Personalization programs are ambitious and rewarding at the same time. As with many digital transformations, there are commonalities but also idiosyncrasies driven by industry, geography, technical maturity, and competitive environment. As I stated at the outset, our conversation with executives on the topic of personalization eventually lead to ROI (what’s the payoff) and execution (how do we begin?),

Yes, personalization programs are worth it

Data on this first question is somewhat sparse, but fortunately, it is favorably consistent across different dimensions. From an engagement standpoint, personalized touchpoints are more relevant and timely, manifested in increases of 20 to 30% in open-rates or click-rates.

From a financial standpoint, we consistently record top-line improvement ranging from 3% to 8%, driven by a significant uptick in marketing efficiency. This is a combination of 30% to 100% improvement in net incremental revenue (return from marketing, net of discounts) and improvements of 20% to 100% in marketing efficiency (dollars generated per each dollar of marketing spent). Ambitious programs can span 24 months, representing an investment of $25-$50 million depending on the initial level of sophistication and the level of parallelization. We have led smaller and still successful programs running for 12 months, with an investment of $12-$15 million. A typical break-even window is between 12 and 18 months, which means the roadmap is self-funding, and generates a generous and ongoing upside after year two.

Short-term financial impact is a very compelling lens, but not the only one for viewing the benefits of a personalization program. Personalization builds long-term competitive advantage in the form of increased consumer engagement, stronger stickiness (i.e. reduced churn) and a more rewarding culture of experimentation.

In short, personalization is worth it.

Personalization programs are marathons, not sprints. The level of investment required to unlock the potential puts pressure on the organization and can potentially result in delays, half-hearted attempts, or inertia altogether. In our experience, some execution elements are common to practically all programs and organizations, even adjusted for industry, technical maturity, and competitive environment. At the same time, when it comes to designing the very first steps of a personalization journey, we find that some decisions remain company-specific.

We propose a framework that subsumes the common themes and key decision points. The list below is a high-level picture of consistent themes or patterns, derived from direct observation of successful and mature programs. In our experience, these themes should also inform new programs, and become the basis for the desired blueprint, regardless of the stage of maturity.

· A dedicated organization: The personalization programs develop a mini-organization within the larger organization, with a defined perimeter and reporting structure (usually within the broader organization, but sometimes as a separate entity altogether). This structure exists mostly to guarantee a level of independence in terms of decision-making, funding, issue escalation and performance measurement. But it also reflects the fact that personalization requires niche and scarce talent. Also, separate reporting allows for flexibility on titles, compensation, and career progression.

· Strong leadership at the top: This means a VP of Personalization fully dedicated to the program and with clear performance targets in terms of financials and/or engagement metrics. Successful programs enjoy very senior sponsorship, often emanating directly from the C-suite. A typical reporting structure leads to the Head of Strategy or the CMO. Tech and Analytics are enablers, but not the right home for personalization leadership.

· 100%-dedicated resources, cross-disciplinary in nature and co-located whenever possible: The complexity of personalization demands that resources with very diverse expertise maintain “one job.” In other words, they do not split time across other commitments outside of personalization. Full dedication does not translate into commitment ad libitum. Niche resources could be 100% dedicated just for a couple of months and then return to their original organization when the need subsides. Typical examples are API developers, UI experts, graphic designers, who are all contributors with a very clear activity and timeline. In our experience, co-location tangibly enhances collaboration across the diverse talent pool dedicated to personalization.

· Agile rituals and ways of working: Personalization is about ongoing improvement, experimentation, and focus on measurable results. The nod to the agile rituals is just one part of the equation. The organization and its leadership also need to embrace an agile approach for the go-to-market by developing a level of comfort with uncertainty (e.g. directional plan for what will be built in two months, but not a full spec sheet) and experimentation (e.g. the notion that some approaches will fail, while others will succeed.). Furthermore, it means comfort with “giving analytics time to learn”, and not just in the machine learning sense of things, but also allowing cross-functional teams time to hone the solution and learn from early-stage approximations.

· A cadence of development in “waves”, with each wave funding the next: The imperative is to manage complexity, and to compartmentalize the effort in time-boxed “waves”. Each wave drives end-to-end execution of select use case(s), and defines start and end-dates for resources dedicated to personalization. The breakdown into waves allows the company to prioritize work with two criteria in mind. First, each wave should continue to build and/or expend core capabilities (tech, data, processes), and second, each wave should deliver measurable financial lift to fund the subsequent wave. It is not uncommon for most efforts to start with a “Wave 0” dedicated to set the rules of engagement and plan the subsequent journey.
=========== Transforming Pricing for Oil & Gas Wholesales: Personalized Pricing for Petrochemicals ===========
In the first part of this two-part series, we introduced a four-step process for transforming pricing for oil and gas wholesales, with a focus on personalized pricing for petroleum products. In this second part, we apply a similar four-step process to the petrochemicals business unit, which faces different challenges because prices are negotiated rather than determined on a dynamic, spot basis.

To this day in the wholesale markets for energy products, established pricing strategies and practices are neither data-driven, nor dynamic, nor “modern.” Pricing still depends heavily on salespeople’s experience, old and rigid segmentations, and legacy systems.

In the second part of our two-part series, we describe in detail how we helped an integrated refinery transform its pricing strategy for petrochemicals (various grades of polymers and polyols.) Our first-of-its-kind implementation underscored the compelling difference between traditional pricing and algorithmic pricing: a margin uplift opportunity of up to $30/ton for petrochemicals products.

Why has the industry been unable until now to identify and tap such significant opportunities? The reasons lie in the technological and cultural clash between traditional and data-driven pricing practices as well as in finding the right balance between complexity of the solution and the uplift it promises. In the case of petrochemicals, the large number of product variants alone makes even the most basic manual data analyses impractical, if not impossible. This only reinforces the team’s reliance on experience and established patterns and systems rather than on advanced analyses.

In terms of balance, the simpler solutions from data driven approaches are easily communicated and implemented, but the returns may not be as significant. Complex “black box” solutions are harder to interpret and challenging to implement, but the returns could go to the levels unimagined before.

In the petrochemical business, prices and volumes are negotiated between sales teams and wholesalers. It may take between one week and one month to strike a deal. Beyond that, the petrochemicals business unit also faces some unique challenges.

Unlike the petroleum business, the petrochemical business’s environment presents some specific challenges we needed to address:

· Lower sales velocity: In line with the length of a weekly sales window, prices of petrochemical products remain static for a longer period.

· Multiple sales channels across numerous countries: Pricing positions are established separately for domestic and global sales, based on pricing trends, inventory, and production costs.

· Large number of products: Hundreds of variants for each product offered in the catalog make it impossible to derive a price elasticity for each variant. To make the analyses practical and feasible, several variants with similar chemical compounding structure and quality gradient were aggregated and bucketed to create a “master product.”

· Limited scope for deploying price elasticity in domestic wholesale: Pricing for domestic wholesales is denominated by a single price for a given quantity in the open market. This leaves little scope for personalization in pricing.

· Opportunities due to volatility in global market: Pricing for direct retail wholesalers in domestic and global markets, however, is heavily personalized based on wholesaler activity, macroeconomic trends, and foreign exchange volatility.

Figure 1 below gives a brief overview of challenges for pricing in petrochemical business:

The sales team engages with the wholesalers over the duration of a week, negotiating for contracts lasting a few weeks. Predicting a single price point for a wholesaler is an uphill task, because of sporadic wholesaler activity, wholesaler loyalty, and the need to clear inventory, all against the subjectivity inherent in negotiations and the overarching strategies to drive profitability.

To facilitate these negotiations and put them on a much firmer basis, we provided the sales teams a data-driven negotiation range for each customer. Sales teams can use these ranges in their negotiations to keep the prices as close to the ceiling as possible and under only special circumstances negotiate below the floor price.

Four steps to personalized pricing in petrochemicals wholesale

The solution design offered in the petrochemical business unit is roughly similar to the first three steps in determining pricing for the petroleum business unit:

However, a significant difference lies in the fourth step. Instead of optimization, the fourth step for the petrochemicals business unit is negotiations. These four steps covered under the pricing algorithm for a petrochemical business unit are discussed in the sections below. For general information on technical approach we took, please refer back to Part 1 of this series.

The objective of data diagnostics is to understand and validate data sources that aid in pricing products better. This step begins with a laundry list of all the possible hypotheses that are potentially relevant both from a wholesaler’s buying perspective as well as from a price setter’s (sales team’s) perspective. We prioritize all the hypotheses on the basis of business fundamentals and availability of data sources.

Key hypotheses for the petrochemicals business unit included an array of factors related to wholesaler purchase patterns and prevailing market conditions. Exhaustive statistical testing and diagnostics allowed us to identify the most influential hypotheses and retain them for the model building exercise. Some of these influential hypotheses include:

· Wholesaler volume: The higher the customer volumes are, the higher the customer elasticities are

· Latency: Customers with higher latency periods may get preferential pricing

· Temporal variables: Production activity varies over the course of the year, depending on upcoming holiday seasons and weather

· Volatility in foreign-exchange (FX) and commodities markets: Increased volatility in the FX and commodities market may change customer elasticities

· Manufacturing indices: Production activity and inflation may help determine the liquidity in the market to purchase products

· Purchase orders yet to be delivered: Customer with multiple open purchase orders may indulge in price hedging

Figure 2 summarizes the variables that were used for hypotheses testing based on market conditions, while Figure 3 shows the variables based on customer purchase patterns.

Useful quick analyses include checking the availability of relevant data sources, checking the quality and timeframe of available data, and performing simple correlation checks, i.e. R-squared checks with respect to the variable(s) of interest etc.

For petrochemicals, wholesalers’ purchase patterns will differ based on the products they buy, meaning they may exhibit significantly different behaviors when buying different products (or variants) from the same supplier. For that reason, it is safe to assume that clustering at the product level will be far more stable than clustering at other levels (e.g. total purchase volume.) Because buying activity is more sporadic and discrete in the petrochemical business unit, assessing wholesaler activity requires a longer time period, in some cases 2–3 years.

In order to derive the best split of wholesalers into multiple clusters, we explore several direct, index, and proxy variables to identify key wholesaler behaviors. These included:

· Wholesaler volume: Variables based on the volume shipped to the wholesaler in the period

· Orders: Proxy to measure wholesaler loyalty, when taken together with recency and latency

· Profitability: Unit profit earned from the wholesaler, relative to market indices in the period

Figure 4 shows the results for one petrochemical product. The x-axis showed the premium (in USD/tonne) relative to a market price, while the y-axis shows the number of purchase orders placed in the respective time period. The size of the bubble reflects the average volume per purchase in kilograms.

Clustering groups the most homogeneous wholesaler behaviors together to permit further action and analysis. In Step 3, we will identify different price-demand curves suitable for the clusters, then “reverse engineer” the demand curves in order to establish a price range for the sales teams to use as the basis for their negotiations.

In economics, a demand curve depicts the relationship between the price of a certain commodity (the y-axis) and the quantity of that commodity demanded at that price (the x-axis).

In this third step, we construct a demand curve for each wholesaler in order to estimate the right price for the quantity that the wholesaler is willing to buy. It builds naturally on the first two steps, because willingness to pay for each wholesaler varies by the wholesaler’s historical purchase behavior, present demand, market conditions, and other relevant variables.

As stated earlier, a wholesaler’s purchase pattern wouldn’t necessarily be the same for all the products, nor would a product’s purchase patterns be the same for all the sales channels. Thus, it was necessary that we build these price demand curves at a sales channel-product level.

Despite the elegance and simplicity of the demand curves the derivation of useful demand curves for a petrochemical business unit requires a more nuanced approach.

First, we need to take the low latency of the petrochemical products into account. That makes it unfeasible to build these curves at cluster/wholesaler level, because that would reduce the number of data points significantly.

Second, the expected precision from the algorithm is quite high, because price setting in a petrochemical business unit must be on the order of a few cents to the dollar. This means that every volume must be accompanied by a price.

The ideal solution is a series of price–volume pairs for each wholesaler. A prudent alternative to facilitate that is to inverse the relationship between price and volume, i.e. to build demand as a function of price, as we show in Figure 6.

Using the clusters in the last step, we built in additional variables that represent wholesaler activity at a cluster level and included a label encoder for the wholesaler’s cluster. With this, we had prepared over 160 variables to simulate price-demand elasticity for each product.

Due to the dynamic nature of the petrochemical market, it was necessary for us to implement a self-learning model to keep up with the changes. We developed a seven-step modeling process that yielded a simple multivariate linear regression with volume as dependent variable. We describe this four-step modeling process briefly as follows:

· Elastic-net regression: To shrink the coefficients of redundant variables to zero and eliminate them.

· Stepwise linear regression: To eliminate the variables that didn’t explain significance variance in data

· Random forest: To select variables based on the derived variable importance score.

· Simple multivariate linear regression: To build the most important predictors with quantity (volume) as the dependent variable and price differential from the market price as the primary independent variable.

During modeling, we also consider other factors and conditions. We chose regression for its interpretability and ease of reverse engineering to determine a range of prices for a particular volume. Out of the 30+ models that we built across all the clusters, log transformation on quantity (log-linear models) revealed better predictions and fit. Here are a couple of good links on modeling a demand curve using multivariate regression — link 1 , link 2, link 3

Using the actual demand curve from the linear regression above, we built two additional demand curves using one standard residual error of the model. By adding one standard residual error to the model, we increased the predicted volume, thus reducing the price for a particular volume. Similarly, by subtracting one standard residual error from the predicted volume, we increase the price for a particular volume.

The demand curves yield volume as a function of price. We then “reverse engineer” these curves to derive price as a function of volume, because this matches the flow of negotiations. The wholesaler will generally enter the negotiation with a specific volume in mind and with price negotiable.

Figure 7 illustrates how the regression equation can be reverse engineered to derive a series of price-volume pairs for each wholesaler. Upon reverse engineering the linear regression equation, the actual price demand curve gave us the predicted price for a particular volume. We created a range of prices bounded by the floor (minimum) and the corresponding ceiling (maximum) prices by incorporating the additional price demand curves. The floor (minimum) price was derived by adding one standard residual error to actual price demand curve (blue colored curve in the figure below), while the ceiling (maximum) price was derived by subtracting one standard residual error from actual price demand curve (red colored curve in the infographic below.) This range of prices for a particular volume — within the maximum price, predicted price and minimum prices — lends the power of negotiations to the sales team.

Armed with the price-demand elasticity for the wholesalers and the resulting price ranges, the sales teams from all the sales channels then engaged with the wholesalers either by cold calling them or following up personally with the major wholesalers. Once the petrochemicals sales team receives the volume request from the customer, the sales team uses the reverse-engineered price demand curves to establish the band of prices within which the wholesaler will most likely be comfortable purchasing. This comfort level is not based only the experience of the sales team or a small number of signals from the market or the wholesaler, but rather on a thorough, advanced, and current analysis of the actual behavior of that wholesaler and similar wholesalers for similar products or variants.

The sales team negotiates the “right” price based on numerous strategies that include promotions, profitability and wholesaler loyalty. But the algorithmic approach shifts the basis of negotiation to a much firmer, data-driven foundation, which should in turn boost the confidence of the sales teams as they put their natural selling skills into practice. The recommended range of prices is also subject to a few additional constraints which force the sales team to think critically “outside the numbers.”

First, the team must be aware that an extremely low volume request from the wholesaler might result in an extremely high price which the wholesaler would be unwilling to pay and vice versa. As a result, the team needs specific guidance and adjustment procedures for the minimum and maximum prices to account for extremely large and small volumes. The negotiations must also consider inventory constraints as well as for some customers who need to receive the same prices. Figure 8 shows this flow.

Second, pricing in the domestic wholesale market is characterized by a single price for a range of expected volume. As a result, volume “slabs” were introduced to dictate one single price for the range of volumes. This is meant to incentivize the wholesalers to purchase higher-than-expected volume to drive sales. Figure 9 shows these slabs.

By implementing algorithmic pricing for petroleum and petrochemical products at scale, we have seen a commodities industry make an important and lasting transformational journey. The business units went from archaic processes driven by heuristics and manual labor to an analytical, data-intensive, automated processes. But this transformational journey would be incomplete without the quantification of the improvements made.

Ways to estimate improvement … and their challenges:

In analytically mature industries — especially those with very large customer bases and frequent variations in price and demand — one can readily measure the impact of pricing analytics through A/B testing, i.e. the use of test and control groups. More details on A/B testing can be found at these links: Link1, Link2

A/B testing did not apply in our case because of the limited wholesaler accounts. In a general retail setting, the population of wholesalers is large enough to test pricing sensitivity in isolation for a few samples. In our case, both petroleum and petrochemical business units have a relatively small number of wholesalers, most of whom show significant recurring revenue and volume. Owing to fears of attrition from price-anchoring wholesalers, it was imperative that we seek alternate methods to measure the improvement.

The alternative way is to benchmark the price performance in a given timeframe against a reference timeframe. We chose this method due to the relative ease in implementing this strategy. For both the petroleum and petrochemical business units, we measure price performance relative to competitor pricing and market pricing of the respective commodity. We tracked the improvement in realized premium over a year and compared it against the premium realized the previous year. Based on our preliminary analysis, we estimate the net annualized improvement at around US$20M.

What’s next for personalized pricing in B2B

The seamless, organization-wide adoption of machine-learning-based pricing in energy wholesales business is a harbinger, both within oil and gas (as companies rush to pursue a digital-first strategy) and in other industries as well. Future applications for data-driven decision making will enable companies to consolidate upstream and downstream operations, streamline processes, and enhance customer journeys. In coming years, we expect a boom in the adoption of similar machine-learning-driven solutions in business units across traditional sectors such as industrial goods and consumer packaged goods.