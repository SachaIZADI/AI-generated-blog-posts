{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "# add the EOS token as PAD token to avoid warnings\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "When AI was first introduced into business processes, it was thought that AI could be used to solve complex problems. However, in the last few years, AI has become more and more popular.\n",
      "\n",
      "AI has been used in a number of industries, such as healthcare, education, finance, health care, and many other fields. It has also become a major player in many industries. In fact, there are many companies that are using AI to help them solve problems in their business. For example,\n"
     ]
    }
   ],
   "source": [
    "# set no_repeat_ngram_size to 2\n",
    "input_ids = tokenizer.encode('When AI was first introduced into business processes,', return_tensors='pt')\n",
    "\n",
    "beam_output = model.generate(\n",
    "    input_ids, \n",
    "    max_length=100, \n",
    "    num_beams=5, \n",
    "    no_repeat_ngram_size=2, \n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n",
      "\n",
      "I've been thinking about this for a while now, and I think it's time for me to take a step back and think about what I want to do next. I've always wanted to be a writer, so I thought I'd share my thoughts on how I would go about writing a book about my love of writing. Here are some of the things I\n"
     ]
    }
   ],
   "source": [
    "# set no_repeat_ngram_size to 2\n",
    "input_ids = tokenizer.encode('I enjoy walking with my cute dog', return_tensors='pt')\n",
    "\n",
    "beam_output = model.generate(\n",
    "    input_ids, \n",
    "    max_length=100, \n",
    "    num_beams=5, \n",
    "    no_repeat_ngram_size=2, \n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/02/2020 11:55:55 - INFO - transformers.training_args -   PyTorch: setting up devices\n",
      "07/02/2020 11:55:55 - WARNING - __main__ -   Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False\n",
      "07/02/2020 11:55:55 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='output', overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Jul02_11-55-55_PAR-ZM1RJLVDQ', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=1000, past_index=-1)\n",
      "07/02/2020 11:55:55 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json from cache at /Users/izadisacha/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.db13c9bc9c7bdd738ec89e069621d88e05dc670366092d809a9cbcac6798e24e\n",
      "07/02/2020 11:55:55 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "07/02/2020 11:55:56 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json from cache at /Users/izadisacha/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.db13c9bc9c7bdd738ec89e069621d88e05dc670366092d809a9cbcac6798e24e\n",
      "07/02/2020 11:55:56 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "07/02/2020 11:55:56 - INFO - transformers.tokenization_utils_base -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json from cache at /Users/izadisacha/.cache/torch/transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
      "07/02/2020 11:55:56 - INFO - transformers.tokenization_utils_base -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt from cache at /Users/izadisacha/.cache/torch/transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "/Users/izadisacha/Documents/nlp_finetuning_copy/venv/lib/python3.7/site-packages/transformers/modeling_auto.py:798: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  FutureWarning,\n",
      "07/02/2020 11:55:56 - INFO - transformers.modeling_utils -   loading weights file https://cdn.huggingface.co/gpt2-pytorch_model.bin from cache at /Users/izadisacha/.cache/torch/transformers/d71fd633e58263bd5e91dd3bde9f658bafd81e11ece622be6a3c2e4d42d8fd89.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1\n",
      "07/02/2020 11:55:59 - INFO - transformers.modeling_utils -   All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "07/02/2020 11:55:59 - WARNING - transformers.modeling_utils -   Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "07/02/2020 11:55:59 - INFO - filelock -   Lock 5955924176 acquired on nlp_finetuning/data/cached_lm_GPT2Tokenizer_1024_posts.txt.lock\n",
      "07/02/2020 11:55:59 - INFO - transformers.data.datasets.language_modeling -   Creating features from dataset file at nlp_finetuning/data\n",
      "07/02/2020 11:56:00 - INFO - transformers.data.datasets.language_modeling -   Saving features into cached file nlp_finetuning/data/cached_lm_GPT2Tokenizer_1024_posts.txt [took 0.002 s]\n",
      "07/02/2020 11:56:00 - INFO - filelock -   Lock 5955924176 released on nlp_finetuning/data/cached_lm_GPT2Tokenizer_1024_posts.txt.lock\n",
      "07/02/2020 11:56:00 - INFO - filelock -   Lock 6733447696 acquired on nlp_finetuning/data/cached_lm_GPT2Tokenizer_1024_posts.txt.lock\n",
      "07/02/2020 11:56:00 - INFO - transformers.data.datasets.language_modeling -   Loading features from cached file nlp_finetuning/data/cached_lm_GPT2Tokenizer_1024_posts.txt [took 0.002 s]\n",
      "07/02/2020 11:56:00 - INFO - filelock -   Lock 6733447696 released on nlp_finetuning/data/cached_lm_GPT2Tokenizer_1024_posts.txt.lock\n",
      "07/02/2020 11:56:00 - INFO - transformers.trainer -   You are instantiating a Trainer but W&B is not installed. To use wandb logging, run `pip install wandb; wandb login` see https://docs.wandb.com/huggingface.\n",
      "07/02/2020 11:56:00 - INFO - transformers.trainer -   ***** Running training *****\n",
      "07/02/2020 11:56:00 - INFO - transformers.trainer -     Num examples = 78\n",
      "07/02/2020 11:56:00 - INFO - transformers.trainer -     Num Epochs = 3\n",
      "07/02/2020 11:56:00 - INFO - transformers.trainer -     Instantaneous batch size per device = 8\n",
      "07/02/2020 11:56:00 - INFO - transformers.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "07/02/2020 11:56:00 - INFO - transformers.trainer -     Gradient Accumulation steps = 1\n",
      "07/02/2020 11:56:00 - INFO - transformers.trainer -     Total optimization steps = 30\n",
      "Epoch:   0%|                                              | 0/3 [00:00<?, ?it/s]\n",
      "Iteration:   0%|                                         | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:  10%|███▏                            | 1/10 [01:49<16:23, 109.24s/it]\u001b[A\n",
      "Iteration:  20%|██████▍                         | 2/10 [03:37<14:31, 108.91s/it]\u001b[A\n",
      "Iteration:  30%|█████████▌                      | 3/10 [05:30<12:51, 110.27s/it]\u001b[A\n",
      "Iteration:  40%|████████████▊                   | 4/10 [07:28<11:14, 112.41s/it]\u001b[A\n",
      "Iteration:  50%|████████████████                | 5/10 [09:26<09:30, 114.15s/it]\u001b[A\n",
      "Iteration:  60%|███████████████████▏            | 6/10 [11:14<07:29, 112.45s/it]\u001b[A\n",
      "Iteration:  70%|██████████████████████▍         | 7/10 [13:07<05:37, 112.37s/it]\u001b[A\n",
      "Iteration:  80%|█████████████████████████▌      | 8/10 [14:50<03:39, 109.60s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  90%|████████████████████████████▊   | 9/10 [16:31<01:46, 106.98s/it]\u001b[A\n",
      "Iteration: 100%|███████████████████████████████| 10/10 [17:36<00:00, 105.60s/it]\u001b[A\n",
      "Epoch:  33%|████████████                        | 1/3 [17:36<35:12, 1056.00s/it]\n",
      "Iteration:   0%|                                         | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:  10%|███▏                            | 1/10 [01:44<15:39, 104.40s/it]\u001b[A\n",
      "Iteration:  20%|██████▍                         | 2/10 [03:21<13:37, 102.15s/it]\u001b[A\n",
      "Iteration:  30%|█████████▌                      | 3/10 [04:58<11:43, 100.57s/it]\u001b[A\n",
      "Iteration:  40%|█████████████▏                   | 4/10 [06:36<09:58, 99.79s/it]\u001b[A\n",
      "Iteration:  50%|████████████████▌                | 5/10 [08:10<08:11, 98.26s/it]\u001b[A\n",
      "Iteration:  60%|███████████████████▊             | 6/10 [09:47<06:30, 97.64s/it]\u001b[A\n",
      "Iteration:  70%|███████████████████████          | 7/10 [11:22<04:50, 96.85s/it]\u001b[A\n",
      "Iteration:  80%|██████████████████████████▍      | 8/10 [12:58<03:13, 96.87s/it]\u001b[A\n",
      "Iteration:  90%|█████████████████████████████▋   | 9/10 [14:33<01:36, 96.03s/it]\u001b[A\n",
      "Iteration: 100%|████████████████████████████████| 10/10 [15:31<00:00, 93.10s/it]\u001b[A\n",
      "Epoch:  67%|████████████████████████            | 2/3 [33:07<16:58, 1018.51s/it]\n",
      "Iteration:   0%|                                         | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:  10%|███▎                             | 1/10 [01:38<14:46, 98.51s/it]\u001b[A\n",
      "Iteration:  20%|██████▌                          | 2/10 [03:17<13:09, 98.70s/it]\u001b[A\n",
      "Iteration:  30%|█████████▉                       | 3/10 [04:53<11:25, 97.92s/it]\u001b[A\n",
      "Iteration:  40%|█████████████▏                   | 4/10 [06:32<09:49, 98.25s/it]\u001b[A\n",
      "Iteration:  50%|████████████████▌                | 5/10 [08:08<08:07, 97.42s/it]\u001b[A\n",
      "Iteration:  60%|███████████████████▊             | 6/10 [09:44<06:28, 97.00s/it]\u001b[A\n",
      "Iteration:  70%|███████████████████████          | 7/10 [11:22<04:51, 97.31s/it]\u001b[A\n",
      "Iteration:  80%|██████████████████████████▍      | 8/10 [13:00<03:14, 97.48s/it]\u001b[A\n",
      "Iteration:  90%|█████████████████████████████▋   | 9/10 [14:35<01:36, 96.75s/it]\u001b[A\n",
      "Iteration: 100%|████████████████████████████████| 10/10 [15:34<00:00, 93.45s/it]\u001b[A\n",
      "Epoch: 100%|█████████████████████████████████████| 3/3 [48:41<00:00, 973.85s/it]\n",
      "07/02/2020 12:44:41 - INFO - transformers.trainer -   \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "07/02/2020 12:44:42 - INFO - transformers.trainer -   Saving model checkpoint to output\n",
      "07/02/2020 12:44:42 - INFO - transformers.configuration_utils -   Configuration saved in output/config.json\n",
      "07/02/2020 12:44:42 - INFO - transformers.modeling_utils -   Model weights saved in output/pytorch_model.bin\n",
      "07/02/2020 12:44:43 - INFO - __main__ -   *** Evaluate ***\n",
      "07/02/2020 12:44:43 - INFO - transformers.trainer -   ***** Running Evaluation *****\n",
      "07/02/2020 12:44:43 - INFO - transformers.trainer -     Num examples = 78\n",
      "07/02/2020 12:44:43 - INFO - transformers.trainer -     Batch size = 8\n",
      "Evaluation: 100%|███████████████████████████████| 10/10 [02:39<00:00, 15.95s/it]\n",
      "07/02/2020 12:47:22 - INFO - transformers.trainer -   {'eval_loss': 3.207511329650879, 'epoch': 3.0, 'step': 30}\n",
      "07/02/2020 12:47:22 - INFO - __main__ -   ***** Eval results *****\n",
      "07/02/2020 12:47:22 - INFO - __main__ -     perplexity = 24.717495918195095\n"
     ]
    }
   ],
   "source": [
    "!python run_language_modeling.py \\\n",
    "    --output_dir=output \\\n",
    "    --model_type=gpt2 \\\n",
    "    --model_name_or_path=gpt2 \\\n",
    "    --do_train \\\n",
    "    --train_data_file=nlp_finetuning/data/posts.txt\\\n",
    "    --do_eval \\\n",
    "    --eval_data_file=nlp_finetuning/data/posts.txt\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"output\")\n",
    "# add the EOS token as PAD token to avoid warnings\n",
    "model = GPT2LMHeadModel.from_pretrained(\"output\", pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Demand forecasting is critical to businesses across almost all industries. It can seem easy, if not impossible, to predict how a company will perform in a given period of time. However, it is important to understand that forecasting can be done in many different ways.\n",
      "\n",
      "In this article, we will take a look at some of the most commonly used forecasting tools and how they can help you predict the future of your business. We will also show you how you can use these tools to make better decisions about how to spend your time and money. If you have any questions or comments, please let us know in the comments below.\n"
     ]
    }
   ],
   "source": [
    "# set no_repeat_ngram_size to 2\n",
    "input_ids = tokenizer.encode('Demand forecasting is critical to businesses across almost all industries. It can seem easy,', return_tensors='pt')\n",
    "\n",
    "beam_output = model.generate(\n",
    "    input_ids, \n",
    "    max_length=100, \n",
    "    num_beams=5, \n",
    "    no_repeat_ngram_size=2, \n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}